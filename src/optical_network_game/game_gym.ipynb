{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "from optical_network_game.node import *\n",
    "from optical_network_game.link import *\n",
    "from optical_network_game.requests import *\n",
    "from optical_network_game.user import *\n",
    "import gym\n",
    "import pygame, sys\n",
    "from pygame.locals import *\n",
    "from gym import spaces\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3 import HerReplayBuffer\n",
    "from stable_baselines3.her.goal_selection_strategy import GoalSelectionStrategy\n",
    "\n",
    "\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#from stable_baselines.common.vec_env import DummyVecEnv\n",
    "#from stable_baselines.deepq.policies import MlpPolicy\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3 import A2C\n",
    "import json\n",
    "import cv2\n",
    "\n",
    "#additional code added by me just for testing\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "#importing IPython's display module to plot images\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython: from IPython import display\n",
    "from itertools import count\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "import tensorboard\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq:\n",
    "    :param log_dir: Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: Verbosity level.\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose: int = 1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, 'best_model')\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "          # Retrieve training reward\n",
    "          x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "          if len(x) > 0:\n",
    "              # Mean training reward over the last 100 episodes\n",
    "              mean_reward = np.mean(y[-100:])\n",
    "              if self.verbose > 0:\n",
    "                print(f\"Num timesteps: {self.num_timesteps}\")\n",
    "                print(f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\")\n",
    "\n",
    "              # New best model, you could save the agent here\n",
    "              if mean_reward > self.best_mean_reward:\n",
    "                  self.best_mean_reward = mean_reward\n",
    "                  # Example for saving best model\n",
    "                  if self.verbose > 0:\n",
    "                    print(f\"Saving new best model to {self.save_path}\")\n",
    "                  self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class game_gym(gym.Env):\n",
    "    '''\n",
    "    Elastic Optical Network (EON) game made into an OpenAI gym environment for easier interface with RL algorithms.\n",
    "    '''\n",
    "    # set game window width and height\n",
    "    WINDOWWIDTH = 1000\n",
    "    WINDOWHEIGHT = 600\n",
    "    # set space between request, topology and spectrum parts of the game\n",
    "    MARGIN = 10\n",
    "    # set width of request space\n",
    "    INCOMINGREQUESTWIDTH = 200\n",
    "    # set width of topology space\n",
    "    NETWORKTOPOLOGYWIDTH = 560\n",
    "    # set width of spectrum space\n",
    "    SELECTEDLINKWIDTH = 200\n",
    "    # set space for score and timer at the top of the game screen\n",
    "    HEADER = 50\n",
    "    # set height of individual requests\n",
    "    REQUESTHEIGHT = 40\n",
    "    # set height of timer bar for individual requests\n",
    "    TIMERBARHEIGHT = 15\n",
    "    NUMBEROFSLOTS = 5\n",
    "    SPECTRUMBOXHEIGHT = 30\n",
    "    SPECTRUMBOXWIDTH = 120\n",
    "\n",
    "    # test if width of game spaces and margin fully cover the game screen width\n",
    "    assert WINDOWWIDTH == INCOMINGREQUESTWIDTH + NETWORKTOPOLOGYWIDTH + SELECTEDLINKWIDTH + 4*MARGIN\n",
    "    # set number of frame resets per second\n",
    "    FPS = 30\n",
    "\n",
    "    # define colours (RED, GREEN, BLUE)\n",
    "    RED = (255, 0, 0)\n",
    "    GRAY = (100, 100, 100)\n",
    "    WHITE = (255, 255, 255)\n",
    "    BLUE = (0, 0, 255)\n",
    "    BLACK = (0, 0, 0)\n",
    "    ORANGE = (255, 128, 0)\n",
    "    LIGHTGRAY = (150, 150, 150)\n",
    "    GREEN = (0, 255, 0)\n",
    "    # idk what this is\n",
    "    BGCOLOR = GRAY\n",
    "    # defined variable for colouring selected and unselected requests\n",
    "    colorRequest = BLACK\n",
    "\n",
    "    def __init__(self, nodeList, linkList, requestList, user):\n",
    "        \n",
    "        self.nodeList = nodeList\n",
    "        self.linkList = linkList\n",
    "        self.requestList = requestList\n",
    "        self.user = user\n",
    "\n",
    "        #ADDED also the req_num as the number of connection requests in the episode\n",
    "        self.req_num = len(requestList)\n",
    "        #debug\n",
    "        print(self.req_num)\n",
    "\n",
    "        #see if this changes things, setting action space to 6 actions only instead of 7 (original)\n",
    "        #changed to 4\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        \n",
    "        self.initialise_values()\n",
    "        \n",
    "        \n",
    "\n",
    "    def initialise_values(self):\n",
    "        '''\n",
    "        Intial values when starting the game. This includes initializing Pygame, setting timer, initializing game display screen and requests.\n",
    "        This is used when initalizing and resetting the game.\n",
    "        '''\n",
    "        self.requestMode = True\n",
    "        self.topologyMode = False\n",
    "        self.spectrumMode = False\n",
    "        self.completions = []\n",
    "\n",
    "        # initialize pygame\n",
    "        pygame.init()\n",
    "\n",
    "        # timer\n",
    "        self.timer_event = pygame.USEREVENT+1\n",
    "        # repeatedly create an event on the event queue every 1000ms / 1s\n",
    "        pygame.time.set_timer(self.timer_event, 1000)\n",
    "        self.timer = 60\n",
    "        # timer for request timer bar\n",
    "        self.timer2 = 60\n",
    "        \n",
    "        # create an object to help track time\n",
    "        self.FPSCLOCK = pygame.time.Clock()\n",
    "        # Initialize a window or screen for display\n",
    "        self.DISPLAYSURF = pygame.display.set_mode((self.WINDOWWIDTH, self.WINDOWHEIGHT))\n",
    "        # Set the game window caption\n",
    "        pygame.display.set_caption('Demo Game')\n",
    "        # fill the game screen with gray\n",
    "        self.DISPLAYSURF.fill(self.BLACK)\n",
    "        # initialize score\n",
    "        self.SCORE = 0\n",
    "        self.reward = 0\n",
    "        \n",
    "        #ADDED cumulative reward\n",
    "        self.reward_sum = 0\n",
    "\n",
    "        #ADDED connection service flag (tracks the number of connections fulfilled)\n",
    "        self.req_complete = 0\n",
    "\n",
    "        #ADDED number of links in routing\n",
    "        self.num_links = 0\n",
    "        \n",
    "        \n",
    "        # stores the requests available to the user in a list\n",
    "        self.activeRequests = []\n",
    "        # automatically selects the first request in the list when game starts\n",
    "        # self.user.selectRequest(self.requestList[0])\n",
    "        # setting value to end episode\n",
    "        self.done = False\n",
    "\n",
    "        # creating observation space for gym\n",
    "        self.observation_space = spaces.Box(\n",
    "            low= 0,\n",
    "            high = 255,\n",
    "            shape= (256, 256, 3),\n",
    "            dtype=np.uint8\n",
    "            )\n",
    "\n",
    "        self.info = {}\n",
    "\n",
    "        if self.user.getCurrentRequest() != None:\n",
    "            self.user.deselectRequest()\n",
    "\n",
    "        highlighted = [0]*self.NUMBEROFSLOTS\n",
    "        for node in self.nodeList:\n",
    "            node.setHighlighted(False)\n",
    "            node.setSelected(False)\n",
    "        for link in self.linkList:\n",
    "            link.setHighlighted(False)\n",
    "            link.setSelected(False)\n",
    "            link.setSpectrumHighlighted(highlighted)\n",
    "            link.setSpectrum(highlighted)\n",
    "        \n",
    "        self.completions = []\n",
    "\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        '''\n",
    "        Resets the game to start state\n",
    "        '''\n",
    "        self.initialise_values()\n",
    "        obs = np.array(pygame.surfarray.array3d(self.DISPLAYSURF.subsurface((210, 0, 560, 600))), dtype=np.uint8)\n",
    "        obs = cv2.resize(obs, dsize=(256, 256))\n",
    "        print(obs.shape)\n",
    "        return obs\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        #debug print\n",
    "        #print(\"ACTION\")\n",
    "        #print(action)\n",
    "        \n",
    "\n",
    "        #TESTING THIS \n",
    "        #resetting the self.reward variable to be 0, thus for every step, the reward \n",
    "        #isnt the cumulative reward, rather the reward gained for the action state.\n",
    "        self.reward = 0\n",
    "        #cause the if action !=6 part sends the chosen agent action to return the reward\n",
    "\n",
    "        for event in pygame.event.get():\n",
    "            # If game screen is closed, Pygame is stopped\n",
    "            if event.type == pygame.QUIT:\n",
    "                self.endGame()\n",
    "        # Updates requests and reduces timer every second\n",
    "            elif event.type == self.timer_event:\n",
    "                self.requestUpdate()\n",
    "\n",
    "            #editing this so only 4 actions\n",
    "            elif event.type == pygame.KEYDOWN:\n",
    "                if event.key == pygame.K_UP:\n",
    "                    action =  0\n",
    "                elif event.key == pygame.K_DOWN:\n",
    "                    action = 1\n",
    "                #elif event.key == pygame.K_LEFT:\n",
    "                   #action = 2\n",
    "                #elif event.key == pygame.K_RIGHT:\n",
    "                    #action = 3\n",
    "                elif event.key == pygame.K_RETURN:\n",
    "                    #action = 4\n",
    "                    action = 2\n",
    "                elif event.key == pygame.K_BACKSPACE:\n",
    "                    #action = 5\n",
    "                    action = 3\n",
    "        #if action != 4:\n",
    "        if self.requestMode == True:\n",
    "            self.request_logic(action)\n",
    "        elif self.topologyMode == True:\n",
    "            self.topology_logic(action)\n",
    "        elif self.spectrumMode == True:\n",
    "            self.spectrum_logic(action)\n",
    "        \n",
    "        obs = np.array(pygame.surfarray.array3d(self.DISPLAYSURF.subsurface((210, 0, 560, 600))), dtype=np.uint8)\n",
    "        obs = cv2.resize(obs, dsize=(256, 256))\n",
    "\n",
    "        # if (self.timer < self.requestList[-1].getTimeStart() and self.activeRequests == []) or self.timer == 0:\n",
    "        if self.timer == 0:\n",
    "            self.done = True\n",
    "            #debug print\n",
    "            print(\"Cumulative Reward Obtained (GAME END):\")\n",
    "            print(self.reward_sum)\n",
    "\n",
    "        self.info[self.timer2] = {\n",
    "            'display': obs,\n",
    "            'user': self.user\n",
    "            }\n",
    "\n",
    "        #debug THIS WORKS\n",
    "        #print(\"STEP REWARD:\")\n",
    "        #print(self.reward)\n",
    "        #adds the reward to the cumulative reward variable\n",
    "        self.reward_sum += self.reward\n",
    "\n",
    "        #updates the score based on the cumulative reward\n",
    "        #self.SCORE += self.reward\n",
    "\n",
    "        return obs, self.reward, self.done, self.info\n",
    "    \n",
    "    # def get_action(self):\n",
    "        \n",
    "        #  for event in pygame.event.get():\n",
    "        #     # If game screen is closed, Pygame is stopped\n",
    "        #     if event.type == pygame.QUIT:\n",
    "        #         self.endGame()\n",
    "        # # Updates requests and reduces timer every second\n",
    "        #     elif event.type == self.timer_event:\n",
    "        #         self.requestUpdate()\n",
    "\n",
    "        #     elif event.type == pygame.KEYDOWN:\n",
    "        #         if event.key == pygame.K_UP:\n",
    "        #             return 0\n",
    "        #         elif event.key == pygame.K_DOWN:\n",
    "        #             return 1\n",
    "        #         elif event.key == pygame.K_LEFT:\n",
    "        #             return 2\n",
    "        #         elif event.key == pygame.K_RIGHT:\n",
    "        #             return 3\n",
    "        #         elif event.key == pygame.K_RETURN:\n",
    "        #             return 4\n",
    "        #         elif event.key == pygame.K_BACKSPACE:\n",
    "        #             return 5\n",
    "        #         else:\n",
    "        #             return 6\n",
    "\n",
    "\n",
    "    def render(self, mode = \"human\"):\n",
    "        '''\n",
    "        Renders the game display screen and updates it per FPS value set. Includes drawing topolgy, requests, spectrum, score and timer.\n",
    "        '''\n",
    "        # for event in pygame.event.get():\n",
    "        #     # If game screen is closed, Pygame is stopped\n",
    "        #     if event.type == pygame.QUIT:\n",
    "        #         self.endGame()\n",
    "        \n",
    "            \n",
    "        # creating game screen\n",
    "        # fill the game screen with gray\n",
    "        self.DISPLAYSURF.fill(GRAY)\n",
    "        # display the score on the game scren\n",
    "        self.displayScore()\n",
    "        # display the timer on the game scren\n",
    "        self.displayTimer()\n",
    "\n",
    "        # draw topology on the game screen\n",
    "        self.drawTopologyScreen()\n",
    "        # draw requests on the game screen\n",
    "        self.drawRequestsScreen()\n",
    "        # draw spectrums on the game screen\n",
    "        self.drawSpectrumScreen()\n",
    "\n",
    "        # Update portions of the screen for software displays (in this case the entire screen is updated)      \n",
    "        pygame.display.update()\n",
    "\n",
    "        # timer2 decreases per frame to allow smooth decrease of timer bar width\n",
    "        self.timer2 -= 1/self.FPS\n",
    "        \n",
    "        # updates the clock once per frame\n",
    "        self.FPSCLOCK.tick(self.FPS)\n",
    "\n",
    "\n",
    "\n",
    "    def displayScore(self):\n",
    "        '''\n",
    "        Function to draw score on game screen.\n",
    "        Used in render().\n",
    "        '''\n",
    "        pygame.font.init()\n",
    "        myfont = pygame.font.SysFont('Calibri', 30)\n",
    "        textsurface = myfont.render(f'SCORE: {str(self.SCORE)}', False, WHITE)\n",
    "        self.DISPLAYSURF.blit(textsurface, (self.WINDOWWIDTH/2-70, 15))\n",
    "\n",
    "\n",
    "\n",
    "    def drawTopologyScreen(self):\n",
    "        '''\n",
    "        Function to draw topolgy on game screen.\n",
    "        Used in render().\n",
    "        '''\n",
    "        # highlighting the topology space when selecting path for easier recognition\n",
    "        if self.topologyMode == True:\n",
    "            color = self.RED\n",
    "        else:\n",
    "            color = self.BLACK\n",
    "\n",
    "        # Draw rectangle where topology is displayed in\n",
    "        pygame.draw.rect(self.DISPLAYSURF, color, (self.MARGIN + self.INCOMINGREQUESTWIDTH + self.MARGIN, \\\n",
    "            self.HEADER, self.NETWORKTOPOLOGYWIDTH, self.WINDOWHEIGHT - self.HEADER - self.MARGIN), 4)\n",
    "\n",
    "        for link in self.linkList:\n",
    "            link.drawLink(self.DISPLAYSURF, self.BLUE)\n",
    "            link.drawSpectrum(self.DISPLAYSURF, link.getX() - self.SPECTRUMBOXWIDTH/2, link.getY() - self.SPECTRUMBOXHEIGHT/2)\n",
    "\n",
    "        for node in self.nodeList:\n",
    "            node.drawNode(self.DISPLAYSURF, self.BLUE)\n",
    "        \n",
    "        \n",
    "\n",
    "    def drawRequestsScreen(self):\n",
    "        '''\n",
    "        Function to display requests on game screen.\n",
    "        Used in render().\n",
    "        '''\n",
    "        # highlighting the request space when selecting requests for easier recognition\n",
    "        if self.requestMode == True:\n",
    "            color = self.RED\n",
    "        else:\n",
    "            color = self.BLACK\n",
    "        \n",
    "        # Draw rectangle where requests are displayed in\n",
    "        pygame.draw.rect(self.DISPLAYSURF, color, (self.MARGIN, self.HEADER, self.INCOMINGREQUESTWIDTH, \\\n",
    "            self.WINDOWHEIGHT - self.HEADER - self.MARGIN), 4)\n",
    "        \n",
    "        # FOR each active request, draw a rectangle displaying the request within it\n",
    "        for i, request in enumerate(self.activeRequests):\n",
    "            requestBox = pygame.Rect(self.MARGIN, self.HEADER + i*(self.REQUESTHEIGHT + self.TIMERBARHEIGHT), self.INCOMINGREQUESTWIDTH, self.REQUESTHEIGHT)\n",
    "            # calculate the time left before request expires\n",
    "            timeLeft = request.timeLimit - (request.timeStart - self.timer2)\n",
    "            # draws a rectangle that indicates the time left for the request before it expires by decreasing its length\n",
    "            if timeLeft > 0:\n",
    "                pygame.draw.rect(self.DISPLAYSURF, self.ORANGE, (self.MARGIN, self.HEADER + (i+1)*self.REQUESTHEIGHT + i*self.TIMERBARHEIGHT, \\\n",
    "                    self.INCOMINGREQUESTWIDTH*timeLeft/request.timeLimit, self.TIMERBARHEIGHT))\n",
    "            # highlighting the selected request for easier recognition\n",
    "            if request.getSelected() == True:\n",
    "                colorRequest = self.RED\n",
    "            else:\n",
    "                colorRequest = self.LIGHTGRAY\n",
    "            pygame.draw.rect(self.DISPLAYSURF, colorRequest, requestBox)\n",
    "            pygame.font.init()\n",
    "            # display source and destination node, as well as bandwidth needed for each request\n",
    "            myfont = pygame.font.SysFont('Calibri', 30)\n",
    "            textsurface = myfont.render(f'({request.sourceNode.getName()}, {request.destNode.getName()}, {request.bandWidth})', False, WHITE)\n",
    "            text_rect = textsurface.get_rect(center=requestBox.center)\n",
    "            self.DISPLAYSURF.blit(textsurface, text_rect)\n",
    "\n",
    "\n",
    "\n",
    "    def drawSpectrumScreen(self):\n",
    "        '''\n",
    "        Function to draw links' spectrums on game screen. The spectrums are updated when they are allocated, and also when links are selected.\n",
    "        Used in render().\n",
    "        '''\n",
    "        # highlighting the spectrum space when doing spectrum allocation for easier recognition\n",
    "        if self.spectrumMode == True:\n",
    "            color = self.RED\n",
    "        else:\n",
    "            color = self.BLACK\n",
    "        \n",
    "        # Draw rectangle where spectrums are displayed in\n",
    "        spectrumBox = pygame.Rect((self.MARGIN + self.INCOMINGREQUESTWIDTH + self.MARGIN + self.NETWORKTOPOLOGYWIDTH + self.MARGIN, \\\n",
    "            self.HEADER, self.SELECTEDLINKWIDTH, self.WINDOWHEIGHT - self.HEADER - self.MARGIN))\n",
    "        pygame.draw.rect(self.DISPLAYSURF, color, spectrumBox, 4)\n",
    "\n",
    "        # drawing spectrum selected and unselected links\n",
    "        self.selectedLinks = []\n",
    "        for entry in self.user.getLinksSelected():\n",
    "            self.selectedLinks.append(entry[1])\n",
    "        unselectedLinks = self.linkList.copy()\n",
    "        for link in self.linkList:\n",
    "            if link in self.selectedLinks:\n",
    "                unselectedLinks.remove(link)\n",
    "        \n",
    "        # selected links text to display\n",
    "        pygame.font.init()\n",
    "        myfont = pygame.font.SysFont('Calibri', 27)\n",
    "        textsurface = myfont.render(f'Selected Links:', False, self.WHITE)\n",
    "        text_rect = textsurface.get_rect(center=spectrumBox.center)\n",
    "        self.DISPLAYSURF.blit(textsurface, (text_rect[0], self.HEADER + self.MARGIN))\n",
    "        \n",
    "        # drawing selected links\n",
    "        if self.selectedLinks != []:\n",
    "            for i in range(len(self.selectedLinks)):\n",
    "                textsurface = myfont.render(f'{self.selectedLinks[i].getName()}', False, self.WHITE)\n",
    "                self.DISPLAYSURF.blit(textsurface, (self.MARGIN + self.INCOMINGREQUESTWIDTH + self.MARGIN + self.NETWORKTOPOLOGYWIDTH + self.MARGIN + 6, \\\n",
    "                    self.HEADER + self.MARGIN + (i + 1)*(self.SPECTRUMBOXHEIGHT + 5)))\n",
    "                self.selectedLinks[i].drawSpectrum(self.DISPLAYSURF, self.MARGIN + self.INCOMINGREQUESTWIDTH + self.MARGIN + self.NETWORKTOPOLOGYWIDTH + \\\n",
    "                    self.MARGIN + 6 + 35, self.HEADER + self.MARGIN + (i + 1)*(self.SPECTRUMBOXHEIGHT + 5))\n",
    "\n",
    "        # unselected links text to display\n",
    "        textsurface = myfont.render(f'Unselected Links:', False, self.WHITE)\n",
    "        text_rect = textsurface.get_rect(center=spectrumBox.center)\n",
    "        self.DISPLAYSURF.blit(textsurface, (text_rect[0], self.HEADER + self.MARGIN + (len(self.selectedLinks) + 1)*(self.SPECTRUMBOXHEIGHT + 5)))\n",
    "\n",
    "        # drawing unselected links\n",
    "        if unselectedLinks != []:\n",
    "            for i in range(len(unselectedLinks)):\n",
    "                textsurface = myfont.render(f'{unselectedLinks[i].getName()}', False, self.WHITE)\n",
    "                self.DISPLAYSURF.blit(textsurface, (self.MARGIN + self.INCOMINGREQUESTWIDTH + self.MARGIN + self.NETWORKTOPOLOGYWIDTH + self.MARGIN + 6, \\\n",
    "                    self.HEADER + self.MARGIN + (len(self.selectedLinks) + 1)*(self.SPECTRUMBOXHEIGHT + 5) + (i + 1)*(self.SPECTRUMBOXHEIGHT + 5)))\n",
    "                unselectedLinks[i].drawSpectrum(self.DISPLAYSURF, self.MARGIN + self.INCOMINGREQUESTWIDTH + self.MARGIN + self.NETWORKTOPOLOGYWIDTH + \\\n",
    "                    self.MARGIN + 6 + 35, self.HEADER + self.MARGIN + (len(self.selectedLinks) + 1)*(self.SPECTRUMBOXHEIGHT + 5) + \\\n",
    "                    (i + 1)*(self.SPECTRUMBOXHEIGHT + 5))\n",
    "\n",
    "\n",
    "\n",
    "    # drawing clock\n",
    "    def displayTimer(self):\n",
    "        '''\n",
    "        Function to draw timer on game screen.\n",
    "        Used in render().\n",
    "        '''\n",
    "        pygame.font.init()\n",
    "        myfont = pygame.font.SysFont('Calibri', 30)\n",
    "        textsurface = myfont.render(f'Time: {str(self.timer)}', False, self.WHITE)\n",
    "        self.DISPLAYSURF.blit(textsurface, (self.WINDOWWIDTH - 150, 15))\n",
    "\n",
    "\n",
    "    def requestUpdate(self):\n",
    "        '''\n",
    "        Function to update the requests available to player. Adds and removes active requests.\n",
    "        Also updates the timer, decreasing it by one.\n",
    "        Used in render().\n",
    "        '''\n",
    "        # sending in requests\n",
    "        # occurs every secon\n",
    "        # FOR each request in the game\n",
    "        for request in self.requestList:\n",
    "            # IF the game timer matches the start time of the request\n",
    "            # THEN the request becomes active\n",
    "            if self.timer == request.timeStart:\n",
    "                self.activeRequests.append(request)\n",
    "        \n",
    "        for request in self.activeRequests:\n",
    "            # IF the game timer matches the end time of the request (calculated based on time limit of request)\n",
    "            # THEN the request is considered blocked and score decreases. Request is also de-activated\n",
    "            if self.timer == request.timeStart - request.timeLimit + 1:\n",
    "                request.setBlock(True)\n",
    "\n",
    "                #COMMENTED OUT, TESTING SCORE PRINTOUT\n",
    "                #changed to -5 from -1\n",
    "                self.SCORE -= 5\n",
    "\n",
    "                #changed to -1\n",
    "                #reward expiry\n",
    "                #self.reward -= 100\n",
    "                self.reward -= 10\n",
    "                \n",
    "                #adding to the req_complete flag \n",
    "                self.req_complete += 1\n",
    "\n",
    "                if self.req_complete == self.req_num:\n",
    "\n",
    "                    #if the only connection expires then the episode ends\n",
    "                    #debug\n",
    "                    #changed this so that if the connection which expires is the final conn then the episode ends\n",
    "                    print(\"Request Timed Out, cumulative Reward:\")\n",
    "                    print(self.reward_sum)\n",
    "                    self.done = True\n",
    "\n",
    "                try:\n",
    "                    self.activeRequests.remove(request)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "        # sets slots to 0 when the request runs out after allocation\n",
    "        for curr_request, link_list, spectrum in self.completions:\n",
    "            if self.timer == curr_request.getTimeDeallocated():\n",
    "                for link in link_list:\n",
    "                    spectrumCopy = link[1].getSpectrum().copy()\n",
    "                    for i, slot in enumerate(spectrum):\n",
    "                        if slot == 1:\n",
    "                            spectrumCopy[i] = 0\n",
    "\n",
    "                    link[1].setSpectrum(spectrumCopy)\n",
    "\n",
    "        # IF user has selected a request\n",
    "        if self.user.getCurrentRequest() != None:\n",
    "            # IF selected request expires before it is completed\n",
    "            # THEN the links selected by the user thus far is removed and the links the user can choose is reset\n",
    "            if self.timer == self.user.getCurrentRequest().timeStart - self.user.getCurrentRequest().timeLimit + 1 and self.requestMode == False:\n",
    "                availableLinks = self.clearAll()\n",
    "                \n",
    "\n",
    "            # ELSE when user has selected a request that has not expired, user can still continue to service it\n",
    "        # timer countsdown every second\n",
    "            elif self.timer == self.user.getCurrentRequest().timeStart - self.user.getCurrentRequest().timeLimit + 1 and self.requestMode == True:\n",
    "                self.user.deselectRequest()\n",
    "        # decrease timer by 1\n",
    "        self.timer -= 1\n",
    "        self.timer2 = self.timer\n",
    "        if self.user.getCurrentRequest() == None:\n",
    "            if self.activeRequests != []:\n",
    "                self.user.selectRequest(self.activeRequests[0])\n",
    "            \n",
    "\n",
    "    def request_logic(self, action):\n",
    "        # IF there are no selected requests and there are active requests\n",
    "        # THEN the first indexed request in the list of active requests is selected\n",
    "        # IN THE EVENT THAT selected request expires\n",
    "        # if self.user.getCurrentRequest() == None and self.activeRequests != []:\n",
    "        #     self.user.selectRequest(self.activeRequests[0])\n",
    "            \n",
    "        # IF there are not active requests\n",
    "        # THEN do nothing\n",
    "        if self.activeRequests == []:\n",
    "            \n",
    "            #placeholder reward to test \n",
    "            #changed to 2 from 4\n",
    "            if action == 2:\n",
    "                #print(\"no request (enter) reward\")\n",
    "                #originally set to + 10\n",
    "                #self.reward += 30\n",
    "                \n",
    "                #testing this normalised\n",
    "                #self.reward += 0.3\n",
    "                self.reward += 0.3\n",
    "\n",
    "            elif action == 0 or action == 1 or action == 3: \n",
    "                #originally set to -10\n",
    "                #self.reward -= 1\n",
    "\n",
    "                #testing this normalised\n",
    "                #self.reward -= 1\n",
    "                self.reward -= 1\n",
    "\n",
    "            pass\n",
    "\n",
    "        else:\n",
    "            # define number of active requests\n",
    "            activeRequestsLength = len(self.activeRequests)\n",
    "            # define the request the user is currently at\n",
    "            if self.user.getCurrentRequest() in self.activeRequests:\n",
    "                requestIndex = self.activeRequests.index(self.user.getCurrentRequest())\n",
    "            else:\n",
    "                return\n",
    "            # IF DOWN arrow key is pressed\n",
    "            # THEN the request below the current one is selected\n",
    "            if action == 1:\n",
    "                # IF DOWN arrow key is pressed and it is already the last request in the list\n",
    "                # THEN the first request in the list is selected\n",
    "                if requestIndex == activeRequestsLength - 1:\n",
    "                    requestIndex = 0\n",
    "                else:\n",
    "                    requestIndex += 1\n",
    "                # deselects the old request and selects the new one\n",
    "                self.user.deselectRequest()\n",
    "                self.user.selectRequest(self.activeRequests[requestIndex])\n",
    "\n",
    "                #Experimenting reward function\n",
    "                #unchanged\n",
    "                #self.reward -= 1\n",
    "\n",
    "                #testing this normalised\n",
    "                self.reward -= 0.3\n",
    "\n",
    "            # IF UP arrow key is pressed\n",
    "            # THEN the request above the current one is selected\n",
    "            elif action == 0:\n",
    "                # IF UP arrow key is pressed and it is already the first request in the list\n",
    "                # THEN the last request in the list is selected\n",
    "                if requestIndex == 0:\n",
    "                    requestIndex = activeRequestsLength - 1\n",
    "                else:\n",
    "                    requestIndex -= 1\n",
    "                # deselects the old request and selects the new one\n",
    "                self.user.deselectRequest()\n",
    "                self.user.selectRequest(self.activeRequests[requestIndex])\n",
    "\n",
    "                #Experimenting reward function\n",
    "                #unchanged\n",
    "                #self.reward -= 1\n",
    "\n",
    "                #testing this normalised\n",
    "                self.reward -= 0.3\n",
    "            \n",
    "            # IF ENTER key is pressed\n",
    "            # THEN the user moves to the topology space to service the request selected\n",
    "            #changed from 4 to 2\n",
    "            elif action == 2:\n",
    "                #self.reward += 0\n",
    "                #experimenting reward function\n",
    "                #changed to +10 originally\n",
    "                #changed to 20 on latest run\n",
    "                #print(\"enter with request\")\n",
    "                #self.reward += 30\n",
    "\n",
    "                #testing this normalised\n",
    "                self.reward += 1\n",
    "                \n",
    "                self.requestMode = False\n",
    "                self.topologyMode = True\n",
    "                #set number of links value to 0 upon initiation of topology mode\n",
    "                self.num_links = 0\n",
    "\n",
    "                #adding score reward for progressing from game mode\n",
    "                self.SCORE += 1\n",
    "\n",
    "                #TESTING if agent gets reward = current cumulative reward in each mode (so basically reset to 0 or times 2 if -ve or +ve)\n",
    "                #might need to change this to just adding like 20 as a one step reward if the agent passes into a new stage\n",
    "                #self.reward += 100\n",
    "\n",
    "                # user automatically starts at the source node of the request\n",
    "                self.user.setCurrentNode(self.user.currentRequest.getSourceNode())\n",
    "                # source node is automatically set as selected\n",
    "                self.user.getCurrentNode().setSelected(True)\n",
    "                # the first link and adjacent node connected to the source node (in the list) will be automatically highlighted\n",
    "                self.user.getCurrentNode().getLinks()[0][0].setHighlighted(True)\n",
    "                self.user.getCurrentNode().getLinks()[0][1].setHighlighted(True)\n",
    "                # defines index for use in topology space\n",
    "                self.index = 0\n",
    "\n",
    "                \n",
    "            #changed to only backspace (action 3)\n",
    "            elif action == 3:\n",
    "                #if left right or backspace pressed\n",
    "                #experimenting reward function\n",
    "                #changed to -10\n",
    "                #self.reward -= 5\n",
    "                \n",
    "                #testing this normalised\n",
    "                self.reward -= 1\n",
    "            \n",
    "\n",
    "\n",
    "    def topology_logic(self, action):\n",
    "        # IF BACKSPACE key is pressed\n",
    "        #changed to 3 for backspace from 5\n",
    "        if action == 3:\n",
    "            # IF BACKSPACE key is pressed and the user is at the source node\n",
    "            # THEN the user moves back to selecting a request, highlights will be reset\n",
    "            if self.user.getCurrentNode() == self.user.getCurrentRequest().getSourceNode():\n",
    "                for node in self.nodeList:\n",
    "                    node.setHighlighted(False)\n",
    "                    node.setSelected(False)\n",
    "                for link in self.linkList:\n",
    "                    link.setHighlighted(False)\n",
    "                    link.setSelected(False)\n",
    "                self.requestMode = True\n",
    "                self.topologyMode = False\n",
    "\n",
    "                #returning to request mode action\n",
    "                #experimenting reward function\n",
    "                #changed to -10\n",
    "                #self.reward -= 1\n",
    "                #self.reward -= 10\n",
    "\n",
    "                #testing this normalised\n",
    "                self.reward -= 1\n",
    "\n",
    "                #adding visual score screen update set to -5 first\n",
    "                self.SCORE -= 5\n",
    "                \n",
    "            # IF BACKSPACE key is pressed and the user is not at the source node\n",
    "            # THEN the user moves back to previous node\n",
    "            else:\n",
    "                links_selected = self.user.getLinksSelected()\n",
    "                highlightedSpectrum = [0]*NUMBEROFSLOTS\n",
    "                links_selected[-1][1].setSpectrumHighlighted(highlightedSpectrum)\n",
    "                \n",
    "                # define previous node and link pair from selected links list\n",
    "                previous = self.user.getLinksSelected()[-1]\n",
    "                # deselects the current node user is at\n",
    "                self.user.getCurrentNode().setSelected(False)\n",
    "                # selects the pervious node user was at\n",
    "                self.user.setCurrentNode(previous[0])\n",
    "\n",
    "\n",
    "                #experimenting reward function\n",
    "                #undoing routing selection\n",
    "                #set to -1\n",
    "                #self.reward -= 1\n",
    "\n",
    "                #testing this normalised\n",
    "                #changed to -0.3 from -0.1\n",
    "                self.reward -= 0.2\n",
    "\n",
    "                #reducing number of links\n",
    "                self.num_links -= 1\n",
    "\n",
    "\n",
    "                # deselects the link user chose to get to the current node\n",
    "                previous[1].setSelected(False)\n",
    "                # removes the node and link pair from the selected links list\n",
    "                self.user.getLinksSelected().remove(previous)\n",
    "                \n",
    "                # removing all highlights (makes it easier since only highlights will be where user is at)\n",
    "                for node in self.nodeList:\n",
    "                    node.setHighlighted(False)\n",
    "                for link in self.linkList:\n",
    "                    link.setHighlighted(False)\n",
    "                # refreshes the links user can choose\n",
    "                availableLinks = self.checkAvailable()\n",
    "                # the first link and adjacent node connected to the current node (in the list) will be automatically highlighted\n",
    "                if availableLinks != []:\n",
    "                    availableLinks[self.index][0].setHighlighted(True)\n",
    "                    availableLinks[self.index][1].setHighlighted(True)\n",
    "        \n",
    "        # ELSE IF any button except BACKSPACE is pressed\n",
    "        else:\n",
    "            # refreshes the links user can choose\n",
    "            availableLinks = self.checkAvailable()\n",
    "            \n",
    "            # the first link and adjacent node connected to the current node (in the list) will be automatically highlighted\n",
    "            if availableLinks != []:\n",
    "                availableLinks[self.index][0].setHighlighted(True)\n",
    "                availableLinks[self.index][1].setHighlighted(True)\n",
    "            else:\n",
    "                # self.DISPLAYSURF.fill(self.RED)\n",
    "                pass\n",
    "                \n",
    "\n",
    "        # IF UP arrow key is pressed\n",
    "        # THEN the link above the current one is selected\n",
    "        if action == 0:\n",
    "            \n",
    "            #CONDITIONAL STATEMENT TO CHECK NODE ALLOCATION\n",
    "            #if the current node selected was the destination node then penalise agent for moving away\n",
    "            if self.user.getCurrentNode() == self.user.getCurrentRequest().getDestNode():\n",
    "                #initial reward setting\n",
    "                self.reward -= 1\n",
    "            #then carry on the rest of the code\n",
    "            \n",
    "            # de-highlights the current link\n",
    "            availableLinks[self.index][0].setHighlighted(False)\n",
    "            availableLinks[self.index][1].setHighlighted(False)\n",
    "            # IF UP arrow key is pressed and it is already the highest link\n",
    "            # THEN the lowest link is selected\n",
    "            if self.index == 0:\n",
    "                self.index = len(availableLinks) - 1\n",
    "            else:\n",
    "                self.index -= 1\n",
    "            # highlights the current link\n",
    "            availableLinks[self.index][0].setHighlighted(True)\n",
    "            availableLinks[self.index][1].setHighlighted(True)\n",
    "\n",
    "\n",
    "            #CONDITIONAL STATEMENT TO CHECK NODE AFTER MOVEMENT\n",
    "            #if the new node is the dest node then positive reward agent\n",
    "            if self.user.getCurrentNode() == self.user.getCurrentRequest().getDestNode():\n",
    "                self.reward += 2\n",
    "\n",
    "            #else if the new node selected is not the destinatino node then negative reward\n",
    "            elif self.user.getCurrentNode() != self.user.getCurrentRequest().getDestNode():\n",
    "                #initial reward\n",
    "                self.reward -= 0\n",
    "\n",
    "            #Experimenting reward function\n",
    "            #unchanged\n",
    "            #self.reward -= 1\n",
    "\n",
    "            #testing this normalised\n",
    "            #changed to 0.4 from 0.5\n",
    "            #self.reward += 0.4\n",
    "\n",
    "        # IF DOWN arrow key is pressed\n",
    "        # THEN the link below the current one is selected\n",
    "        elif action == 1:\n",
    "            \n",
    "            #CONDITIONAL STATEMENT TO CHECK NODE ALLOCATION\n",
    "            #if the current node selected was the destination node then penalise agent for moving away\n",
    "            if self.user.getCurrentNode() == self.user.getCurrentRequest().getDestNode():\n",
    "                #initial reward setting\n",
    "                self.reward -= 1\n",
    "            #then carry on the rest of the code\n",
    "            \n",
    "            # de-highlights the current link\n",
    "            availableLinks[self.index][0].setHighlighted(False)\n",
    "            availableLinks[self.index][1].setHighlighted(False)\n",
    "            # IF DOWN arrow key is pressed and it is already the lowest link\n",
    "            # THEN the highest link is selected\n",
    "            if self.index == len(availableLinks) - 1:\n",
    "                self.index = 0\n",
    "            else:\n",
    "                self.index += 1\n",
    "            # highlights the current link\n",
    "            availableLinks[self.index][0].setHighlighted(True)\n",
    "            availableLinks[self.index][1].setHighlighted(True)\n",
    "\n",
    "            \n",
    "            #CONDITIONAL STATEMENT TO CHECK NODE AFTER MOVEMENT\n",
    "            #if the new node is the dest node then positive reward agent\n",
    "            if self.user.getCurrentNode() == self.user.getCurrentRequest().getDestNode():\n",
    "                self.reward += 2\n",
    "\n",
    "            #else if the new node selected is not the destinatino node then negative reward\n",
    "            elif self.user.getCurrentNode() != self.user.getCurrentRequest().getDestNode():\n",
    "                #initial reward\n",
    "                self.reward -= 0\n",
    "\n",
    "\n",
    "            #Experimenting reward function\n",
    "            #unchanged\n",
    "            #self.reward -= 1\n",
    "\n",
    "            #testing this normalised\n",
    "            #changed to 0.4 from 0.5\n",
    "            #self.reward += 0.4\n",
    "\n",
    "        # IF ENTER key is pressed\n",
    "        # THEN the user selects the link and moves to the adjacent node\n",
    "        #changed to 2 from 4 for smaller action steps\n",
    "        elif action == 2:\n",
    "            \n",
    "            #experimenting reward function\n",
    "            #commented out for now\n",
    "            #self.reward += 1\n",
    "            \n",
    "            # IF ENTER key is pressed and user has not reached the destination node\n",
    "            if self.user.getCurrentNode() != self.user.getCurrentRequest().getDestNode():\n",
    "                 #routing to a non destination node\n",
    "                #Experimenting reward function\n",
    "                #changed to +1\n",
    "                #self.reward -= 5\n",
    "                #self.reward += 1\n",
    "\n",
    "                #testing this normalised\n",
    "                self.reward += 0.2\n",
    "\n",
    "                #increasing number of links\n",
    "                self.num_links += 1\n",
    "\n",
    "                # IF the selected link does not move the user to the destination node\n",
    "                # THEN the link and node is de-highlighted and set to selected,\n",
    "                # user moves to the adjacent node connected to the selected link\n",
    "                if availableLinks[self.index][0] != self.user.getCurrentRequest().getDestNode():\n",
    "                    availableLinks[self.index][0].setHighlighted(False)\n",
    "                    availableLinks[self.index][1].setHighlighted(False)\n",
    "                    availableLinks[self.index][0].setSelected(True)\n",
    "                    availableLinks[self.index][1].setSelected(True)\n",
    "                    # current node and link selected is added to the list\n",
    "                    self.user.addLink(self.user.getCurrentNode(), availableLinks[self.index][1])\n",
    "                    # new current node is set to adjacent node connected to the selected link\n",
    "                    self.user.setCurrentNode(availableLinks[self.index][0])\n",
    "                    # index is set back to default 0 (as it is a new node)\n",
    "                    self.index = 0\n",
    "                    # links the user can choose are refreshed\n",
    "                    availableLinks = self.checkAvailable()\n",
    "                    # the first link and adjacent node connected to the current node (in the list) will be automatically highlighted\n",
    "                    if availableLinks != []:\n",
    "                        availableLinks[self.index][0].setHighlighted(True)\n",
    "                        availableLinks[self.index][1].setHighlighted(True)\n",
    "\n",
    "                        # need to include selecting first few slots automatically\n",
    "                        bandwidth = self.user.getCurrentRequest().getBandwidth()\n",
    "                        linksSelected = [link[1] for link in self.user.getLinksSelected()]\n",
    "                        highlightedSpectrum = [0]*self.NUMBEROFSLOTS\n",
    "                        for i in range(bandwidth):\n",
    "                            highlightedSpectrum[i] = 1\n",
    "                        for link in linksSelected:\n",
    "                            link.setSpectrumHighlighted(highlightedSpectrum)\n",
    "                        self.spectrumIndex = 0\n",
    "\n",
    "                    else:\n",
    "                        # undo selection\n",
    "                        # define previous node and link pair from selected links list\n",
    "                        previous = self.user.getLinksSelected()[-1]\n",
    "                        # deselects the current node user is at\n",
    "                        self.user.getCurrentNode().setSelected(False)\n",
    "                        # selects the pervious node user was at\n",
    "                        self.user.setCurrentNode(previous[0])\n",
    "\n",
    "                        # deselects the link user chose to get to the current node\n",
    "                        previous[1].setSelected(False)\n",
    "                        # removes the node and link pair from the selected links list\n",
    "                        self.user.getLinksSelected().remove(previous)\n",
    "                        \n",
    "                        # removing all highlights (makes it easier since only highlights will be where user is at)\n",
    "                        for node in self.nodeList:\n",
    "                            node.setHighlighted(False)\n",
    "                        for link in self.linkList:\n",
    "                            link.setHighlighted(False)\n",
    "                        # refreshes the links user can choose\n",
    "                        availableLinks = self.checkAvailable()\n",
    "                        availableLinks[self.index][0].setHighlighted(True)\n",
    "                        availableLinks[self.index][1].setHighlighted(True)\n",
    "                        self.DISPLAYSURF.fill(self.RED)\n",
    "\n",
    "                   \n",
    "\n",
    "                # ELSE IF the selected link moves the user to the destination node\n",
    "                # THEN the link and node is de-highlighted and set to selected,\n",
    "                # user moves to the spectrum space for spectrum allocation\n",
    "                else:\n",
    "                    availableLinks[self.index][0].setHighlighted(False)\n",
    "                    availableLinks[self.index][1].setHighlighted(False)\n",
    "                    availableLinks[self.index][0].setSelected(True)\n",
    "                    availableLinks[self.index][1].setSelected(True)\n",
    "                    # current node and link selected is added to the list\n",
    "                    self.user.addLink(self.user.getCurrentNode(), availableLinks[self.index][1])\n",
    "                    self.topologyMode = False\n",
    "                    self.spectrumMode = True\n",
    "\n",
    "                    #adding number of links to be 1 from reset value of 0\n",
    "                    self.num_links += 1\n",
    "\n",
    "                    # need to include selecting first few slots automatically\n",
    "                    bandwidth = self.user.getCurrentRequest().getBandwidth()\n",
    "                    linksSelected = [link[1] for link in self.user.getLinksSelected()]\n",
    "                    highlightedSpectrum = [0]*self.NUMBEROFSLOTS\n",
    "                    for i in range(bandwidth):\n",
    "                        highlightedSpectrum[i] = 1\n",
    "                    for link in linksSelected:\n",
    "                        link.setSpectrumHighlighted(highlightedSpectrum)\n",
    "                    self.spectrumIndex = 0\n",
    "                    \n",
    "                    #moving to spectrum mode\n",
    "\n",
    "                    #CONDITIONAL REWARDS FOR NUMBER OF LINKS\n",
    "                    #if the shortest route was chosen give huge reward\n",
    "                    if self.num_links == 1 or len(linksSelected) == 1:\n",
    "                        #debug print\n",
    "                        print(\"1 link\")                       \n",
    "                        print(len(linksSelected))\n",
    "                        print(self.num_links)\n",
    "                        self.reward += 5\n",
    "                        self.SCORE += 5\n",
    "\n",
    "                    #if route chosen was 2 links, give small negative reward\n",
    "                    elif self.num_links == 2 or len(linksSelected) == 2:\n",
    "                        #debug print\n",
    "                        print(\"2 links\")\n",
    "                        print(len(linksSelected))\n",
    "                        print(self.num_links)\n",
    "                        self.reward -= 50\n",
    "                        self.SCORE -= 1\n",
    "\n",
    "                    #if route chosen was greater than or equal to 3 links, give large negative reward\n",
    "                    elif self.num_links >= 3 or len(linksSelected) >= 3:\n",
    "                        #debug print\n",
    "                        print(\"more than 3 links\")\n",
    "                        self.reward -= 100\n",
    "                        self.SCORE -= 5\n",
    "\n",
    "\n",
    "                    #Experimenting reward function\n",
    "                    #added reward + 20\n",
    "                    #self.reward += 30\n",
    "\n",
    "                    #testing this normalised\n",
    "                    #changed from 1 to 2\n",
    "                    #self.reward += 2\n",
    "\n",
    "                    #adding score reward for progressing from game mode\n",
    "                    #changed to 4 from 2\n",
    "                    #self.SCORE += 4\n",
    "\n",
    "                    #TESTING if agent gets reward = current cumulative reward in each mode (so basically reset to 0 or times 2 if -ve or +ve)\n",
    "                    #might need to change this to just adding like 20 as a one step reward if the agent passes into a new stage\n",
    "                    #self.reward += 100\n",
    "        \n",
    "        #commented out as left and right not used anymore\n",
    "        #elif action == 2 or action == 3:\n",
    "            #Experimenting reward function\n",
    "            #if left or right used\n",
    "            #changed to -20\n",
    "            #self.reward -= 5\n",
    "            #self.reward -= 20\n",
    "\n",
    "            #testing this normalised\n",
    "            #self.reward -= 1\n",
    "\n",
    "\n",
    "    def spectrum_logic(self, action):\n",
    "        # if backspace is pressed go back to topology mode\n",
    "        # should go back to node before destination node\n",
    "        # selected links should be deselected\n",
    "        # automatically highlight links\n",
    "        # removes the links from user selected links\n",
    "        #changed to 3 from 5 for new backsapce action value\n",
    "        if action == 3:\n",
    "            self.topologyMode = True\n",
    "            self.spectrumMode = False\n",
    "\n",
    "            #linksSelected = [link[1] for link in self.user.getLinksSelected()]\n",
    "            links_selected = self.user.getLinksSelected()\n",
    "            highlightedSpectrum = [0]*NUMBEROFSLOTS\n",
    "            #for link in linksSelected:\n",
    "            #    link.setSpectrumHighlighted(highlightedSpectrum)\n",
    "\n",
    "            links_selected[-1][1].setSpectrumHighlighted(highlightedSpectrum)\n",
    "\n",
    "            #links_selected = self.user.getLinksSelected()\n",
    "\n",
    "            self.user.setCurrentNode(links_selected[-1][0])\n",
    "            links_selected[-1][1].setSelected(False)\n",
    "            self.user.getCurrentRequest().getDestNode().setSelected(False)\n",
    "            availableLinks = self.checkAvailable()\n",
    "            availableLinks[self.index][0].setHighlighted(True)\n",
    "            availableLinks[self.index][1].setHighlighted(True)\n",
    "            self.user.getLinksSelected().remove(links_selected[-1])\n",
    "\n",
    "            #returning to topology mode\n",
    "            #Experimenting reward function\n",
    "            #changed to -20\n",
    "            #self.reward -= 1\n",
    "            #self.reward -= 20\n",
    "\n",
    "            #testing this normalised\n",
    "            self.reward -= 3\n",
    "\n",
    "            #setting visual changes to the score screen set to -5 for now\n",
    "            self.SCORE -= 5\n",
    "            \n",
    "\n",
    "        # if left is pressed then the selected should be shifted to the left by 1 unless at the most left where it will jump to right\n",
    "        #LEFT CHANGED TO UP action is now 0 from 2\n",
    "        elif action == 0:\n",
    "            bandwidth = self.user.getCurrentRequest().getBandwidth()\n",
    "            if self.spectrumIndex == 0:\n",
    "                self.spectrumIndex = self.NUMBEROFSLOTS - bandwidth\n",
    "            else:\n",
    "                self.spectrumIndex -= 1\n",
    "            highlightedSpectrum = [0]*5\n",
    "            linksSelected = [link[1] for link in self.user.getLinksSelected()]\n",
    "            for i in range(bandwidth):\n",
    "                highlightedSpectrum[i + self.spectrumIndex] = 1\n",
    "            for link in linksSelected:\n",
    "                link.setSpectrumHighlighted(highlightedSpectrum)\n",
    "\n",
    "            #Experimenting reward function\n",
    "            #unchanged\n",
    "            #self.reward -= 1\n",
    "\n",
    "            #testing this normalised\n",
    "            self.reward += 1\n",
    "\n",
    "\n",
    "        # if right is pressed then the selected should be shifted to the right by 1 unless at the most right where it will jump to left\n",
    "        #RIGHT CHANGED TO down action is now 1 from 2\n",
    "        elif action == 1:\n",
    "            bandwidth = self.user.getCurrentRequest().getBandwidth()\n",
    "            if self.spectrumIndex == self.NUMBEROFSLOTS - bandwidth:\n",
    "                self.spectrumIndex = 0\n",
    "            else:\n",
    "                self.spectrumIndex += 1\n",
    "            highlightedSpectrum = [0]*5\n",
    "            linksSelected = [link[1] for link in self.user.getLinksSelected()]\n",
    "            for i in range(bandwidth):\n",
    "                highlightedSpectrum[i + self.spectrumIndex] = 1\n",
    "            for link in linksSelected:\n",
    "                link.setSpectrumHighlighted(highlightedSpectrum)\n",
    "            \n",
    "            #Experimenting reward function\n",
    "            #unchanged\n",
    "            #self.reward -= 1\n",
    "\n",
    "            #testing this normalised\n",
    "            self.reward += 1\n",
    "\n",
    "        # if return is pressed, selected links should be checked for if they are valid and if they are they should be selected and links\n",
    "        # should be updated\n",
    "        # otherwise an error message should pop up\n",
    "        #changed from 4 to 2 for enter\n",
    "        elif action == 2:\n",
    "            # check that there are no conflicts\n",
    "            linksSelected = [link[1] for link in self.user.getLinksSelected()]\n",
    "            possible = True\n",
    "            for link in linksSelected:\n",
    "                for i in range(self.NUMBEROFSLOTS):\n",
    "                    if link.getSpectrumHighlighted()[i] == 1:\n",
    "                        if link.getSpectrum()[i] == 1:\n",
    "                            # create error screen\n",
    "                            # self.DISPLAYSURF.fill(self.RED)\n",
    "                            # pygame.display.update()\n",
    "                            # print(\"error\")\n",
    "                            possible = False\n",
    "                            \n",
    "                            \n",
    "\n",
    "            if possible == True:\n",
    "                self.completions.append((self.user.getCurrentRequest(), self.user.getLinksSelected().copy(), link.getSpectrumHighlighted().copy()))\n",
    "                for link in linksSelected:\n",
    "                    newSelected = [sum(x) for x in zip(link.getSpectrum(), link.getSpectrumHighlighted())]\n",
    "                    link.setSpectrum(newSelected)\n",
    "                    highlightedSpectrum = [0]*5\n",
    "                    link.setSpectrumHighlighted(highlightedSpectrum)\n",
    "                # throw back into request mode and add point and deselect highlighted spectrum, remove request\n",
    "                \n",
    "                #COMMENTED OUT, TESTING SCORE PRINTOUT\n",
    "                #set to 10 from +1\n",
    "                self.SCORE += 10\n",
    "                \n",
    "                self.user.getCurrentRequest().complete()\n",
    "                self.activeRequests.remove(self.user.getCurrentRequest())\n",
    "                self.user.getCurrentRequest().setTimeAllocated(self.timer)\n",
    "                availableLinks = self.clearAll()\n",
    "\n",
    "                #successful spectrum assignment (one connection serviced)\n",
    "                #experimenting reward function\n",
    "                #changed to + 30\n",
    "                #self.reward += 100\n",
    "                #self.reward += 30\n",
    "\n",
    "                #testing this normalised\n",
    "                #set to +10 from 1\n",
    "                self.reward += 5\n",
    "                \n",
    "                #TESTING if agent gets reward = current cumulative reward in each mode (so basically reset to 0 or times 2 if -ve or +ve)\n",
    "                #might need to change this to just adding like 20 as a one step reward if the agent passes into a new stage\n",
    "                #self.reward += 100\n",
    "                \n",
    "\n",
    "                #adding to the req_complete flag \n",
    "                self.req_complete += 1\n",
    "\n",
    "                if self.req_complete == self.req_num:\n",
    "                    #changed so that if the number of requests is serviced then the episode ends\n",
    "                    #debug\n",
    "                    print(\"End episode, cumulative Reward:\")\n",
    "                    print(self.reward_sum)\n",
    "                    #CHANGE THIS SO THAT IT SUCCESSFULLY ENDS THE GAME UPON CONNECTION SERVICED\n",
    "                    self.done = True\n",
    "            \n",
    "            else:\n",
    "                #invalid spectrum assignment\n",
    "                #Experimenting reward function\n",
    "                #changed to - 10\n",
    "                #self.reward -= 5\n",
    "                #self.reward -= 30\n",
    "\n",
    "                #testing this normalised\n",
    "                self.reward -= 50\n",
    "                #setting such that score is minused\n",
    "                self.SCORE -= 1\n",
    "\n",
    "        #commented out as they are not used now\n",
    "        #elif action == 0 or action == 1:\n",
    "            #if up or down are used (irrelevant controls in this mode)\n",
    "            #experimenting reward function \n",
    "            #changed to -20\n",
    "            #self.reward -= 5\n",
    "            #self.reward -= 20\n",
    "\n",
    "            #testing this normalised\n",
    "            #self.reward -= 1\n",
    "\n",
    "    def checkAvailable(self):\n",
    "        '''\n",
    "        Function to check whether links have been selected and removes from possible routes.\n",
    "        This means that the user will not be able to select links that have already been selected.\n",
    "        Used in clearAll()\n",
    "        '''\n",
    "        availableLinks = []\n",
    "        for entry in self.user.getCurrentNode().getLinks():\n",
    "            if (entry[1].getSelected() == False or entry[0].getSelected() == False) and entry[0].getSource() == False:\n",
    "                availableLinks.append(entry)\n",
    "                \n",
    "        return availableLinks\n",
    "    \n",
    "    \n",
    "    def clearAll(self):\n",
    "        '''\n",
    "        Clears all previously selected nodes and links.\n",
    "        Used in requestUpdate()\n",
    "        '''\n",
    "        self.user.getLinksSelected().clear()\n",
    "        availableLinks = self.checkAvailable()\n",
    "        # IF user has selected a request and is still trying to service the request when the request expired\n",
    "        # THEN the request is deselected, progress in servicing it will be reset, \n",
    "        # user then needs to choose another request\n",
    "        \n",
    "        # the request is deselcted automatically since it has expired\n",
    "        self.user.deselectRequest()\n",
    "        # nodes and links that user has selected or is selecting will be removed\n",
    "        highlighted = [0]*self.NUMBEROFSLOTS\n",
    "        for node in self.nodeList:\n",
    "            node.setHighlighted(False)\n",
    "            node.setSelected(False)\n",
    "        for link in self.linkList:\n",
    "            link.setHighlighted(False)\n",
    "            link.setSelected(False)\n",
    "            link.setSpectrumHighlighted(highlighted)\n",
    "        # user is returned to request mode\n",
    "        self.requestMode = True\n",
    "        self.topologyMode = False\n",
    "        self.spectrumMode = False\n",
    "        return availableLinks\n",
    "\n",
    "\n",
    "    def endGame(self):\n",
    "        '''\n",
    "        Function to end the game by quitting Pygame and exiting system.\n",
    "        Used in render()\n",
    "        '''\n",
    "        pygame.quit()\n",
    "        sys.exit()\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################################\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating fixed test topology\n",
    "def createTestTopology():\n",
    "    # testNodes\n",
    "    nodeA = Node(0, 'A', 300, 200)\n",
    "    nodeB = Node(1, 'B', 300, 400)\n",
    "    nodeC = Node(2, 'C', 650, 200)\n",
    "    nodeD = Node(3, 'D', 650, 400)\n",
    "    # testLinks\n",
    "    link1 = Link(0, nodeA, nodeB)\n",
    "    link2 = Link(1, nodeB, nodeC)\n",
    "    link3 = Link(2, nodeB, nodeD)\n",
    "    link4 = Link(3, nodeA, nodeC)\n",
    "    link5 = Link(4, nodeC, nodeD)\n",
    "\n",
    "    nodeList = [nodeA, nodeB, nodeC, nodeD]\n",
    "    linkList = [link1, link2, link3, link4, link5]\n",
    "\n",
    "    # save the links associated to each node in a list\n",
    "    for node in nodeList:\n",
    "        node.setLinks(linkList)\n",
    "    return nodeList, linkList\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create log dir for saving model\n",
    "log_dir = os.path.join(os.getcwd(), \"tmp/\")\n",
    "#os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Create log dir for tensorboard log\n",
    "log_tensorboard = os.path.join(os.getcwd(), \"tensorboard_logs/\")\n",
    "os.makedirs(log_tensorboard, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "(256, 256, 3)\n",
      "Using cuda device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:221: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 3.93GB > 1.58GB\n",
      "  \"This system does not have apparently enough memory to store the complete \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256, 3)\n",
      "Logging to ./tensorboard_logs/DQN_tensor_5\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "-3341.1999999999507\n",
      "(256, 256, 3)\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "-3127.1999999999525\n",
      "(256, 256, 3)\n",
      "Num timesteps: 10000\n",
      "Best mean reward: -inf - Last mean reward per episode: -3229.20\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "2 links\n",
      "2\n",
      "7\n",
      "2 links\n",
      "2\n",
      "9\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "6\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "4\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "1 link\n",
      "1\n",
      "6\n",
      "1 link\n",
      "1\n",
      "8\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "1 link\n",
      "1\n",
      "6\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "4\n",
      "1 link\n",
      "1\n",
      "6\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "1 link\n",
      "1\n",
      "6\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "2 links\n",
      "2\n",
      "7\n",
      "2 links\n",
      "2\n",
      "9\n",
      "2 links\n",
      "2\n",
      "11\n",
      "2 links\n",
      "2\n",
      "14\n",
      "2 links\n",
      "2\n",
      "16\n",
      "2 links\n",
      "2\n",
      "22\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "6\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "-855.2999999999968\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "-2530.800000000001\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 3.88e+03  |\n",
      "|    ep_rew_mean      | -2.46e+03 |\n",
      "|    exploration_rate | 0.984     |\n",
      "| time/               |           |\n",
      "|    episodes         | 4         |\n",
      "|    fps              | 121       |\n",
      "|    time_elapsed     | 127       |\n",
      "|    total_timesteps  | 15540     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 1.15      |\n",
      "|    n_updates        | 2         |\n",
      "-----------------------------------\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "5\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "Num timesteps: 20000\n",
      "Best mean reward: -3229.20 - Last mean reward per episode: -2458.62\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-2897.399999999976\n",
      "(256, 256, 3)\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "1 link\n",
      "1\n",
      "6\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "-3215.4999999999586\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "4\n",
      "1 link\n",
      "1\n",
      "6\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "Num timesteps: 30000\n",
      "Best mean reward: -2458.62 - Last mean reward per episode: -2656.23\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "-3288.0999999999513\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "-3157.999999999955\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.39e+03 |\n",
      "|    ep_rew_mean      | -2.8e+03 |\n",
      "|    exploration_rate | 0.965    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 138      |\n",
      "|    time_elapsed     | 253      |\n",
      "|    total_timesteps  | 35123    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.337    |\n",
      "|    n_updates        | 12       |\n",
      "----------------------------------\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "-2869.1999999999534\n",
      "(256, 256, 3)\n",
      "Num timesteps: 40000\n",
      "Best mean reward: -2458.62 - Last mean reward per episode: -2804.19\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "1 link\n",
      "1\n",
      "6\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "-2715.599999999975\n",
      "(256, 256, 3)\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "-2855.299999999963\n",
      "(256, 256, 3)\n",
      "1 link\n",
      "1\n",
      "2\n",
      "Num timesteps: 50000\n",
      "Best mean reward: -2458.62 - Last mean reward per episode: -2799.87\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-3015.2999999999547\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4.52e+03  |\n",
      "|    ep_rew_mean      | -2.82e+03 |\n",
      "|    exploration_rate | 0.946     |\n",
      "| time/               |           |\n",
      "|    episodes         | 12        |\n",
      "|    fps              | 143       |\n",
      "|    time_elapsed     | 378       |\n",
      "|    total_timesteps  | 54189     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 0.322     |\n",
      "|    n_updates        | 22        |\n",
      "-----------------------------------\n",
      "2 links\n",
      "2\n",
      "4\n",
      "1 link\n",
      "1\n",
      "5\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-3005.0999999999503\n",
      "(256, 256, 3)\n",
      "1 link\n",
      "1\n",
      "2\n",
      "Num timesteps: 60000\n",
      "Best mean reward: -2458.62 - Last mean reward per episode: -2831.46\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "2 links\n",
      "2\n",
      "3\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "7\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-2936.2999999999593\n",
      "(256, 256, 3)\n",
      "2 links\n",
      "2\n",
      "3\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "-2859.7999999999593\n",
      "(256, 256, 3)\n",
      "2 links\n",
      "2\n",
      "3\n",
      "Num timesteps: 70000\n",
      "Best mean reward: -2458.62 - Last mean reward per episode: -2839.67\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-2887.999999999956\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4.62e+03  |\n",
      "|    ep_rew_mean      | -2.84e+03 |\n",
      "|    exploration_rate | 0.926     |\n",
      "| time/               |           |\n",
      "|    episodes         | 16        |\n",
      "|    fps              | 146       |\n",
      "|    time_elapsed     | 503       |\n",
      "|    total_timesteps  | 73870     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 0.298     |\n",
      "|    n_updates        | 31        |\n",
      "-----------------------------------\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "2 links\n",
      "2\n",
      "7\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-2843.9999999999595\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "Num timesteps: 80000\n",
      "Best mean reward: -2458.62 - Last mean reward per episode: -2842.18\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-2916.2999999999683\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "-2828.8999999999614\n",
      "(256, 256, 3)\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "Num timesteps: 90000\n",
      "Best mean reward: -2458.62 - Last mean reward per episode: -2844.86\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "End episode, cumulative Reward:\n",
      "-2723.6999999999703\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4.67e+03  |\n",
      "|    ep_rew_mean      | -2.84e+03 |\n",
      "|    exploration_rate | 0.907     |\n",
      "| time/               |           |\n",
      "|    episodes         | 20        |\n",
      "|    fps              | 148       |\n",
      "|    time_elapsed     | 628       |\n",
      "|    total_timesteps  | 93361     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 0.246     |\n",
      "|    n_updates        | 41        |\n",
      "-----------------------------------\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "5\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "6\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-2691.199999999968\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "Num timesteps: 100000\n",
      "Best mean reward: -2458.62 - Last mean reward per episode: -2831.30\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "1 link\n",
      "1\n",
      "6\n",
      "1 link\n",
      "1\n",
      "8\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-2658.4999999999723\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-2764.4999999999654\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "Num timesteps: 110000\n",
      "Best mean reward: -2458.62 - Last mean reward per episode: -2820.44\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "-2661.19999999997\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4.7e+03   |\n",
      "|    ep_rew_mean      | -2.81e+03 |\n",
      "|    exploration_rate | 0.887     |\n",
      "| time/               |           |\n",
      "|    episodes         | 24        |\n",
      "|    fps              | 149       |\n",
      "|    time_elapsed     | 753       |\n",
      "|    total_timesteps  | 112717    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 0.173     |\n",
      "|    n_updates        | 51        |\n",
      "-----------------------------------\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "-2697.49999999997\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "4\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "Num timesteps: 120000\n",
      "Best mean reward: -2458.62 - Last mean reward per episode: -2808.76\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "1 link\n",
      "1\n",
      "6\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-2620.5999999999813\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "-2682.0999999999767\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "Num timesteps: 130000\n",
      "Best mean reward: -2458.62 - Last mean reward per episode: -2796.73\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "-2596.899999999974\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4.73e+03  |\n",
      "|    ep_rew_mean      | -2.79e+03 |\n",
      "|    exploration_rate | 0.868     |\n",
      "| time/               |           |\n",
      "|    episodes         | 28        |\n",
      "|    fps              | 150       |\n",
      "|    time_elapsed     | 878       |\n",
      "|    total_timesteps  | 132372    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 0.144     |\n",
      "|    n_updates        | 61        |\n",
      "-----------------------------------\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-2594.4999999999804\n",
      "(256, 256, 3)\n",
      "2 links\n",
      "2\n",
      "4\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "Num timesteps: 140000\n",
      "Best mean reward: -2458.62 - Last mean reward per episode: -2782.52\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-2490.1999999999985\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "2 links\n",
      "2\n",
      "7\n",
      "2 links\n",
      "2\n",
      "9\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "-2437.699999999994\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "Num timesteps: 150000\n",
      "Best mean reward: -2458.62 - Last mean reward per episode: -2761.64\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-2365.5000000000055\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4.74e+03  |\n",
      "|    ep_rew_mean      | -2.75e+03 |\n",
      "|    exploration_rate | 0.848     |\n",
      "| time/               |           |\n",
      "|    episodes         | 32        |\n",
      "|    fps              | 151       |\n",
      "|    time_elapsed     | 1003      |\n",
      "|    total_timesteps  | 151616    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 0.0911    |\n",
      "|    n_updates        | 70        |\n",
      "-----------------------------------\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-2501.9999999999927\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "1 link\n",
      "1\n",
      "2\n",
      "Num timesteps: 160000\n",
      "Best mean reward: -2458.62 - Last mean reward per episode: -2741.47\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "-2450.600000000004\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-2325.8000000000093\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "Num timesteps: 170000\n",
      "Best mean reward: -2458.62 - Last mean reward per episode: -2720.99\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "-2193.600000000022\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4.73e+03  |\n",
      "|    ep_rew_mean      | -2.71e+03 |\n",
      "|    exploration_rate | 0.83      |\n",
      "| time/               |           |\n",
      "|    episodes         | 36        |\n",
      "|    fps              | 151       |\n",
      "|    time_elapsed     | 1128      |\n",
      "|    total_timesteps  | 170432    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 0.128     |\n",
      "|    n_updates        | 80        |\n",
      "-----------------------------------\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "1 link\n",
      "1\n",
      "6\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-2400.800000000004\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "-2268.7000000000253\n",
      "(256, 256, 3)\n",
      "Num timesteps: 180000\n",
      "Best mean reward: -2458.62 - Last mean reward per episode: -2686.39\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-2237.600000000032\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-2210.60000000003\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4.73e+03  |\n",
      "|    ep_rew_mean      | -2.66e+03 |\n",
      "|    exploration_rate | 0.811     |\n",
      "| time/               |           |\n",
      "|    episodes         | 40        |\n",
      "|    fps              | 150       |\n",
      "|    time_elapsed     | 1253      |\n",
      "|    total_timesteps  | 189005    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 0.0281    |\n",
      "|    n_updates        | 89        |\n",
      "-----------------------------------\n",
      "1 link\n",
      "1\n",
      "2\n",
      "Num timesteps: 190000\n",
      "Best mean reward: -2458.62 - Last mean reward per episode: -2663.03\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "1 link\n",
      "1\n",
      "6\n",
      "1 link\n",
      "1\n",
      "8\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-2143.9000000000415\n",
      "(256, 256, 3)\n",
      "2 links\n",
      "2\n",
      "4\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "1 link\n",
      "1\n",
      "6\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "-2289.700000000018\n",
      "(256, 256, 3)\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "1 link\n",
      "1\n",
      "6\n",
      "2 links\n",
      "2\n",
      "3\n",
      "Num timesteps: 200000\n",
      "Best mean reward: -2458.62 - Last mean reward per episode: -2641.54\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-2323.7000000000057\n",
      "(256, 256, 3)\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "2 links\n",
      "2\n",
      "7\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-2149.0000000000414\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4.72e+03  |\n",
      "|    ep_rew_mean      | -2.62e+03 |\n",
      "|    exploration_rate | 0.792     |\n",
      "| time/               |           |\n",
      "|    episodes         | 44        |\n",
      "|    fps              | 150       |\n",
      "|    time_elapsed     | 1377      |\n",
      "|    total_timesteps  | 207853    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 0.102     |\n",
      "|    n_updates        | 98        |\n",
      "-----------------------------------\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "Num timesteps: 210000\n",
      "Best mean reward: -2458.62 - Last mean reward per episode: -2622.90\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "End episode, cumulative Reward:\n",
      "-2229.8000000000384\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "2 links\n",
      "2\n",
      "7\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-2096.2000000000476\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "Num timesteps: 220000\n",
      "Best mean reward: -2458.62 - Last mean reward per episode: -2602.68\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "2 links\n",
      "2\n",
      "7\n",
      "End episode, cumulative Reward:\n",
      "-2040.400000000058\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "-2039.0000000000587\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4.71e+03  |\n",
      "|    ep_rew_mean      | -2.58e+03 |\n",
      "|    exploration_rate | 0.774     |\n",
      "| time/               |           |\n",
      "|    episodes         | 48        |\n",
      "|    fps              | 150       |\n",
      "|    time_elapsed     | 1502      |\n",
      "|    total_timesteps  | 226247    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 0.0423    |\n",
      "|    n_updates        | 108       |\n",
      "-----------------------------------\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "Num timesteps: 230000\n",
      "Best mean reward: -2458.62 - Last mean reward per episode: -2579.02\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-2031.8000000000584\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-2109.6000000000604\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "Num timesteps: 240000\n",
      "Best mean reward: -2458.62 - Last mean reward per episode: -2558.48\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "-2023.7000000000587\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-2008.7000000000598\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4.71e+03  |\n",
      "|    ep_rew_mean      | -2.54e+03 |\n",
      "|    exploration_rate | 0.755     |\n",
      "| time/               |           |\n",
      "|    episodes         | 52        |\n",
      "|    fps              | 150       |\n",
      "|    time_elapsed     | 1626      |\n",
      "|    total_timesteps  | 244981    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 0.00391   |\n",
      "|    n_updates        | 117       |\n",
      "-----------------------------------\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "7\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-2082.4000000000597\n",
      "(256, 256, 3)\n",
      "1 link\n",
      "1\n",
      "2\n",
      "Num timesteps: 250000\n",
      "Best mean reward: -2458.62 - Last mean reward per episode: -2528.75\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "4\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "1 link\n",
      "1\n",
      "6\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-1980.8000000000638\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-1967.2000000000648\n",
      "(256, 256, 3)\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "Num timesteps: 260000\n",
      "Best mean reward: -2458.62 - Last mean reward per episode: -2508.40\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "-1900.600000000062\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.71e+03 |\n",
      "|    ep_rew_mean      | -2.5e+03 |\n",
      "|    exploration_rate | 0.736    |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 150      |\n",
      "|    time_elapsed     | 1751     |\n",
      "|    total_timesteps  | 263756   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00589  |\n",
      "|    n_updates        | 126      |\n",
      "----------------------------------\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "5\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "End episode, cumulative Reward:\n",
      "-1878.9000000000667\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "Num timesteps: 270000\n",
      "Best mean reward: -2458.62 - Last mean reward per episode: -2486.52\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-1931.900000000064\n",
      "(256, 256, 3)\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "3\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-1766.40000000006\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "Num timesteps: 280000\n",
      "Best mean reward: -2458.62 - Last mean reward per episode: -2464.74\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "-1795.4000000000642\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4.71e+03  |\n",
      "|    ep_rew_mean      | -2.45e+03 |\n",
      "|    exploration_rate | 0.718     |\n",
      "| time/               |           |\n",
      "|    episodes         | 60        |\n",
      "|    fps              | 150       |\n",
      "|    time_elapsed     | 1875      |\n",
      "|    total_timesteps  | 282353    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 0.0248    |\n",
      "|    n_updates        | 136       |\n",
      "-----------------------------------\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "End episode, cumulative Reward:\n",
      "-1825.9000000000663\n",
      "(256, 256, 3)\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "7\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "Num timesteps: 290000\n",
      "Best mean reward: -2458.62 - Last mean reward per episode: -2443.13\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "9\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-1692.2000000000592\n",
      "(256, 256, 3)\n",
      "2 links\n",
      "2\n",
      "3\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-1779.3000000000632\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "Num timesteps: 300000\n",
      "Best mean reward: -2443.13 - Last mean reward per episode: -2420.52\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "2 links\n",
      "2\n",
      "3\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-1823.4000000000597\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4.7e+03   |\n",
      "|    ep_rew_mean      | -2.41e+03 |\n",
      "|    exploration_rate | 0.699     |\n",
      "| time/               |           |\n",
      "|    episodes         | 64        |\n",
      "|    fps              | 150       |\n",
      "|    time_elapsed     | 2000      |\n",
      "|    total_timesteps  | 300555    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 0.158     |\n",
      "|    n_updates        | 145       |\n",
      "-----------------------------------\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-1607.5000000000628\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-1705.300000000065\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "Num timesteps: 310000\n",
      "Best mean reward: -2420.52 - Last mean reward per episode: -2388.09\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-1598.8000000000563\n",
      "(256, 256, 3)\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-1572.2000000000553\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4.68e+03  |\n",
      "|    ep_rew_mean      | -2.36e+03 |\n",
      "|    exploration_rate | 0.681     |\n",
      "| time/               |           |\n",
      "|    episodes         | 68        |\n",
      "|    fps              | 149       |\n",
      "|    time_elapsed     | 2124      |\n",
      "|    total_timesteps  | 318544    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 0.0373    |\n",
      "|    n_updates        | 154       |\n",
      "-----------------------------------\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "Num timesteps: 320000\n",
      "Best mean reward: -2388.09 - Last mean reward per episode: -2364.34\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-1497.6000000000552\n",
      "(256, 256, 3)\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-1600.8000000000604\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "1 link\n",
      "1\n",
      "2\n",
      "Num timesteps: 330000\n",
      "Best mean reward: -2364.34 - Last mean reward per episode: -2340.90\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "-1523.0000000000628\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "1 link\n",
      "1\n",
      "6\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-1526.400000000057\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4.67e+03  |\n",
      "|    ep_rew_mean      | -2.32e+03 |\n",
      "|    exploration_rate | 0.664     |\n",
      "| time/               |           |\n",
      "|    episodes         | 72        |\n",
      "|    fps              | 149       |\n",
      "|    time_elapsed     | 2249      |\n",
      "|    total_timesteps  | 336201    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 0.00244   |\n",
      "|    n_updates        | 163       |\n",
      "-----------------------------------\n",
      "2 links\n",
      "2\n",
      "4\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "more than 3 links\n",
      "Num timesteps: 340000\n",
      "Best mean reward: -2340.90 - Last mean reward per episode: -2318.09\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-1494.2000000000603\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-1519.100000000061\n",
      "(256, 256, 3)\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "-1547.200000000064\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "Num timesteps: 350000\n",
      "Best mean reward: -2318.09 - Last mean reward per episode: -2285.97\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "6\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "7\n",
      "2 links\n",
      "2\n",
      "9\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "-1455.3000000000575\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4.66e+03  |\n",
      "|    ep_rew_mean      | -2.27e+03 |\n",
      "|    exploration_rate | 0.646     |\n",
      "| time/               |           |\n",
      "|    episodes         | 76        |\n",
      "|    fps              | 149       |\n",
      "|    time_elapsed     | 2373      |\n",
      "|    total_timesteps  | 354100    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 0.0103    |\n",
      "|    n_updates        | 172       |\n",
      "-----------------------------------\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-1477.9000000000633\n",
      "(256, 256, 3)\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "Num timesteps: 360000\n",
      "Best mean reward: -2285.97 - Last mean reward per episode: -2264.56\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-1437.4000000000583\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-1445.8000000000643\n",
      "(256, 256, 3)\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "Num timesteps: 370000\n",
      "Best mean reward: -2264.56 - Last mean reward per episode: -2243.60\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "-1364.5000000000605\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4.65e+03  |\n",
      "|    ep_rew_mean      | -2.23e+03 |\n",
      "|    exploration_rate | 0.628     |\n",
      "| time/               |           |\n",
      "|    episodes         | 80        |\n",
      "|    fps              | 148       |\n",
      "|    time_elapsed     | 2498      |\n",
      "|    total_timesteps  | 371964    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 0.00307   |\n",
      "|    n_updates        | 180       |\n",
      "-----------------------------------\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-1334.2000000000614\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "Num timesteps: 380000\n",
      "Best mean reward: -2243.60 - Last mean reward per episode: -2221.40\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-1276.0000000000532\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "5\n",
      "End episode, cumulative Reward:\n",
      "-1353.4000000000603\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-1318.7000000000608\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4.64e+03  |\n",
      "|    ep_rew_mean      | -2.19e+03 |\n",
      "|    exploration_rate | 0.61      |\n",
      "| time/               |           |\n",
      "|    episodes         | 84        |\n",
      "|    fps              | 148       |\n",
      "|    time_elapsed     | 2622      |\n",
      "|    total_timesteps  | 389866    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 0.0023    |\n",
      "|    n_updates        | 189       |\n",
      "-----------------------------------\n",
      "Num timesteps: 390000\n",
      "Best mean reward: -2221.40 - Last mean reward per episode: -2188.88\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "2 links\n",
      "2\n",
      "6\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "15\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "2 links\n",
      "2\n",
      "7\n",
      "1 link\n",
      "1\n",
      "8\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "-1783.800000000075\n",
      "(256, 256, 3)\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "2 links\n",
      "2\n",
      "3\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-1241.400000000057\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "Num timesteps: 400000\n",
      "Best mean reward: -2188.88 - Last mean reward per episode: -2173.04\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "2 links\n",
      "2\n",
      "6\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-1224.3000000000525\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-1226.3000000000527\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4.63e+03  |\n",
      "|    ep_rew_mean      | -2.15e+03 |\n",
      "|    exploration_rate | 0.593     |\n",
      "| time/               |           |\n",
      "|    episodes         | 88        |\n",
      "|    fps              | 148       |\n",
      "|    time_elapsed     | 2747      |\n",
      "|    total_timesteps  | 407361    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 0.142     |\n",
      "|    n_updates        | 198       |\n",
      "-----------------------------------\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "Num timesteps: 410000\n",
      "Best mean reward: -2173.04 - Last mean reward per episode: -2151.39\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-1188.800000000054\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "2 links\n",
      "2\n",
      "3\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "7\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-1151.0000000000553\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "more than 3 links\n",
      "Num timesteps: 420000\n",
      "Best mean reward: -2151.39 - Last mean reward per episode: -2129.47\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-1085.1000000000483\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-1195.400000000058\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4.62e+03  |\n",
      "|    ep_rew_mean      | -2.11e+03 |\n",
      "|    exploration_rate | 0.575     |\n",
      "| time/               |           |\n",
      "|    episodes         | 92        |\n",
      "|    fps              | 148       |\n",
      "|    time_elapsed     | 2871      |\n",
      "|    total_timesteps  | 424972    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 0.0122    |\n",
      "|    n_updates        | 207       |\n",
      "-----------------------------------\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-1102.0000000000525\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "Num timesteps: 430000\n",
      "Best mean reward: -2129.47 - Last mean reward per episode: -2096.98\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-1114.1000000000547\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-1015.1000000000438\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "Num timesteps: 440000\n",
      "Best mean reward: -2096.98 - Last mean reward per episode: -2075.14\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "5\n",
      "End episode, cumulative Reward:\n",
      "-929.6000000000425\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4.61e+03  |\n",
      "|    ep_rew_mean      | -2.06e+03 |\n",
      "|    exploration_rate | 0.558     |\n",
      "| time/               |           |\n",
      "|    episodes         | 96        |\n",
      "|    fps              | 147       |\n",
      "|    time_elapsed     | 2995      |\n",
      "|    total_timesteps  | 442248    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 0.145     |\n",
      "|    n_updates        | 216       |\n",
      "-----------------------------------\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "2 links\n",
      "2\n",
      "7\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-982.5000000000555\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "more than 3 links\n",
      "Num timesteps: 450000\n",
      "Best mean reward: -2075.14 - Last mean reward per episode: -2051.97\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-933.6000000000433\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-1014.6000000000436\n",
      "(256, 256, 3)\n",
      "2 links\n",
      "2\n",
      "10\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "-909.2000000000353\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4.59e+03  |\n",
      "|    ep_rew_mean      | -2.02e+03 |\n",
      "|    exploration_rate | 0.541     |\n",
      "| time/               |           |\n",
      "|    episodes         | 100       |\n",
      "|    fps              | 147       |\n",
      "|    time_elapsed     | 3120      |\n",
      "|    total_timesteps  | 459237    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 0.0114    |\n",
      "|    n_updates        | 224       |\n",
      "-----------------------------------\n",
      "more than 3 links\n",
      "Num timesteps: 460000\n",
      "Best mean reward: -2051.97 - Last mean reward per episode: -2018.83\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-969.2000000000485\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "-964.7000000000425\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "Num timesteps: 470000\n",
      "Best mean reward: -2018.83 - Last mean reward per episode: -1973.49\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "-899.4000000000347\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-862.8000000000368\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4.61e+03  |\n",
      "|    ep_rew_mean      | -1.96e+03 |\n",
      "|    exploration_rate | 0.523     |\n",
      "| time/               |           |\n",
      "|    episodes         | 104       |\n",
      "|    fps              | 146       |\n",
      "|    time_elapsed     | 3244      |\n",
      "|    total_timesteps  | 476529    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 0.000445  |\n",
      "|    n_updates        | 233       |\n",
      "-----------------------------------\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "1 link\n",
      "1\n",
      "2\n",
      "Num timesteps: 480000\n",
      "Best mean reward: -1973.49 - Last mean reward per episode: -1957.25\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-856.4000000000322\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-821.200000000031\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-879.7000000000339\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "Num timesteps: 490000\n",
      "Best mean reward: -1957.25 - Last mean reward per episode: -1888.81\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-741.1000000000145\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4.58e+03  |\n",
      "|    ep_rew_mean      | -1.86e+03 |\n",
      "|    exploration_rate | 0.507     |\n",
      "| time/               |           |\n",
      "|    episodes         | 108       |\n",
      "|    fps              | 146       |\n",
      "|    time_elapsed     | 3369      |\n",
      "|    total_timesteps  | 493187    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 0.00104   |\n",
      "|    n_updates        | 241       |\n",
      "-----------------------------------\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "5\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-766.4000000000177\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "Num timesteps: 500000\n",
      "Best mean reward: -1888.81 - Last mean reward per episode: -1843.61\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-763.2000000000207\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-705.7000000000145\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-814.9000000000335\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4.56e+03  |\n",
      "|    ep_rew_mean      | -1.78e+03 |\n",
      "|    exploration_rate | 0.49      |\n",
      "| time/               |           |\n",
      "|    episodes         | 112       |\n",
      "|    fps              | 145       |\n",
      "|    time_elapsed     | 3493      |\n",
      "|    total_timesteps  | 509793    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 0.00796   |\n",
      "|    n_updates        | 249       |\n",
      "-----------------------------------\n",
      "1 link\n",
      "1\n",
      "2\n",
      "Num timesteps: 510000\n",
      "Best mean reward: -1843.61 - Last mean reward per episode: -1780.59\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "-662.2000000000075\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-655.5000000000149\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "3\n",
      "Num timesteps: 520000\n",
      "Best mean reward: -1780.59 - Last mean reward per episode: -1734.35\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-665.4000000000029\n",
      "(256, 256, 3)\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-619.1000000000047\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4.52e+03  |\n",
      "|    ep_rew_mean      | -1.69e+03 |\n",
      "|    exploration_rate | 0.474     |\n",
      "| time/               |           |\n",
      "|    episodes         | 116       |\n",
      "|    fps              | 145       |\n",
      "|    time_elapsed     | 3617      |\n",
      "|    total_timesteps  | 526300    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 0.0179    |\n",
      "|    n_updates        | 258       |\n",
      "-----------------------------------\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "Num timesteps: 530000\n",
      "Best mean reward: -1734.35 - Last mean reward per episode: -1689.72\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "End episode, cumulative Reward:\n",
      "-665.4000000000078\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "2 links\n",
      "2\n",
      "7\n",
      "2 links\n",
      "2\n",
      "9\n",
      "2 links\n",
      "2\n",
      "4\n",
      "2 links\n",
      "2\n",
      "6\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "-726.6000000000195\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "3\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-597.1999999999969\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "Num timesteps: 540000\n",
      "Best mean reward: -1689.72 - Last mean reward per episode: -1623.72\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "-523.5999999999826\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.5e+03  |\n",
      "|    ep_rew_mean      | -1.6e+03 |\n",
      "|    exploration_rate | 0.457    |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 145      |\n",
      "|    time_elapsed     | 3742     |\n",
      "|    total_timesteps  | 543300   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00702  |\n",
      "|    n_updates        | 266      |\n",
      "----------------------------------\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "-541.2999999999829\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "Num timesteps: 550000\n",
      "Best mean reward: -1623.72 - Last mean reward per episode: -1580.22\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-544.8999999999871\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-535.1999999999849\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "Num timesteps: 560000\n",
      "Best mean reward: -1580.22 - Last mean reward per episode: -1536.79\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "-434.3999999999763\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4.47e+03  |\n",
      "|    ep_rew_mean      | -1.51e+03 |\n",
      "|    exploration_rate | 0.44      |\n",
      "| time/               |           |\n",
      "|    episodes         | 124       |\n",
      "|    fps              | 144       |\n",
      "|    time_elapsed     | 3866      |\n",
      "|    total_timesteps  | 560163    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 0.00028   |\n",
      "|    n_updates        | 275       |\n",
      "-----------------------------------\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "-570.1999999999928\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-510.4999999999773\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "Num timesteps: 570000\n",
      "Best mean reward: -1536.79 - Last mean reward per episode: -1472.15\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-432.29999999997955\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "-401.19999999997816\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4.44e+03  |\n",
      "|    ep_rew_mean      | -1.43e+03 |\n",
      "|    exploration_rate | 0.423     |\n",
      "| time/               |           |\n",
      "|    episodes         | 128       |\n",
      "|    fps              | 144       |\n",
      "|    time_elapsed     | 3990      |\n",
      "|    total_timesteps  | 576765    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 5.04e-05  |\n",
      "|    n_updates        | 283       |\n",
      "-----------------------------------\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "2 links\n",
      "2\n",
      "7\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "Num timesteps: 580000\n",
      "Best mean reward: -1472.15 - Last mean reward per episode: -1427.69\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "-464.0999999999771\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-396.599999999982\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-465.2999999999768\n",
      "(256, 256, 3)\n",
      "1 link\n",
      "1\n",
      "2\n",
      "Num timesteps: 590000\n",
      "Best mean reward: -1427.69 - Last mean reward per episode: -1365.73\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-390.9999999999799\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4.42e+03  |\n",
      "|    ep_rew_mean      | -1.35e+03 |\n",
      "|    exploration_rate | 0.406     |\n",
      "| time/               |           |\n",
      "|    episodes         | 132       |\n",
      "|    fps              | 144       |\n",
      "|    time_elapsed     | 4115      |\n",
      "|    total_timesteps  | 593547    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 0.00767   |\n",
      "|    n_updates        | 291       |\n",
      "-----------------------------------\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-293.69999999997896\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "Num timesteps: 600000\n",
      "Best mean reward: -1365.73 - Last mean reward per episode: -1323.90\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-355.2999999999791\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-331.5999999999829\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "Num timesteps: 610000\n",
      "Best mean reward: -1323.90 - Last mean reward per episode: -1283.01\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-297.0999999999853\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4.4e+03   |\n",
      "|    ep_rew_mean      | -1.26e+03 |\n",
      "|    exploration_rate | 0.39      |\n",
      "| time/               |           |\n",
      "|    episodes         | 136       |\n",
      "|    fps              | 143       |\n",
      "|    time_elapsed     | 4239      |\n",
      "|    total_timesteps  | 610015    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 0.0048    |\n",
      "|    n_updates        | 300       |\n",
      "-----------------------------------\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-285.8999999999817\n",
      "(256, 256, 3)\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-227.29999999999728\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "Num timesteps: 620000\n",
      "Best mean reward: -1283.01 - Last mean reward per episode: -1222.48\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-281.59999999998536\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-238.69999999998385\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4.38e+03  |\n",
      "|    ep_rew_mean      | -1.18e+03 |\n",
      "|    exploration_rate | 0.373     |\n",
      "| time/               |           |\n",
      "|    episodes         | 140       |\n",
      "|    fps              | 143       |\n",
      "|    time_elapsed     | 4363      |\n",
      "|    total_timesteps  | 626719    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 0.000142  |\n",
      "|    n_updates        | 308       |\n",
      "-----------------------------------\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "Num timesteps: 630000\n",
      "Best mean reward: -1222.48 - Last mean reward per episode: -1183.20\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-196.8999999999919\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-191.9999999999971\n",
      "(256, 256, 3)\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-265.0999999999879\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "Num timesteps: 640000\n",
      "Best mean reward: -1183.20 - Last mean reward per episode: -1122.17\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-185.4999999999921\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.35e+03 |\n",
      "|    ep_rew_mean      | -1.1e+03 |\n",
      "|    exploration_rate | 0.357    |\n",
      "| time/               |          |\n",
      "|    episodes         | 144      |\n",
      "|    fps              | 143      |\n",
      "|    time_elapsed     | 4487     |\n",
      "|    total_timesteps  | 643149   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000273 |\n",
      "|    n_updates        | 316      |\n",
      "----------------------------------\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-146.60000000000315\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "Num timesteps: 650000\n",
      "Best mean reward: -1122.17 - Last mean reward per episode: -1081.70\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-157.50000000000256\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-151.8999999999994\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "-165.59999999999718\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4.34e+03  |\n",
      "|    ep_rew_mean      | -1.02e+03 |\n",
      "|    exploration_rate | 0.34      |\n",
      "| time/               |           |\n",
      "|    episodes         | 148       |\n",
      "|    fps              | 143       |\n",
      "|    time_elapsed     | 4612      |\n",
      "|    total_timesteps  | 659792    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.001     |\n",
      "|    loss             | 0.0122    |\n",
      "|    n_updates        | 324       |\n",
      "-----------------------------------\n",
      "more than 3 links\n",
      "Num timesteps: 660000\n",
      "Best mean reward: -1081.70 - Last mean reward per episode: -1024.69\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-90.80000000000656\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-100.10000000000723\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "Num timesteps: 670000\n",
      "Best mean reward: -1024.69 - Last mean reward per episode: -985.19\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-89.10000000000215\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-65.40000000000211\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.31e+03 |\n",
      "|    ep_rew_mean      | -946     |\n",
      "|    exploration_rate | 0.324    |\n",
      "| time/               |          |\n",
      "|    episodes         | 152      |\n",
      "|    fps              | 142      |\n",
      "|    time_elapsed     | 4737     |\n",
      "|    total_timesteps  | 675956   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 7.36e-05 |\n",
      "|    n_updates        | 332      |\n",
      "----------------------------------\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "Num timesteps: 680000\n",
      "Best mean reward: -985.19 - Last mean reward per episode: -946.41\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-13.799999999999358\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-34.7999999999995\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-38.40000000000421\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "Num timesteps: 690000\n",
      "Best mean reward: -946.41 - Last mean reward per episode: -886.98\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "21.600000000001426\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.28e+03 |\n",
      "|    ep_rew_mean      | -868     |\n",
      "|    exploration_rate | 0.308    |\n",
      "| time/               |          |\n",
      "|    episodes         | 156      |\n",
      "|    fps              | 142      |\n",
      "|    time_elapsed     | 4862     |\n",
      "|    total_timesteps  | 691902   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 7.16e-05 |\n",
      "|    n_updates        | 340      |\n",
      "----------------------------------\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "-27.399999999998954\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "Num timesteps: 700000\n",
      "Best mean reward: -886.98 - Last mean reward per episode: -849.24\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "50.09999999999827\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "60.5999999999961\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "110.99999999999437\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.26e+03 |\n",
      "|    ep_rew_mean      | -792     |\n",
      "|    exploration_rate | 0.292    |\n",
      "| time/               |          |\n",
      "|    episodes         | 160      |\n",
      "|    fps              | 142      |\n",
      "|    time_elapsed     | 4987     |\n",
      "|    total_timesteps  | 708204   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.03e-05 |\n",
      "|    n_updates        | 349      |\n",
      "----------------------------------\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "Num timesteps: 710000\n",
      "Best mean reward: -849.24 - Last mean reward per episode: -792.08\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "100.39999999999355\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "71.6999999999935\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "Num timesteps: 720000\n",
      "Best mean reward: -792.08 - Last mean reward per episode: -755.18\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "99.79999999999352\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "181.9000000000037\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.24e+03 |\n",
      "|    ep_rew_mean      | -716     |\n",
      "|    exploration_rate | 0.276    |\n",
      "| time/               |          |\n",
      "|    episodes         | 164      |\n",
      "|    fps              | 141      |\n",
      "|    time_elapsed     | 5111     |\n",
      "|    total_timesteps  | 724244   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.137    |\n",
      "|    n_updates        | 357      |\n",
      "----------------------------------\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "133.39999999999907\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "Num timesteps: 730000\n",
      "Best mean reward: -755.18 - Last mean reward per episode: -698.93\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "169.5000000000042\n",
      "(256, 256, 3)\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "125.79999999999255\n",
      "(256, 256, 3)\n",
      "2 links\n",
      "2\n",
      "12\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "Num timesteps: 740000\n",
      "Best mean reward: -698.93 - Last mean reward per episode: -662.94\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "232.00000000001086\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.22e+03 |\n",
      "|    ep_rew_mean      | -645     |\n",
      "|    exploration_rate | 0.26     |\n",
      "| time/               |          |\n",
      "|    episodes         | 168      |\n",
      "|    fps              | 141      |\n",
      "|    time_elapsed     | 5235     |\n",
      "|    total_timesteps  | 740383   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.000826 |\n",
      "|    n_updates        | 365      |\n",
      "----------------------------------\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "244.7000000000125\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "247.70000000001508\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "Num timesteps: 750000\n",
      "Best mean reward: -662.94 - Last mean reward per episode: -608.99\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "3\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "205.00000000001654\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "233.80000000001183\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.2e+03  |\n",
      "|    ep_rew_mean      | -574     |\n",
      "|    exploration_rate | 0.243    |\n",
      "| time/               |          |\n",
      "|    episodes         | 172      |\n",
      "|    fps              | 141      |\n",
      "|    time_elapsed     | 5359     |\n",
      "|    total_timesteps  | 756574   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 6.89e-05 |\n",
      "|    n_updates        | 373      |\n",
      "----------------------------------\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "Num timesteps: 760000\n",
      "Best mean reward: -608.99 - Last mean reward per episode: -574.10\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "284.5000000000176\n",
      "(256, 256, 3)\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "304.1000000000208\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "313.5000000000191\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "Num timesteps: 770000\n",
      "Best mean reward: -574.10 - Last mean reward per episode: -519.48\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "351.0000000000187\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.18e+03 |\n",
      "|    ep_rew_mean      | -501     |\n",
      "|    exploration_rate | 0.227    |\n",
      "| time/               |          |\n",
      "|    episodes         | 176      |\n",
      "|    fps              | 140      |\n",
      "|    time_elapsed     | 5484     |\n",
      "|    total_timesteps  | 772541   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00784  |\n",
      "|    n_updates        | 381      |\n",
      "----------------------------------\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "312.10000000002003\n",
      "(256, 256, 3)\n",
      "2 links\n",
      "2\n",
      "8\n",
      "2 links\n",
      "2\n",
      "10\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "3\n",
      "more than 3 links\n",
      "Num timesteps: 780000\n",
      "Best mean reward: -519.48 - Last mean reward per episode: -483.51\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "285.3000000000137\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "322.5000000000203\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "395.3000000000256\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.17e+03 |\n",
      "|    ep_rew_mean      | -431     |\n",
      "|    exploration_rate | 0.211    |\n",
      "| time/               |          |\n",
      "|    episodes         | 180      |\n",
      "|    fps              | 140      |\n",
      "|    time_elapsed     | 5610     |\n",
      "|    total_timesteps  | 788730   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00204  |\n",
      "|    n_updates        | 389      |\n",
      "----------------------------------\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "Num timesteps: 790000\n",
      "Best mean reward: -483.51 - Last mean reward per episode: -431.01\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "471.00000000002893\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "490.2000000000309\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "3\n",
      "Num timesteps: 800000\n",
      "Best mean reward: -431.01 - Last mean reward per episode: -395.29\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "561.2000000000127\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "481.40000000003033\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.17e+03 |\n",
      "|    ep_rew_mean      | -358     |\n",
      "|    exploration_rate | 0.193    |\n",
      "| time/               |          |\n",
      "|    episodes         | 184      |\n",
      "|    fps              | 140      |\n",
      "|    time_elapsed     | 5734     |\n",
      "|    total_timesteps  | 806631   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.21e-05 |\n",
      "|    n_updates        | 398      |\n",
      "----------------------------------\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "Num timesteps: 810000\n",
      "Best mean reward: -395.29 - Last mean reward per episode: -358.15\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "591.0999999999985\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "599.1999999999974\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "Num timesteps: 820000\n",
      "Best mean reward: -358.15 - Last mean reward per episode: -315.99\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "550.5000000000101\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "577.2999999999978\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.17e+03 |\n",
      "|    ep_rew_mean      | -280     |\n",
      "|    exploration_rate | 0.175    |\n",
      "| time/               |          |\n",
      "|    episodes         | 188      |\n",
      "|    fps              | 140      |\n",
      "|    time_elapsed     | 5858     |\n",
      "|    total_timesteps  | 824635   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0076   |\n",
      "|    n_updates        | 407      |\n",
      "----------------------------------\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "597.6999999999948\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "Num timesteps: 830000\n",
      "Best mean reward: -315.99 - Last mean reward per episode: -262.34\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "602.999999999998\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "628.9999999999875\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "Num timesteps: 840000\n",
      "Best mean reward: -262.34 - Last mean reward per episode: -227.66\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "678.0999999999776\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.17e+03 |\n",
      "|    ep_rew_mean      | -209     |\n",
      "|    exploration_rate | 0.158    |\n",
      "| time/               |          |\n",
      "|    episodes         | 192      |\n",
      "|    fps              | 140      |\n",
      "|    time_elapsed     | 5984     |\n",
      "|    total_timesteps  | 842208   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.24e-06 |\n",
      "|    n_updates        | 416      |\n",
      "----------------------------------\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "697.6999999999696\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "Num timesteps: 850000\n",
      "Best mean reward: -227.66 - Last mean reward per episode: -190.93\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "736.4999999999693\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "3\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "763.7999999999581\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "Num timesteps: 860000\n",
      "Best mean reward: -190.93 - Last mean reward per episode: -154.63\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "657.7999999999817\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.18e+03 |\n",
      "|    ep_rew_mean      | -139     |\n",
      "|    exploration_rate | 0.15     |\n",
      "| time/               |          |\n",
      "|    episodes         | 196      |\n",
      "|    fps              | 140      |\n",
      "|    time_elapsed     | 6108     |\n",
      "|    total_timesteps  | 860263   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.131    |\n",
      "|    n_updates        | 425      |\n",
      "----------------------------------\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "686.3999999999754\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "726.4999999999649\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "Num timesteps: 870000\n",
      "Best mean reward: -154.63 - Last mean reward per episode: -105.47\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "660.8999999999786\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "694.5999999999756\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.19e+03 |\n",
      "|    ep_rew_mean      | -72.7    |\n",
      "|    exploration_rate | 0.15     |\n",
      "| time/               |          |\n",
      "|    episodes         | 200      |\n",
      "|    fps              | 140      |\n",
      "|    time_elapsed     | 6232     |\n",
      "|    total_timesteps  | 878157   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 2.15e-05 |\n",
      "|    n_updates        | 434      |\n",
      "----------------------------------\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "Num timesteps: 880000\n",
      "Best mean reward: -105.47 - Last mean reward per episode: -72.68\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "671.3999999999766\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "662.3999999999824\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "Num timesteps: 890000\n",
      "Best mean reward: -72.68 - Last mean reward per episode: -40.00\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "679.5999999999779\n",
      "(256, 256, 3)\n",
      "2 links\n",
      "2\n",
      "3\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "688.3999999999731\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.19e+03 |\n",
      "|    ep_rew_mean      | -8.7     |\n",
      "|    exploration_rate | 0.15     |\n",
      "| time/               |          |\n",
      "|    episodes         | 204      |\n",
      "|    fps              | 140      |\n",
      "|    time_elapsed     | 6356     |\n",
      "|    total_timesteps  | 895871   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 4.68e-06 |\n",
      "|    n_updates        | 442      |\n",
      "----------------------------------\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "Num timesteps: 900000\n",
      "Best mean reward: -40.00 - Last mean reward per episode: -8.70\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "684.199999999975\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "712.2999999999699\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "715.6999999999737\n",
      "(256, 256, 3)\n",
      "1 link\n",
      "1\n",
      "2\n",
      "Num timesteps: 910000\n",
      "Best mean reward: -8.70 - Last mean reward per episode: 38.00\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "739.8999999999633\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.21e+03 |\n",
      "|    ep_rew_mean      | 52.8     |\n",
      "|    exploration_rate | 0.15     |\n",
      "| time/               |          |\n",
      "|    episodes         | 208      |\n",
      "|    fps              | 141      |\n",
      "|    time_elapsed     | 6481     |\n",
      "|    total_timesteps  | 913871   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00263  |\n",
      "|    n_updates        | 451      |\n",
      "----------------------------------\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "694.3999999999727\n",
      "(256, 256, 3)\n",
      "2 links\n",
      "2\n",
      "7\n",
      "1 link\n",
      "1\n",
      "2\n",
      "Num timesteps: 920000\n",
      "Best mean reward: 38.00 - Last mean reward per episode: 67.42\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "592.0000000000016\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "621.5999999999922\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "676.8999999999762\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.2e+03  |\n",
      "|    ep_rew_mean      | 109      |\n",
      "|    exploration_rate | 0.15     |\n",
      "| time/               |          |\n",
      "|    episodes         | 212      |\n",
      "|    fps              | 140      |\n",
      "|    time_elapsed     | 6605     |\n",
      "|    total_timesteps  | 929981   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.0113   |\n",
      "|    n_updates        | 459      |\n",
      "----------------------------------\n",
      "Num timesteps: 930000\n",
      "Best mean reward: 67.42 - Last mean reward per episode: 109.16\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "618.7999999999934\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "650.7999999999829\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "Num timesteps: 940000\n",
      "Best mean reward: 109.16 - Last mean reward per episode: 135.03\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "619.7999999999929\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "606.299999999998\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.2e+03  |\n",
      "|    ep_rew_mean      | 160      |\n",
      "|    exploration_rate | 0.15     |\n",
      "| time/               |          |\n",
      "|    episodes         | 216      |\n",
      "|    fps              | 140      |\n",
      "|    time_elapsed     | 6729     |\n",
      "|    total_timesteps  | 945899   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.00319  |\n",
      "|    n_updates        | 467      |\n",
      "----------------------------------\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "605.9999999999931\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "Num timesteps: 950000\n",
      "Best mean reward: 135.03 - Last mean reward per episode: 172.85\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "1 link\n",
      "1\n",
      "2\n",
      "1 link\n",
      "1\n",
      "4\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "623.4999999999915\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "660.2999999999838\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "Num timesteps: 960000\n",
      "Best mean reward: 172.85 - Last mean reward per episode: 198.93\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "662.9999999999762\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.2e+03  |\n",
      "|    ep_rew_mean      | 211      |\n",
      "|    exploration_rate | 0.15     |\n",
      "| time/               |          |\n",
      "|    episodes         | 220      |\n",
      "|    fps              | 140      |\n",
      "|    time_elapsed     | 6853     |\n",
      "|    total_timesteps  | 963149   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 0.253    |\n",
      "|    n_updates        | 476      |\n",
      "----------------------------------\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "End episode, cumulative Reward:\n",
      "711.7999999999676\n",
      "(256, 256, 3)\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "Num timesteps: 970000\n",
      "Best mean reward: 198.93 - Last mean reward per episode: 223.32\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "2 links\n",
      "2\n",
      "3\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "671.6999999999819\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "665.6999999999846\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "Num timesteps: 980000\n",
      "Best mean reward: 223.32 - Last mean reward per episode: 247.50\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "648.3999999999854\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.2e+03  |\n",
      "|    ep_rew_mean      | 258      |\n",
      "|    exploration_rate | 0.15     |\n",
      "| time/               |          |\n",
      "|    episodes         | 224      |\n",
      "|    fps              | 140      |\n",
      "|    time_elapsed     | 6978     |\n",
      "|    total_timesteps  | 980242   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 7.9e-06  |\n",
      "|    n_updates        | 485      |\n",
      "----------------------------------\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "618.4999999999931\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "648.5999999999863\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "Num timesteps: 990000\n",
      "Best mean reward: 247.50 - Last mean reward per episode: 281.81\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "601.8000000000038\n",
      "(256, 256, 3)\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "2 links\n",
      "2\n",
      "5\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "End episode, cumulative Reward:\n",
      "633.3999999999974\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.2e+03  |\n",
      "|    ep_rew_mean      | 302      |\n",
      "|    exploration_rate | 0.15     |\n",
      "| time/               |          |\n",
      "|    episodes         | 228      |\n",
      "|    fps              | 140      |\n",
      "|    time_elapsed     | 7102     |\n",
      "|    total_timesteps  | 996520   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.001    |\n",
      "|    loss             | 1.09e-06 |\n",
      "|    n_updates        | 493      |\n",
      "----------------------------------\n",
      "more than 3 links\n",
      "more than 3 links\n",
      "2 links\n",
      "2\n",
      "3\n",
      "1 link\n",
      "1\n",
      "2\n",
      "more than 3 links\n",
      "Num timesteps: 1000000\n",
      "Best mean reward: 281.81 - Last mean reward per episode: 302.49\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\src\\optical_network_game\\tensorboard_logs/best_model\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#main function with callback\n",
    "\n",
    "# Create and wrap the environment\n",
    "nodeList, linkList = createTestTopology()\n",
    "#changed to only have 1 request per episode\n",
    "#from 6 originally\n",
    "requestList = generateRequests(nodeList, 6)\n",
    "user = User()\n",
    "\n",
    "\n",
    "env = game_gym(nodeList, linkList, requestList, user)\n",
    "check_env(env)\n",
    "\n",
    "eveon = Monitor(env, log_tensorboard)\n",
    "\n",
    "# Create the callback: check every 1000 steps\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=10000, log_dir=log_tensorboard)\n",
    "\n",
    "#hyperparameters testing\n",
    "lr = 0.001\n",
    "gamma = 0.7\n",
    "eps_start = 1\n",
    "eps_end = 0.15\n",
    "train_freq = (2000, \"step\")\n",
    "target_update_interval = 20000\n",
    "policy_kwargs = {\n",
    "    'net_arch':[64,64] #MLP hidden layer size\n",
    "}\n",
    "\n",
    "\n",
    "# Train the agent\n",
    "#model = DQN('MlpPolicy', eveon, verbose=2, buffer_size=100)\n",
    "model = DQN('MlpPolicy', eveon, learning_starts=10000, buffer_size=10000, verbose=1, device=\"auto\", learning_rate=lr, gamma=gamma, exploration_fraction=0.85, exploration_initial_eps=eps_start, exploration_final_eps=eps_end, target_update_interval=target_update_interval, train_freq=train_freq, tensorboard_log=\"./tensorboard_logs/\")\n",
    "    \n",
    "\n",
    "timesteps = int(1000000)\n",
    "model.learn(total_timesteps=timesteps, tb_log_name=\"DQN_tensor\", callback=callback)\n",
    "\n",
    "# # Close the environment\n",
    "# eveon.close()\n",
    "\n",
    "# Plot rewards\n",
    "plot_results([log_tensorboard], timesteps, results_plotter.X_TIMESTEPS, \"DQN EON\")\n",
    "plt.show()\n",
    "plt.savefig(\"episode rewards over timestamps.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 18272), started 22:41:33 ago. (Use '!kill 18272' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-31465e11cff5f0dc\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-31465e11cff5f0dc\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#to start tensorboard logging, select the --logdir where the logs are at (in this case tensorboard_logs)\n",
    "%tensorboard --logdir tensorboard_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"DQNEveon_1mil_030322\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAIN FUNCTION\n",
    "def main():\n",
    "\n",
    "    nodeList, linkList = createTestTopology()\n",
    "\n",
    "    #changed to only have 1 request per episode\n",
    "    #from 6 originally\n",
    "    requestList = generateRequests(nodeList, 6)\n",
    "\n",
    "    user = User()\n",
    "    eveon = game_gym(nodeList, linkList, requestList, user)\n",
    "\n",
    "    check_env(eveon, warn=True)\n",
    "\n",
    "    #hyperparameters testing\n",
    "    lr = 0.001\n",
    "    gamma = 0.77\n",
    "    eps_start = 1\n",
    "    eps_end = 0.15\n",
    "    train_freq = (100, \"step\")\n",
    "    target_update_interval = 2000\n",
    "    policy_kwarg = {\n",
    "        'net_arch': [64,64] #MLP hidden layer size\n",
    "    }\n",
    "    #learning_starts = 500 #memory warmup(?)\n",
    "\n",
    "\n",
    "    #added additional hyperparameters to test the training\n",
    "    #model = DQN('MlpPolicy', eveon, learning_rate=lr, verbose=1, buffer_size=100, device=\"auto\", gamma=gamma, exploration_initial_eps=eps_start, exploration_final_eps=eps_end, exploration_fraction=1)\n",
    "    \n",
    "    #added new parameters to DQN model\n",
    "    model = DQN('MlpPolicy', eveon, batch_size=32 ,buffer_size=450, verbose=1, device=\"auto\", learning_rate=lr, gamma=gamma, exploration_fraction=0.85, exploration_initial_eps=eps_start, exploration_final_eps=eps_end, target_update_interval=target_update_interval, train_freq=train_freq)\n",
    "    #for the model might want to change to CnnPolicy (used when DQN uses images as input)\n",
    "\n",
    "    #added log for every 3000 timesteps\n",
    "    model.learn(total_timesteps=1000000)\n",
    "    \n",
    "    '''\n",
    "    for i in range(6):\n",
    "        #for loop which increases the number of requests per training loop\n",
    "        req_num = random.randint(1, i + 2)\n",
    "\n",
    "        #generate requestlist for for loop\n",
    "        requestList = generateRequests(nodeList, req_num)\n",
    "\n",
    "        model.learn(total_timesteps=15000)\n",
    "        \n",
    "        #debug for training\n",
    "        print(\"Model Trained with \" + str(req_num) + \" requests\")\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    model.save(\"DQNEveon_testing_260222\")\n",
    "\n",
    "    \n",
    "    # THIS IS THE TESTING LOOP OF THE AGENT PLAYING THE GAME\n",
    "    obs = eveon.reset()\n",
    "    while True :\n",
    "        action, states_ = model.predict(obs, deterministic=False)\n",
    "        # action = 6\n",
    "        obs, rewards, dones, info = eveon.step(action)\n",
    "        \n",
    "        print(\"Action:\")\n",
    "        print(action)\n",
    "        #time.sleep(1)\n",
    "        #clear_output(wait=True)\n",
    "\n",
    "\n",
    "        if dones == True:\n",
    "            #debug print\n",
    "            print(\"########################Reward Obtained:\")\n",
    "            print(eveon.reward)\n",
    "            # with open('info.json', 'w') as outfile:\n",
    "            #     json.dump(info, outfile)\n",
    "\n",
    "            eveon.reset()\n",
    "\n",
    "        eveon.render()\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    model = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:221: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 3.93GB > 2.34GB\n",
      "  \"This system does not have apparently enough memory to store the complete \"\n"
     ]
    }
   ],
   "source": [
    "#test2 is the successful one (but still just spams enter key)\n",
    "#LOADING MODEL FROM ZIP\n",
    "#loaded_model = DQN.load(\"DQNEveon_test2\")\n",
    "#loaded_model = DQN.load(\"DQNEveon_testing_surface_250222\")\n",
    "\n",
    "#2mil timestep model load\n",
    "#NOTE that old model on more complex game not compatible with new observation window\n",
    "#loaded_model = DQN.load(\"DQNEveon_2mil_270222\", device=\"cpu\")\n",
    "#best model load during 2mil training\n",
    "#loaded_model = DQN.load(\"./Models/270222_Model_2mil/best_model.zip\", device=\"cpu\")\n",
    "\n",
    "#loading 1mil semi simplified model with new rewards\n",
    "loaded_model = DQN.load(\"DQNEveon_1mil_030322.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Agent Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "(256, 256, 3)\n",
      "(256, 256, 3)\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "more than 3 links\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "more than 3 links\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "Action:\n",
      "2\n",
      "End episode, cumulative Reward:\n",
      "284.00000000000483\n",
      "Action:\n",
      "2\n",
      "########################Reward Obtained:\n",
      "5\n",
      "(256, 256, 3)\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "2 links\n",
      "2\n",
      "3\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "more than 3 links\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n",
      "Action:\n",
      "2\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda\\envs\\RL\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3465: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "nodeList, linkList = createTestTopology()\n",
    "\n",
    "#changed to only have 1 request per episode\n",
    "#from 6 originally\n",
    "requestList = generateRequests(nodeList, 6)\n",
    "\n",
    "user = User()\n",
    "eveon = game_gym(nodeList, linkList, requestList, user)\n",
    "\n",
    "check_env(eveon, warn=True)\n",
    "\n",
    "# THIS IS THE TESTING LOOP OF THE AGENT PLAYING THE GAME|\n",
    "obs = eveon.reset()\n",
    "while True :\n",
    "\n",
    "    #enable this if using older models with the wider observation space\n",
    "    #obs = cv2.resize(obs, dsize=(600, 1000))\n",
    "    \n",
    "    \n",
    "    #trying to test if deterministic true or false changes model actions\n",
    "    action, states_ = loaded_model.predict(obs, deterministic=True)\n",
    "    # action = 6\n",
    "    obs, rewards, dones, info = eveon.step(action)\n",
    "    \n",
    "    print(\"Action:\")\n",
    "    print(action)\n",
    "    #time.sleep(1)\n",
    "    #clear_output(wait=True)\n",
    "\n",
    "\n",
    "    if dones == True:\n",
    "        #debug print\n",
    "        print(\"########################Reward Obtained:\")\n",
    "        print(eveon.reward)\n",
    "        # with open('info.json', 'w') as outfile:\n",
    "        #     json.dump(info, outfile)\n",
    "\n",
    "        eveon.reset()\n",
    "\n",
    "    eveon.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Action Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "(1000, 600, 3)\n",
      "(1000, 600, 3)\n",
      "3\n",
      "2\n",
      "3\n",
      "3\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "3\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "3\n",
      "1\n",
      "0\n",
      "1\n",
      "3\n",
      "3\n",
      "3\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "2\n",
      "3\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "0\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "3\n",
      "3\n",
      "0\n",
      "3\n",
      "1\n",
      "3\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "3\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "3\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "3\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "3\n",
      "3\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "1\n",
      "0\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "3\n",
      "2\n",
      "2\n",
      "0\n",
      "3\n",
      "1\n",
      "3\n",
      "1\n",
      "2\n",
      "1\n",
      "3\n",
      "1\n",
      "3\n",
      "0\n",
      "1\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "3\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "3\n",
      "3\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "3\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "3\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "3\n",
      "2\n",
      "3\n",
      "2\n",
      "3\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "3\n",
      "0\n",
      "3\n",
      "2\n",
      "0\n",
      "3\n",
      "0\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "3\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "3\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "3\n",
      "1\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "2\n",
      "3\n",
      "3\n",
      "1\n",
      "2\n",
      "1\n",
      "3\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "3\n",
      "2\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "3\n",
      "2\n",
      "0\n",
      "3\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "3\n",
      "2\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "3\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "1\n",
      "3\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "3\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "3\n",
      "2\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "2\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "3\n",
      "3\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "3\n",
      "0\n",
      "3\n",
      "1\n",
      "0\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "3\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "3\n",
      "2\n",
      "3\n",
      "1\n",
      "3\n",
      "0\n",
      "3\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "3\n",
      "0\n",
      "2\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "3\n",
      "2\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "3\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "3\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "3\n",
      "1\n",
      "3\n",
      "2\n",
      "3\n",
      "2\n",
      "0\n",
      "3\n",
      "0\n",
      "3\n",
      "3\n",
      "3\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "3\n",
      "3\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "0\n",
      "1\n",
      "3\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "3\n",
      "3\n",
      "0\n",
      "3\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "3\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "3\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "3\n",
      "2\n",
      "3\n",
      "0\n",
      "3\n",
      "2\n",
      "3\n",
      "3\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "3\n",
      "0\n",
      "1\n",
      "3\n",
      "3\n",
      "3\n",
      "2\n",
      "0\n",
      "1\n",
      "3\n",
      "0\n",
      "0\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "2\n",
      "0\n",
      "3\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "3\n",
      "3\n",
      "2\n",
      "1\n",
      "3\n",
      "2\n",
      "3\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "3\n",
      "2\n",
      "3\n",
      "3\n",
      "2\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "3\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "3\n",
      "3\n",
      "3\n",
      "2\n",
      "3\n",
      "3\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "3\n",
      "3\n",
      "3\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "3\n",
      "1\n",
      "0\n",
      "1\n",
      "3\n",
      "3\n",
      "0\n",
      "2\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "3\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "1\n",
      "3\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "3\n",
      "1\n",
      "0\n",
      "2\n",
      "0\n",
      "3\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "2\n",
      "0\n",
      "End episode, cumulative Reward:\n",
      "-267.6999999999998\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "1\n",
      "3\n",
      "1\n",
      "3\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "0\n",
      "1\n",
      "3\n",
      "0\n",
      "3\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "3\n",
      "0\n",
      "2\n",
      "3\n",
      "1\n",
      "3\n",
      "3\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "3\n",
      "0\n",
      "3\n",
      "2\n",
      "2\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "3\n",
      "2\n",
      "2\n",
      "3\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "3\n",
      "3\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "3\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "0\n",
      "3\n",
      "3\n",
      "3\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "3\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#agent taking random actions\n",
    "nodeList, linkList = createTestTopology()\n",
    "\n",
    "#changed to only have 1 request per episode\n",
    "#from 6 originally\n",
    "requestList = generateRequests(nodeList, 6)\n",
    "\n",
    "user = User()\n",
    "eveon = game_gym(nodeList, linkList, requestList, user)\n",
    "\n",
    "check_env(eveon, warn=True)\n",
    "#resets the environment\n",
    "obs = eveon.reset()\n",
    "\n",
    "for step in range(2000):\n",
    "\teveon.render()\n",
    "\n",
    "\trand_action = eveon.action_space.sample()\n",
    "\teveon.step(rand_action)\n",
    "\tprint(rand_action)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test main function\n",
    "nodeList, linkList = createTestTopology()\n",
    "requestList = generateRequests(nodeList, 6)\n",
    "\n",
    "user = User()\n",
    "eveon = game_gym(nodeList, linkList, requestList, user)\n",
    "\n",
    "check_env(eveon, warn=True)\n",
    "\n",
    "#defining the agent\n",
    "model = DQN('MlpPolicy', eveon, verbose=1, buffer_size=100, device='cuda')\n",
    "\n",
    "screen = eveon.render()\n",
    "#storing episode_durations during training to plot them\n",
    "#creating empty list to store\n",
    "#adding plot to check the training process?\n",
    "episode_durations = []\n",
    "timestep = 0\n",
    "\n",
    "\n",
    "#trying the double nested for loop\n",
    "\n",
    "for episode in range(100):\n",
    "\n",
    "#resets the environment\n",
    "obs = eveon.reset()\n",
    "\n",
    "#nested for loop iterate over time step\n",
    "for timesetp in count():\n",
    "\n",
    "    action, states_ = model.predict(obs, deterministic=True)\n",
    "    # action = 6\n",
    "    \n",
    "    obs, rewards, dones, info = eveon.step(action)\n",
    "    \n",
    "\n",
    "    print(action)\n",
    "    \n",
    "    #this only runs when the agent actually competed the level\n",
    "    if dones == True:\n",
    "        #debug print\n",
    "        print(\"########################Reward Obtained:\")\n",
    "        print(eveon.reward)\n",
    "\n",
    "        episode_durations.append(timestep)\n",
    "        plot(episode_durations, 100)\n",
    "        \n",
    "        # with open('info.json', 'w') as outfile:\n",
    "        #     json.dump(info, outfile)\n",
    "        timestep += 1\n",
    "        eveon.reset()\n",
    "\n",
    "    eveon.render()\n",
    "\n",
    "    screen = eveon.render('rgb_array')\n",
    "    plt.figure()\n",
    "    plt.imshow(screen)\n",
    "    plt.title('test screen')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40b93cbb1699659d2cb0e714698cbe72f80d4ea3079150298ec50c78e4aa7ede"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('RL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
