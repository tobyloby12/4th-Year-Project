{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.3 (SDL 2.0.16, Python 3.9.10)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "from IPython.display import clear_output, display\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "\n",
    "import optical_network_game.game_gym\n",
    "importlib.reload(optical_network_game.game_gym)\n",
    "from optical_network_game.game_gym import *\n",
    "\n",
    "from optical_network_game.requests import *\n",
    "from optical_network_game.topology_generation import *\n",
    "\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq:\n",
    "    :param log_dir: Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: Verbosity level.\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose: int = 1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = None\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "          # Retrieve training reward\n",
    "          x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "          if len(x) > 0:\n",
    "              # Mean training reward over the last 100 episodes\n",
    "              mean_reward = np.mean(y[-100:])\n",
    "              if self.verbose > 0:\n",
    "                print(f\"Num timesteps: {self.num_timesteps}\")\n",
    "                print(f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\")\n",
    "\n",
    "              # # New best model, you could save the agent here\n",
    "              # if mean_reward > self.best_mean_reward:\n",
    "              #     self.best_mean_reward = mean_reward\n",
    "              #     # Example for saving best model\n",
    "              #     if self.verbose > 0:\n",
    "              #       print(f\"Saving new best model to {self.save_path}\")\n",
    "              #     self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorboardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Custom callback for plotting additional values in tensorboard.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, verbose=0):\n",
    "        super(TensorboardCallback, self).__init__(verbose)\n",
    "    \n",
    "    def _on_step(self) -> bool:\n",
    "        block_ep = self.locals[\"infos\"][0].get('bp')\n",
    "        avg_path_len = self.locals[\"infos\"][0].get('avg_length')\n",
    "        blocked_continuous = self.locals[\"infos\"][0].get('blocked_continuous')\n",
    "        blocked_contiguous = self.locals[\"infos\"][0].get('blocked_contiguous')\n",
    "        self.logger.record('blocking_ratio', block_ep)\n",
    "        self.logger.record('average_route_length', avg_path_len)\n",
    "        self.logger.record('blocked_continuous', blocked_continuous)\n",
    "        self.logger.record('blocked_contiguous', blocked_contiguous)\n",
    "        return True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Create log dir\n",
    "log_dir = os.path.join(os.getcwd(), \"tmp/\")\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VSNL Topology Selected\n",
      "Traffic load is: 15.0\n",
      "Using cpu device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ./dqn_tensorboard/DQN_15\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -28179.999999999898\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -27419.99999999987\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -40599.99999999988\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -17699.999999999993\n",
      "------------------------------------\n",
      "| average_route_length | 2.75      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.6       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 4.6e+04   |\n",
      "|    ep_rew_mean       | -2.84e+04 |\n",
      "|    exploration_rate  | 0.988     |\n",
      "| time/                |           |\n",
      "|    episodes          | 4         |\n",
      "|    fps               | 2681      |\n",
      "|    time_elapsed      | 68        |\n",
      "|    total_timesteps   | 184106    |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000412  |\n",
      "|    n_updates         | 33526     |\n",
      "------------------------------------\n",
      "Num timesteps: 200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -28415.00\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -5919.999999999969\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -32439.999999999894\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -4779.999999999967\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -32819.9999999998\n",
      "------------------------------------\n",
      "| average_route_length | 2         |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.6       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 4.18e+04  |\n",
      "|    ep_rew_mean       | -2.37e+04 |\n",
      "|    exploration_rate  | 0.979     |\n",
      "| time/                |           |\n",
      "|    episodes          | 8         |\n",
      "|    fps               | 2493      |\n",
      "|    time_elapsed      | 134       |\n",
      "|    total_timesteps   | 334625    |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0348    |\n",
      "|    n_updates         | 71156     |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -26359.999999999935\n",
      "Num timesteps: 400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -23964.44\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 6260.000000000087\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -13819.99999999996\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -22139.999999999876\n",
      "------------------------------------\n",
      "| average_route_length | 2.2       |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.5       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 4.18e+04  |\n",
      "|    ep_rew_mean       | -2.04e+04 |\n",
      "|    exploration_rate  | 0.968     |\n",
      "| time/                |           |\n",
      "|    episodes          | 12        |\n",
      "|    fps               | 2345      |\n",
      "|    time_elapsed      | 214       |\n",
      "|    total_timesteps   | 502026    |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000735  |\n",
      "|    n_updates         | 113006    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -939.999999999975\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -31679.999999999894\n",
      "Num timesteps: 600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -19835.71\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -12879.999999999905\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -105540.00000000194\n",
      "------------------------------------\n",
      "| average_route_length | 2.57      |\n",
      "| blocked_contiguous   | 1         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 4.07e+04  |\n",
      "|    ep_rew_mean       | -2.48e+04 |\n",
      "|    exploration_rate  | 0.959     |\n",
      "| time/                |           |\n",
      "|    episodes          | 16        |\n",
      "|    fps               | 2299      |\n",
      "|    time_elapsed      | 283       |\n",
      "|    total_timesteps   | 651691    |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000149  |\n",
      "|    n_updates         | 150422    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -12579.999999999964\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 2660.0000000001046\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -17499.999999999978\n",
      "Num timesteps: 800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -22275.79\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -22779.99999999992\n",
      "------------------------------------\n",
      "| average_route_length | 2         |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 4.12e+04  |\n",
      "|    ep_rew_mean       | -2.23e+04 |\n",
      "|    exploration_rate  | 0.948     |\n",
      "| time/                |           |\n",
      "|    episodes          | 20        |\n",
      "|    fps               | 2294      |\n",
      "|    time_elapsed      | 358       |\n",
      "|    total_timesteps   | 823403    |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00499   |\n",
      "|    n_updates         | 193350    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -11299.999999999989\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -8619.999999999965\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -28499.999999999876\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -2419.9999999999277\n",
      "------------------------------------\n",
      "| average_route_length | 2.45      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.45      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 4.08e+04  |\n",
      "|    ep_rew_mean       | -2.07e+04 |\n",
      "|    exploration_rate  | 0.938     |\n",
      "| time/                |           |\n",
      "|    episodes          | 24        |\n",
      "|    fps               | 2276      |\n",
      "|    time_elapsed      | 430       |\n",
      "|    total_timesteps   | 979205    |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 2.45      |\n",
      "|    n_updates         | 232301    |\n",
      "------------------------------------\n",
      "Num timesteps: 1000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -20690.00\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -3139.9999999999764\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -66120.00000000086\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -7959.9999999999345\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -8379.999999999933\n",
      "------------------------------------\n",
      "| average_route_length | 2.22      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.55      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 3.98e+04  |\n",
      "|    ep_rew_mean       | -2.08e+04 |\n",
      "|    exploration_rate  | 0.929     |\n",
      "| time/                |           |\n",
      "|    episodes          | 28        |\n",
      "|    fps               | 2227      |\n",
      "|    time_elapsed      | 500       |\n",
      "|    total_timesteps   | 1114691   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00118   |\n",
      "|    n_updates         | 266172    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -50760.000000000284\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -19519.999999999993\n",
      "Num timesteps: 1200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -21736.00\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -47620.000000000175\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -18039.99999999998\n",
      "------------------------------------\n",
      "| average_route_length | 2.43      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 3.89e+04  |\n",
      "|    ep_rew_mean       | -2.24e+04 |\n",
      "|    exploration_rate  | 0.921     |\n",
      "| time/                |           |\n",
      "|    episodes          | 32        |\n",
      "|    fps               | 2191      |\n",
      "|    time_elapsed      | 568       |\n",
      "|    total_timesteps   | 1245282   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00156   |\n",
      "|    n_updates         | 298820    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -14499.99999999997\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -6599.9999999999845\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -33139.9999999999\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -18219.999999999975\n",
      "------------------------------------\n",
      "| average_route_length | 2.56      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.55      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 3.81e+04  |\n",
      "|    ep_rew_mean       | -2.19e+04 |\n",
      "|    exploration_rate  | 0.913     |\n",
      "| time/                |           |\n",
      "|    episodes          | 36        |\n",
      "|    fps               | 2163      |\n",
      "|    time_elapsed      | 633       |\n",
      "|    total_timesteps   | 1371490   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0028    |\n",
      "|    n_updates         | 330372    |\n",
      "------------------------------------\n",
      "Num timesteps: 1400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -21940.00\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -49800.00000000025\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -11699.999999999964\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -14299.999999999984\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -10119.999999999998\n",
      "------------------------------------\n",
      "| average_route_length | 2.86      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 3.74e+04  |\n",
      "|    ep_rew_mean       | -2.19e+04 |\n",
      "|    exploration_rate  | 0.905     |\n",
      "| time/                |           |\n",
      "|    episodes          | 40        |\n",
      "|    fps               | 2136      |\n",
      "|    time_elapsed      | 700       |\n",
      "|    total_timesteps   | 1496085   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0015    |\n",
      "|    n_updates         | 361521    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -19299.999999999956\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -102220.000000002\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -9120.000000000004\n",
      "Num timesteps: 1600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -23394.88\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -83560.00000000143\n",
      "------------------------------------\n",
      "| average_route_length | 2.18      |\n",
      "| blocked_contiguous   | 0.5       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.45      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 3.74e+04  |\n",
      "|    ep_rew_mean       | -2.48e+04 |\n",
      "|    exploration_rate  | 0.896     |\n",
      "| time/                |           |\n",
      "|    episodes          | 44        |\n",
      "|    fps               | 2110      |\n",
      "|    time_elapsed      | 780       |\n",
      "|    total_timesteps   | 1646818   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.234     |\n",
      "|    n_updates         | 399204    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -33319.99999999983\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -248699.99999998364\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -59820.00000000066\n",
      "Num timesteps: 1800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -30449.79\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -12199.999999999982\n",
      "------------------------------------\n",
      "| average_route_length | 2.57      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 3.75e+04  |\n",
      "|    ep_rew_mean       | -3.01e+04 |\n",
      "|    exploration_rate  | 0.886     |\n",
      "| time/                |           |\n",
      "|    episodes          | 48        |\n",
      "|    fps               | 2089      |\n",
      "|    time_elapsed      | 862       |\n",
      "|    total_timesteps   | 1801490   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 3.57      |\n",
      "|    n_updates         | 437872    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -4619.999999999957\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -8299.999999999978\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -17940.0\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -14559.999999999935\n",
      "------------------------------------\n",
      "| average_route_length | 2.36      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.45      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 3.72e+04  |\n",
      "|    ep_rew_mean       | -2.86e+04 |\n",
      "|    exploration_rate  | 0.877     |\n",
      "| time/                |           |\n",
      "|    episodes          | 52        |\n",
      "|    fps               | 2073      |\n",
      "|    time_elapsed      | 933       |\n",
      "|    total_timesteps   | 1935738   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0559    |\n",
      "|    n_updates         | 471434    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -51000.0000000003\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -58760.00000000058\n",
      "Num timesteps: 2000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -29594.44\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -136719.99999999907\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -29279.999999999905\n",
      "------------------------------------\n",
      "| average_route_length | 2.71      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 3.69e+04  |\n",
      "|    ep_rew_mean       | -3.15e+04 |\n",
      "|    exploration_rate  | 0.869     |\n",
      "| time/                |           |\n",
      "|    episodes          | 56        |\n",
      "|    fps               | 2059      |\n",
      "|    time_elapsed      | 1003      |\n",
      "|    total_timesteps   | 2066280   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000334  |\n",
      "|    n_updates         | 504069    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -17119.99999999999\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -11539.999999999982\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -19519.99999999995\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -58200.00000000057\n",
      "------------------------------------\n",
      "| average_route_length | 2.71      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 3.65e+04  |\n",
      "|    ep_rew_mean       | -3.12e+04 |\n",
      "|    exploration_rate  | 0.861     |\n",
      "| time/                |           |\n",
      "|    episodes          | 60        |\n",
      "|    fps               | 2047      |\n",
      "|    time_elapsed      | 1068      |\n",
      "|    total_timesteps   | 2187695   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0004    |\n",
      "|    n_updates         | 534423    |\n",
      "------------------------------------\n",
      "Num timesteps: 2200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -31168.67\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -63880.00000000077\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -41099.999999999854\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -13439.999999999927\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -49380.000000000204\n",
      "------------------------------------\n",
      "| average_route_length | 2.29      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 3.62e+04  |\n",
      "|    ep_rew_mean       | -3.18e+04 |\n",
      "|    exploration_rate  | 0.853     |\n",
      "| time/                |           |\n",
      "|    episodes          | 64        |\n",
      "|    fps               | 2035      |\n",
      "|    time_elapsed      | 1137      |\n",
      "|    total_timesteps   | 2316378   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0292    |\n",
      "|    n_updates         | 566594    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -361919.99999997014\n",
      "Num timesteps: 2400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -36916.00\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -31799.999999999847\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -189039.99999999203\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -32179.999999999785\n",
      "-----------------------------------\n",
      "| average_route_length | 2        |\n",
      "| blocked_contiguous   | 0        |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.55     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 3.66e+04 |\n",
      "|    ep_rew_mean       | -3.9e+04 |\n",
      "|    exploration_rate  | 0.842    |\n",
      "| time/                |          |\n",
      "|    episodes          | 68       |\n",
      "|    fps               | 2017     |\n",
      "|    time_elapsed      | 1234     |\n",
      "|    total_timesteps   | 2490172  |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00318  |\n",
      "|    n_updates         | 610042   |\n",
      "-----------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -30019.99999999987\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -123260.00000000108\n",
      "Num timesteps: 2600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -40079.14\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -105820.00000000239\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -2079.99999999995\n",
      "------------------------------------\n",
      "| average_route_length | 2.22      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.55      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 3.67e+04  |\n",
      "|    ep_rew_mean       | -4.05e+04 |\n",
      "|    exploration_rate  | 0.832     |\n",
      "| time/                |           |\n",
      "|    episodes          | 72        |\n",
      "|    fps               | 2009      |\n",
      "|    time_elapsed      | 1316      |\n",
      "|    total_timesteps   | 2645131   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00038   |\n",
      "|    n_updates         | 648782    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -77500.00000000093\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -39479.99999999986\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -19120.000000000015\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -18859.999999999964\n",
      "------------------------------------\n",
      "| average_route_length | 2.14      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 3.64e+04  |\n",
      "|    ep_rew_mean       | -4.04e+04 |\n",
      "|    exploration_rate  | 0.825     |\n",
      "| time/                |           |\n",
      "|    episodes          | 76        |\n",
      "|    fps               | 2002      |\n",
      "|    time_elapsed      | 1381      |\n",
      "|    total_timesteps   | 2765533   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000741  |\n",
      "|    n_updates         | 678883    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -15360.000000000002\n",
      "Num timesteps: 2800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -40043.38\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -5919.999999999939\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -11439.999999999978\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -54060.0000000004\n",
      "------------------------------------\n",
      "| average_route_length | 2.71      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 3.58e+04  |\n",
      "|    ep_rew_mean       | -3.94e+04 |\n",
      "|    exploration_rate  | 0.819     |\n",
      "| time/                |           |\n",
      "|    episodes          | 80        |\n",
      "|    fps               | 1997      |\n",
      "|    time_elapsed      | 1433      |\n",
      "|    total_timesteps   | 2863152   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 8.82e-05  |\n",
      "|    n_updates         | 703287    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -34879.99999999985\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -110660.00000000217\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -75820.00000000121\n",
      "Num timesteps: 3000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -40671.81\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -39619.999999999876\n",
      "------------------------------------\n",
      "| average_route_length | 3         |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 3.59e+04  |\n",
      "|    ep_rew_mean       | -4.07e+04 |\n",
      "|    exploration_rate  | 0.809     |\n",
      "| time/                |           |\n",
      "|    episodes          | 84        |\n",
      "|    fps               | 1990      |\n",
      "|    time_elapsed      | 1513      |\n",
      "|    total_timesteps   | 3013080   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00196   |\n",
      "|    n_updates         | 740769    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -88080.00000000182\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -82600.00000000112\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -14660.000000000007\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -80120.00000000109\n",
      "------------------------------------\n",
      "| average_route_length | 2.56      |\n",
      "| blocked_contiguous   | 0.143     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.55      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 3.58e+04  |\n",
      "|    ep_rew_mean       | -4.18e+04 |\n",
      "|    exploration_rate  | 0.8       |\n",
      "| time/                |           |\n",
      "|    episodes          | 88        |\n",
      "|    fps               | 1983      |\n",
      "|    time_elapsed      | 1589      |\n",
      "|    total_timesteps   | 3153370   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00186   |\n",
      "|    n_updates         | 775842    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -26899.999999999938\n",
      "Num timesteps: 3200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -41655.96\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -222719.99999998862\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -221579.999999989\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -50580.00000000028\n",
      "------------------------------------\n",
      "| average_route_length | 2.38      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.6       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 3.59e+04  |\n",
      "|    ep_rew_mean       | -4.57e+04 |\n",
      "|    exploration_rate  | 0.791     |\n",
      "| time/                |           |\n",
      "|    episodes          | 92        |\n",
      "|    fps               | 1977      |\n",
      "|    time_elapsed      | 1669      |\n",
      "|    total_timesteps   | 3300706   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000734  |\n",
      "|    n_updates         | 812676    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -115680.0000000023\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -2839.999999999898\n",
      "Num timesteps: 3400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -45962.55\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -95900.00000000179\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -34719.99999999983\n",
      "------------------------------------\n",
      "| average_route_length | 2.43      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 3.59e+04  |\n",
      "|    ep_rew_mean       | -4.64e+04 |\n",
      "|    exploration_rate  | 0.782     |\n",
      "| time/                |           |\n",
      "|    episodes          | 96        |\n",
      "|    fps               | 1971      |\n",
      "|    time_elapsed      | 1749      |\n",
      "|    total_timesteps   | 3449128   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00192   |\n",
      "|    n_updates         | 849781    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -81520.00000000112\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -6659.999999999951\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -135960.0000000002\n",
      "Num timesteps: 3600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -47221.62\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -40179.999999999935\n",
      "------------------------------------\n",
      "| average_route_length | 2.56      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.55      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 3.6e+04   |\n",
      "|    ep_rew_mean       | -4.72e+04 |\n",
      "|    exploration_rate  | 0.772     |\n",
      "| time/                |           |\n",
      "|    episodes          | 100       |\n",
      "|    fps               | 1965      |\n",
      "|    time_elapsed      | 1832      |\n",
      "|    total_timesteps   | 3600750   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000665  |\n",
      "|    n_updates         | 887687    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -191559.99999999176\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -117140.00000000205\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -111000.0000000023\n",
      "Num timesteps: 3800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -50385.60\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -145859.9999999976\n",
      "------------------------------------\n",
      "| average_route_length | 2.38      |\n",
      "| blocked_contiguous   | 0.2       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 3.62e+04  |\n",
      "|    ep_rew_mean       | -5.17e+04 |\n",
      "|    exploration_rate  | 0.759     |\n",
      "| time/                |           |\n",
      "|    episodes          | 104       |\n",
      "|    fps               | 1957      |\n",
      "|    time_elapsed      | 1943      |\n",
      "|    total_timesteps   | 3804816   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00252   |\n",
      "|    n_updates         | 938703    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -144259.9999999994\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -115180.0000000023\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -164699.99999999543\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -13019.999999999953\n",
      "------------------------------------\n",
      "| average_route_length | 2.44      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.55      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 3.63e+04  |\n",
      "|    ep_rew_mean       | -5.53e+04 |\n",
      "|    exploration_rate  | 0.749     |\n",
      "| time/                |           |\n",
      "|    episodes          | 108       |\n",
      "|    fps               | 1951      |\n",
      "|    time_elapsed      | 2030      |\n",
      "|    total_timesteps   | 3962776   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000952  |\n",
      "|    n_updates         | 978193    |\n",
      "------------------------------------\n",
      "Num timesteps: 4000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -55279.20\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -34179.99999999981\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -1419.9999999999534\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -98940.00000000176\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -82720.00000000118\n",
      "------------------------------------\n",
      "| average_route_length | 2.5       |\n",
      "| blocked_contiguous   | 0.1       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.5       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 3.61e+04  |\n",
      "|    ep_rew_mean       | -5.69e+04 |\n",
      "|    exploration_rate  | 0.739     |\n",
      "| time/                |           |\n",
      "|    episodes          | 112       |\n",
      "|    fps               | 1949      |\n",
      "|    time_elapsed      | 2111      |\n",
      "|    total_timesteps   | 4116761   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00588   |\n",
      "|    n_updates         | 1016690   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -16899.999999999945\n",
      "Num timesteps: 4200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -57050.80\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -32119.9999999997\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -26359.999999999887\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -70060.00000000067\n",
      "------------------------------------\n",
      "| average_route_length | 2.75      |\n",
      "| blocked_contiguous   | 0.1       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.6       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 3.63e+04  |\n",
      "|    ep_rew_mean       | -5.68e+04 |\n",
      "|    exploration_rate  | 0.729     |\n",
      "| time/                |           |\n",
      "|    episodes          | 116       |\n",
      "|    fps               | 1948      |\n",
      "|    time_elapsed      | 2196      |\n",
      "|    total_timesteps   | 4280662   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0303    |\n",
      "|    n_updates         | 1057665   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -127340.00000000131\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -13619.99999999993\n",
      "Num timesteps: 4400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -58145.60\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -37199.99999999983\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -39759.999999999876\n",
      "------------------------------------\n",
      "| average_route_length | 2.86      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 3.62e+04  |\n",
      "|    ep_rew_mean       | -5.85e+04 |\n",
      "|    exploration_rate  | 0.719     |\n",
      "| time/                |           |\n",
      "|    episodes          | 120       |\n",
      "|    fps               | 1950      |\n",
      "|    time_elapsed      | 2278      |\n",
      "|    total_timesteps   | 4444555   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.116     |\n",
      "|    n_updates         | 1098638   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -45600.00000000009\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -11799.999999999967\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -84120.00000000121\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -103680.00000000192\n",
      "------------------------------------\n",
      "| average_route_length | 3         |\n",
      "| blocked_contiguous   | 0.1       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.6       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 3.61e+04  |\n",
      "|    ep_rew_mean       | -6.05e+04 |\n",
      "|    exploration_rate  | 0.709     |\n",
      "| time/                |           |\n",
      "|    episodes          | 124       |\n",
      "|    fps               | 1952      |\n",
      "|    time_elapsed      | 2352      |\n",
      "|    total_timesteps   | 4592395   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00168   |\n",
      "|    n_updates         | 1135598   |\n",
      "------------------------------------\n",
      "Num timesteps: 4600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -60456.00\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -88120.00000000146\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -187219.99999999267\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -66820.00000000108\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -27299.999999999876\n",
      "------------------------------------\n",
      "| average_route_length | 2.44      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.55      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 3.67e+04  |\n",
      "|    ep_rew_mean       | -6.33e+04 |\n",
      "|    exploration_rate  | 0.697     |\n",
      "| time/                |           |\n",
      "|    episodes          | 128       |\n",
      "|    fps               | 1952      |\n",
      "|    time_elapsed      | 2449      |\n",
      "|    total_timesteps   | 4782014   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0014    |\n",
      "|    n_updates         | 1183003   |\n",
      "------------------------------------\n",
      "Num timesteps: 4800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -63294.60\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -262379.99999998335\n",
      "No more requests.\n",
      "Total reward for this episode is -396259.99999996496\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -26959.999999999924\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -108540.00000000208\n",
      "------------------------------------\n",
      "| average_route_length | 3         |\n",
      "| blocked_contiguous   | 0.0769    |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 3.73e+04  |\n",
      "|    ep_rew_mean       | -6.99e+04 |\n",
      "|    exploration_rate  | 0.685     |\n",
      "| time/                |           |\n",
      "|    episodes          | 132       |\n",
      "|    fps               | 1949      |\n",
      "|    time_elapsed      | 2554      |\n",
      "|    total_timesteps   | 4979326   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00154   |\n",
      "|    n_updates         | 1232331   |\n",
      "------------------------------------\n",
      "Num timesteps: 5000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -69882.20\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -70540.00000000096\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -122380.00000000131\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -322639.99999997596\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -39959.99999999984\n",
      "------------------------------------\n",
      "| average_route_length | 2.67      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.7       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 3.8e+04   |\n",
      "|    ep_rew_mean       | -7.47e+04 |\n",
      "|    exploration_rate  | 0.672     |\n",
      "| time/                |           |\n",
      "|    episodes          | 136       |\n",
      "|    fps               | 1952      |\n",
      "|    time_elapsed      | 2648      |\n",
      "|    total_timesteps   | 5171696   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.104     |\n",
      "|    n_updates         | 1280423   |\n",
      "------------------------------------\n",
      "Num timesteps: 5200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -74712.80\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -51040.00000000016\n",
      "No more requests.\n",
      "Total reward for this episode is -267859.99999998097\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -285939.99999997916\n",
      "Num timesteps: 5400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -80008.80\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -219259.99999998783\n",
      "------------------------------------\n",
      "| average_route_length | 2.91      |\n",
      "| blocked_contiguous   | 0.231     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.45      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 3.91e+04  |\n",
      "|    ep_rew_mean       | -8.21e+04 |\n",
      "|    exploration_rate  | 0.658     |\n",
      "| time/                |           |\n",
      "|    episodes          | 140       |\n",
      "|    fps               | 1948      |\n",
      "|    time_elapsed      | 2774      |\n",
      "|    total_timesteps   | 5407148   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000279  |\n",
      "|    n_updates         | 1339286   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -102320.00000000196\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -152839.99999999534\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -27939.9999999999\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -39799.9999999999\n",
      "------------------------------------\n",
      "| average_route_length | 2.33      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.55      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 3.94e+04  |\n",
      "|    ep_rew_mean       | -8.32e+04 |\n",
      "|    exploration_rate  | 0.646     |\n",
      "| time/                |           |\n",
      "|    episodes          | 144       |\n",
      "|    fps               | 1952      |\n",
      "|    time_elapsed      | 2864      |\n",
      "|    total_timesteps   | 5591431   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00222   |\n",
      "|    n_updates         | 1385357   |\n",
      "------------------------------------\n",
      "Num timesteps: 5600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -83187.20\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -219659.99999998757\n",
      "No more requests.\n",
      "Total reward for this episode is -310599.99999997485\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -244719.99999998484\n",
      "Num timesteps: 5800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -87524.20\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -129479.9999999994\n",
      "------------------------------------\n",
      "| average_route_length | 2.58      |\n",
      "| blocked_contiguous   | 0.154     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.4       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 4.06e+04  |\n",
      "|    ep_rew_mean       | -8.87e+04 |\n",
      "|    exploration_rate  | 0.629     |\n",
      "| time/                |           |\n",
      "|    episodes          | 148       |\n",
      "|    fps               | 1956      |\n",
      "|    time_elapsed      | 2995      |\n",
      "|    total_timesteps   | 5858950   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000625  |\n",
      "|    n_updates         | 1452237   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -133539.9999999989\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -179319.9999999933\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -34139.99999999982\n",
      "Num timesteps: 6000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -91858.40\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -73960.0000000008\n",
      "------------------------------------\n",
      "| average_route_length | 3.17      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.7       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 4.09e+04  |\n",
      "|    ep_rew_mean       | -9.25e+04 |\n",
      "|    exploration_rate  | 0.618     |\n",
      "| time/                |           |\n",
      "|    episodes          | 152       |\n",
      "|    fps               | 1953      |\n",
      "|    time_elapsed      | 3085      |\n",
      "|    total_timesteps   | 6026619   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.485     |\n",
      "|    n_updates         | 1494154   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -228579.9999999872\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -59280.000000000335\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -135499.9999999986\n",
      "Num timesteps: 6200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -94221.20\n",
      "No more requests.\n",
      "Total reward for this episode is -438899.99999997026\n",
      "------------------------------------\n",
      "| average_route_length | 3.33      |\n",
      "| blocked_contiguous   | 0.462     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.4       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 4.2e+04   |\n",
      "|    ep_rew_mean       | -9.83e+04 |\n",
      "|    exploration_rate  | 0.603     |\n",
      "| time/                |           |\n",
      "|    episodes          | 156       |\n",
      "|    fps               | 1954      |\n",
      "|    time_elapsed      | 3205      |\n",
      "|    total_timesteps   | 6264401   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.991     |\n",
      "|    n_updates         | 1553600   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -268839.99999998236\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -98520.00000000164\n",
      "Num timesteps: 6400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -101710.00\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -209179.99999999005\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -114320.00000000166\n",
      "------------------------------------\n",
      "| average_route_length | 3         |\n",
      "| blocked_contiguous   | 0.0769    |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.5       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 4.29e+04  |\n",
      "|    ep_rew_mean       | -1.04e+05 |\n",
      "|    exploration_rate  | 0.59      |\n",
      "| time/                |           |\n",
      "|    episodes          | 160       |\n",
      "|    fps               | 1948      |\n",
      "|    time_elapsed      | 3322      |\n",
      "|    total_timesteps   | 6475703   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000854  |\n",
      "|    n_updates         | 1606425   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -224419.9999999879\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -14600.0\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -256919.99999998292\n",
      "Num timesteps: 6600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -107943.00\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -45940.00000000008\n",
      "------------------------------------\n",
      "| average_route_length | 2.43      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 4.31e+04  |\n",
      "|    ep_rew_mean       | -1.08e+05 |\n",
      "|    exploration_rate  | 0.58      |\n",
      "| time/                |           |\n",
      "|    episodes          | 164       |\n",
      "|    fps               | 1949      |\n",
      "|    time_elapsed      | 3400      |\n",
      "|    total_timesteps   | 6627845   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000559  |\n",
      "|    n_updates         | 1644461   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -74980.00000000095\n",
      "No more requests.\n",
      "Total reward for this episode is -371199.9999999691\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -28079.99999999987\n",
      "Num timesteps: 6800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -106829.20\n",
      "No more requests.\n",
      "Total reward for this episode is -412739.9999999645\n",
      "------------------------------------\n",
      "| average_route_length | 3.33      |\n",
      "| blocked_contiguous   | 0.462     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.4       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 4.37e+04  |\n",
      "|    ep_rew_mean       | -1.11e+05 |\n",
      "|    exploration_rate  | 0.565     |\n",
      "| time/                |           |\n",
      "|    episodes          | 168       |\n",
      "|    fps               | 1950      |\n",
      "|    time_elapsed      | 3518      |\n",
      "|    total_timesteps   | 6863456   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00314   |\n",
      "|    n_updates         | 1703363   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -59700.00000000041\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -250359.99999998507\n",
      "Num timesteps: 7000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -112208.20\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -234859.99999998568\n",
      "No more requests.\n",
      "Total reward for this episode is -374959.99999996874\n",
      "------------------------------------\n",
      "| average_route_length | 3.38      |\n",
      "| blocked_contiguous   | 0.538     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 4.47e+04  |\n",
      "|    ep_rew_mean       | -1.17e+05 |\n",
      "|    exploration_rate  | 0.549     |\n",
      "| time/                |           |\n",
      "|    episodes          | 172       |\n",
      "|    fps               | 1949      |\n",
      "|    time_elapsed      | 3648      |\n",
      "|    total_timesteps   | 7113910   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000602  |\n",
      "|    n_updates         | 1765977   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -16860.000000000004\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -85000.00000000128\n",
      "Num timesteps: 7200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -117079.80\n",
      "No more requests.\n",
      "Total reward for this episode is -421059.99999996775\n",
      "No more requests.\n",
      "Total reward for this episode is -349879.99999997276\n",
      "------------------------------------\n",
      "| average_route_length | 3.31      |\n",
      "| blocked_contiguous   | 0.462     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 4.55e+04  |\n",
      "|    ep_rew_mean       | -1.24e+05 |\n",
      "|    exploration_rate  | 0.537     |\n",
      "| time/                |           |\n",
      "|    episodes          | 176       |\n",
      "|    fps               | 1950      |\n",
      "|    time_elapsed      | 3749      |\n",
      "|    total_timesteps   | 7315246   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000591  |\n",
      "|    n_updates         | 1816311   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -293139.9999999782\n",
      "Num timesteps: 7400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -127198.40\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -29219.99999999987\n",
      "No more requests.\n",
      "Total reward for this episode is -353879.9999999717\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -25459.999999999938\n",
      "------------------------------------\n",
      "| average_route_length | 2.57      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 4.66e+04  |\n",
      "|    ep_rew_mean       | -1.31e+05 |\n",
      "|    exploration_rate  | 0.523     |\n",
      "| time/                |           |\n",
      "|    episodes          | 180       |\n",
      "|    fps               | 1951      |\n",
      "|    time_elapsed      | 3856      |\n",
      "|    total_timesteps   | 7525510   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00106   |\n",
      "|    n_updates         | 1868877   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -41059.999999999905\n",
      "Num timesteps: 7600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -130637.20\n",
      "No more requests.\n",
      "Total reward for this episode is -307039.9999999792\n",
      "No more requests.\n",
      "Total reward for this episode is -320759.999999977\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -11980.000000000002\n",
      "------------------------------------\n",
      "| average_route_length | 2.67      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.85      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 4.7e+04   |\n",
      "|    ep_rew_mean       | -1.35e+05 |\n",
      "|    exploration_rate  | 0.511     |\n",
      "| time/                |           |\n",
      "|    episodes          | 184       |\n",
      "|    fps               | 1949      |\n",
      "|    time_elapsed      | 3958      |\n",
      "|    total_timesteps   | 7715324   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00112   |\n",
      "|    n_updates         | 1916330   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -222639.99999998882\n",
      "Num timesteps: 7800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -136136.40\n",
      "No more requests.\n",
      "Total reward for this episode is -277439.999999981\n",
      "No more requests.\n",
      "Total reward for this episode is -268239.9999999832\n",
      "Num timesteps: 8000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -140631.80\n",
      "No more requests.\n",
      "Total reward for this episode is -322919.9999999767\n",
      "------------------------------------\n",
      "| average_route_length | 3.15      |\n",
      "| blocked_contiguous   | 0.462     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 4.87e+04  |\n",
      "|    ep_rew_mean       | -1.43e+05 |\n",
      "|    exploration_rate  | 0.492     |\n",
      "| time/                |           |\n",
      "|    episodes          | 188       |\n",
      "|    fps               | 1949      |\n",
      "|    time_elapsed      | 4114      |\n",
      "|    total_timesteps   | 8019921   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00184   |\n",
      "|    n_updates         | 1992480   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -84740.00000000131\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -56980.00000000053\n",
      "Num timesteps: 8200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -141986.40\n",
      "No more requests.\n",
      "Total reward for this episode is -308219.9999999786\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -304679.9999999784\n",
      "------------------------------------\n",
      "| average_route_length | 3.6       |\n",
      "| blocked_contiguous   | 0.462     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.5       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 4.97e+04  |\n",
      "|    ep_rew_mean       | -1.45e+05 |\n",
      "|    exploration_rate  | 0.476     |\n",
      "| time/                |           |\n",
      "|    episodes          | 192       |\n",
      "|    fps               | 1949      |\n",
      "|    time_elapsed      | 4243      |\n",
      "|    total_timesteps   | 8271988   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0061    |\n",
      "|    n_updates         | 2055496   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -110800.0000000021\n",
      "Num timesteps: 8400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -145356.20\n",
      "No more requests.\n",
      "Total reward for this episode is -297499.99999998097\n",
      "No more requests.\n",
      "Total reward for this episode is -296659.99999997945\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -191119.99999999278\n",
      "------------------------------------\n",
      "| average_route_length | 3.78      |\n",
      "| blocked_contiguous   | 0.308     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.55      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 5.1e+04   |\n",
      "|    ep_rew_mean       | -1.52e+05 |\n",
      "|    exploration_rate  | 0.459     |\n",
      "| time/                |           |\n",
      "|    episodes          | 196       |\n",
      "|    fps               | 1948      |\n",
      "|    time_elapsed      | 4388      |\n",
      "|    total_timesteps   | 8549924   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000232  |\n",
      "|    n_updates         | 2124980   |\n",
      "------------------------------------\n",
      "Num timesteps: 8600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -151885.60\n",
      "No more requests.\n",
      "Total reward for this episode is -325239.9999999768\n",
      "No more requests.\n",
      "Total reward for this episode is -262319.999999984\n",
      "No more requests.\n",
      "Total reward for this episode is -245859.99999998676\n",
      "Num timesteps: 8800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -157995.20\n",
      "No more requests.\n",
      "Total reward for this episode is -328619.99999997613\n",
      "------------------------------------\n",
      "| average_route_length | 3.58      |\n",
      "| blocked_contiguous   | 0.538     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.4       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 5.24e+04  |\n",
      "|    ep_rew_mean       | -1.61e+05 |\n",
      "|    exploration_rate  | 0.44      |\n",
      "| time/                |           |\n",
      "|    episodes          | 200       |\n",
      "|    fps               | 1945      |\n",
      "|    time_elapsed      | 4544      |\n",
      "|    total_timesteps   | 8842733   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000433  |\n",
      "|    n_updates         | 2198183   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -138919.9999999997\n",
      "No more requests.\n",
      "Total reward for this episode is -240979.99999998685\n",
      "Num timesteps: 9000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -161497.80\n",
      "No more requests.\n",
      "Total reward for this episode is -148739.99999999747\n",
      "No more requests.\n",
      "Total reward for this episode is -248519.999999985\n",
      "------------------------------------\n",
      "| average_route_length | 3.07      |\n",
      "| blocked_contiguous   | 0.429     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 5.32e+04  |\n",
      "|    ep_rew_mean       | -1.63e+05 |\n",
      "|    exploration_rate  | 0.422     |\n",
      "| time/                |           |\n",
      "|    episodes          | 204       |\n",
      "|    fps               | 1944      |\n",
      "|    time_elapsed      | 4691      |\n",
      "|    total_timesteps   | 9123899   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000866  |\n",
      "|    n_updates         | 2268474   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -39019.999999999876\n",
      "Num timesteps: 9200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -161860.60\n",
      "No more requests.\n",
      "Total reward for this episode is -186979.99999999296\n",
      "No more requests.\n",
      "Total reward for this episode is -360559.99999997404\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -281859.99999998225\n",
      "------------------------------------\n",
      "| average_route_length | 3.5       |\n",
      "| blocked_contiguous   | 0.5       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.4       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 5.43e+04  |\n",
      "|    ep_rew_mean       | -1.67e+05 |\n",
      "|    exploration_rate  | 0.405     |\n",
      "| time/                |           |\n",
      "|    episodes          | 208       |\n",
      "|    fps               | 1943      |\n",
      "|    time_elapsed      | 4830      |\n",
      "|    total_timesteps   | 9389364   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00475   |\n",
      "|    n_updates         | 2334840   |\n",
      "------------------------------------\n",
      "Num timesteps: 9400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -167236.80\n",
      "No more requests.\n",
      "Total reward for this episode is -320019.999999977\n",
      "No more requests.\n",
      "Total reward for this episode is -303519.99999998003\n",
      "Num timesteps: 9600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -173022.40\n",
      "No more requests.\n",
      "Total reward for this episode is -232059.9999999869\n",
      "No more requests.\n",
      "Total reward for this episode is -310759.99999997864\n",
      "------------------------------------\n",
      "| average_route_length | 4.08      |\n",
      "| blocked_contiguous   | 0.5       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.4       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 5.57e+04  |\n",
      "|    ep_rew_mean       | -1.77e+05 |\n",
      "|    exploration_rate  | 0.386     |\n",
      "| time/                |           |\n",
      "|    episodes          | 212       |\n",
      "|    fps               | 1942      |\n",
      "|    time_elapsed      | 4989      |\n",
      "|    total_timesteps   | 9690904   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000612  |\n",
      "|    n_updates         | 2410225   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -248419.99999998696\n",
      "Num timesteps: 9800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -178859.00\n",
      "No more requests.\n",
      "Total reward for this episode is -210039.99999999013\n",
      "No more requests.\n",
      "Total reward for this episode is -207939.99999999086\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -196919.99999999185\n",
      "------------------------------------\n",
      "| average_route_length | 3.36      |\n",
      "| blocked_contiguous   | 0.357     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.45      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 5.7e+04   |\n",
      "|    ep_rew_mean       | -1.84e+05 |\n",
      "|    exploration_rate  | 0.368     |\n",
      "| time/                |           |\n",
      "|    episodes          | 216       |\n",
      "|    fps               | 1941      |\n",
      "|    time_elapsed      | 5142      |\n",
      "|    total_timesteps   | 9982346   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00129   |\n",
      "|    n_updates         | 2483086   |\n",
      "------------------------------------\n",
      "Num timesteps: 10000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -183733.80\n",
      "No more requests.\n",
      "Total reward for this episode is -284859.99999998225\n",
      "No more requests.\n",
      "Total reward for this episode is -322299.9999999782\n",
      "Num timesteps: 10200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -188407.00\n",
      "No more requests.\n",
      "Total reward for this episode is -240079.9999999875\n",
      "No more requests.\n",
      "Total reward for this episode is -316319.9999999797\n",
      "------------------------------------\n",
      "| average_route_length | 3.91      |\n",
      "| blocked_contiguous   | 0.571     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.45      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 5.83e+04  |\n",
      "|    ep_rew_mean       | -1.93e+05 |\n",
      "|    exploration_rate  | 0.349     |\n",
      "| time/                |           |\n",
      "|    episodes          | 220       |\n",
      "|    fps               | 1938      |\n",
      "|    time_elapsed      | 5300      |\n",
      "|    total_timesteps   | 10277089  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000236  |\n",
      "|    n_updates         | 2556772   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -12520.000000000002\n",
      "No more requests.\n",
      "Total reward for this episode is -196819.9999999928\n",
      "Num timesteps: 10400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -194527.60\n",
      "No more requests.\n",
      "Total reward for this episode is -227399.9999999899\n",
      "No more requests.\n",
      "Total reward for this episode is -227319.99999998897\n",
      "------------------------------------\n",
      "| average_route_length | 3.23      |\n",
      "| blocked_contiguous   | 0.429     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 5.92e+04  |\n",
      "|    ep_rew_mean       | -1.97e+05 |\n",
      "|    exploration_rate  | 0.334     |\n",
      "| time/                |           |\n",
      "|    episodes          | 224       |\n",
      "|    fps               | 1936      |\n",
      "|    time_elapsed      | 5428      |\n",
      "|    total_timesteps   | 10514443  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 6.31e-05  |\n",
      "|    n_updates         | 2616110   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -6139.999999999992\n",
      "Num timesteps: 10600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -196283.20\n",
      "No more requests.\n",
      "Total reward for this episode is -186039.99999999418\n",
      "No more requests.\n",
      "Total reward for this episode is -186979.9999999943\n",
      "No more requests.\n",
      "Total reward for this episode is -250039.99999998722\n",
      "-----------------------------------\n",
      "| average_route_length | 3.5      |\n",
      "| blocked_contiguous   | 0.429    |\n",
      "| blocked_continuous   | 0.0714   |\n",
      "| blocking_ratio       | 0.4      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 5.98e+04 |\n",
      "|    ep_rew_mean       | -2e+05   |\n",
      "|    exploration_rate  | 0.319    |\n",
      "| time/                |          |\n",
      "|    episodes          | 228      |\n",
      "|    fps               | 1935     |\n",
      "|    time_elapsed      | 5558     |\n",
      "|    total_timesteps   | 10758491 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.003    |\n",
      "|    n_updates         | 2677122  |\n",
      "-----------------------------------\n",
      "Num timesteps: 10800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -199717.20\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -102900.00000000169\n",
      "No more requests.\n",
      "Total reward for this episode is -242859.99999998836\n",
      "No more requests.\n",
      "Total reward for this episode is -209999.99999999185\n",
      "Num timesteps: 11000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -198424.40\n",
      "No more requests.\n",
      "Total reward for this episode is -146319.99999999697\n",
      "------------------------------------\n",
      "| average_route_length | 2.4       |\n",
      "| blocked_contiguous   | 0.357     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.04e+04  |\n",
      "|    ep_rew_mean       | -1.99e+05 |\n",
      "|    exploration_rate  | 0.302     |\n",
      "| time/                |           |\n",
      "|    episodes          | 232       |\n",
      "|    fps               | 1933      |\n",
      "|    time_elapsed      | 5701      |\n",
      "|    total_timesteps   | 11023991  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00416   |\n",
      "|    n_updates         | 2743497   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -226639.9999999891\n",
      "No more requests.\n",
      "Total reward for this episode is -199539.9999999927\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -3539.9999999999977\n",
      "Num timesteps: 11200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -197958.60\n",
      "No more requests.\n",
      "Total reward for this episode is -298739.9999999833\n",
      "------------------------------------\n",
      "| average_route_length | 4.7       |\n",
      "| blocked_contiguous   | 0.571     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.5       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.1e+04   |\n",
      "|    ep_rew_mean       | -2.01e+05 |\n",
      "|    exploration_rate  | 0.286     |\n",
      "| time/                |           |\n",
      "|    episodes          | 236       |\n",
      "|    fps               | 1931      |\n",
      "|    time_elapsed      | 5832      |\n",
      "|    total_timesteps   | 11266763  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000257  |\n",
      "|    n_updates         | 2804190   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -159199.9999999966\n",
      "Num timesteps: 11400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -201639.20\n",
      "No more requests.\n",
      "Total reward for this episode is -127140.00000000035\n",
      "No more requests.\n",
      "Total reward for this episode is -134399.99999999907\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -7219.999999999996\n",
      "------------------------------------\n",
      "| average_route_length | 2.4       |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.75      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.1e+04   |\n",
      "|    ep_rew_mean       | -1.96e+05 |\n",
      "|    exploration_rate  | 0.271     |\n",
      "| time/                |           |\n",
      "|    episodes          | 240       |\n",
      "|    fps               | 1929      |\n",
      "|    time_elapsed      | 5962      |\n",
      "|    total_timesteps   | 11507292  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00767   |\n",
      "|    n_updates         | 2864322   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -212719.99999999139\n",
      "Num timesteps: 11600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -197606.40\n",
      "No more requests.\n",
      "Total reward for this episode is -193719.99999999322\n",
      "No more requests.\n",
      "Total reward for this episode is -216859.99999999139\n",
      "No more requests.\n",
      "Total reward for this episode is -226419.99999999104\n",
      "------------------------------------\n",
      "| average_route_length | 3.91      |\n",
      "| blocked_contiguous   | 0.5       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.45      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.2e+04   |\n",
      "|    ep_rew_mean       | -2.02e+05 |\n",
      "|    exploration_rate  | 0.253     |\n",
      "| time/                |           |\n",
      "|    episodes          | 244       |\n",
      "|    fps               | 1926      |\n",
      "|    time_elapsed      | 6118      |\n",
      "|    total_timesteps   | 11788100  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 312       |\n",
      "|    n_updates         | 2934524   |\n",
      "------------------------------------\n",
      "Num timesteps: 11800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -201787.40\n",
      "No more requests.\n",
      "Total reward for this episode is -177879.99999999517\n",
      "No more requests.\n",
      "Total reward for this episode is -227199.99999998923\n",
      "Num timesteps: 12000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -200539.20\n",
      "No more requests.\n",
      "Total reward for this episode is -146399.99999999808\n",
      "No more requests.\n",
      "Total reward for this episode is -211459.99999999194\n",
      "-----------------------------------\n",
      "| average_route_length | 3.83     |\n",
      "| blocked_contiguous   | 0.5      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.4      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 6.21e+04 |\n",
      "|    ep_rew_mean       | -2e+05   |\n",
      "|    exploration_rate  | 0.235    |\n",
      "| time/                |          |\n",
      "|    episodes          | 248      |\n",
      "|    fps               | 1923     |\n",
      "|    time_elapsed      | 6276     |\n",
      "|    total_timesteps   | 12072487 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 312      |\n",
      "|    n_updates         | 3005621  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -170579.9999999963\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -15220.000000000002\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -10980.000000000002\n",
      "Num timesteps: 12200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -198888.40\n",
      "No more requests.\n",
      "Total reward for this episode is -131039.99999999872\n",
      "------------------------------------\n",
      "| average_route_length | 2.86      |\n",
      "| blocked_contiguous   | 0.429     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.21e+04  |\n",
      "|    ep_rew_mean       | -1.99e+05 |\n",
      "|    exploration_rate  | 0.225     |\n",
      "| time/                |           |\n",
      "|    episodes          | 252       |\n",
      "|    fps               | 1920      |\n",
      "|    time_elapsed      | 6369      |\n",
      "|    total_timesteps   | 12234170  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 5.49e-05  |\n",
      "|    n_updates         | 3046042   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -161819.99999999662\n",
      "No more requests.\n",
      "Total reward for this episode is -145499.9999999983\n",
      "Num timesteps: 12400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -199670.60\n",
      "No more requests.\n",
      "Total reward for this episode is -131659.99999999956\n",
      "No more requests.\n",
      "Total reward for this episode is -131799.9999999996\n",
      "------------------------------------\n",
      "| average_route_length | 2.86      |\n",
      "| blocked_contiguous   | 0.286     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.25e+04  |\n",
      "|    ep_rew_mean       | -1.96e+05 |\n",
      "|    exploration_rate  | 0.207     |\n",
      "| time/                |           |\n",
      "|    episodes          | 256       |\n",
      "|    fps               | 1917      |\n",
      "|    time_elapsed      | 6527      |\n",
      "|    total_timesteps   | 12513252  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00211   |\n",
      "|    n_updates         | 3115812   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -183899.99999999456\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -10920.0\n",
      "Num timesteps: 12600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -194637.00\n",
      "No more requests.\n",
      "Total reward for this episode is -205959.99999999284\n",
      "No more requests.\n",
      "Total reward for this episode is -177859.9999999951\n",
      "------------------------------------\n",
      "| average_route_length | 3.5       |\n",
      "| blocked_contiguous   | 0.429     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.4       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.26e+04  |\n",
      "|    ep_rew_mean       | -1.95e+05 |\n",
      "|    exploration_rate  | 0.193     |\n",
      "| time/                |           |\n",
      "|    episodes          | 260       |\n",
      "|    fps               | 1914      |\n",
      "|    time_elapsed      | 6653      |\n",
      "|    total_timesteps   | 12739999  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00312   |\n",
      "|    n_updates         | 3172499   |\n",
      "------------------------------------\n",
      "Num timesteps: 12800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -195251.40\n",
      "No more requests.\n",
      "Total reward for this episode is -205879.99999999214\n",
      "No more requests.\n",
      "Total reward for this episode is -174459.99999999587\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -6919.999999999996\n",
      "No more requests.\n",
      "Total reward for this episode is -146059.99999999872\n",
      "------------------------------------\n",
      "| average_route_length | 3.54      |\n",
      "| blocked_contiguous   | 0.5       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.34e+04  |\n",
      "|    ep_rew_mean       | -1.95e+05 |\n",
      "|    exploration_rate  | 0.178     |\n",
      "| time/                |           |\n",
      "|    episodes          | 264       |\n",
      "|    fps               | 1912      |\n",
      "|    time_elapsed      | 6781      |\n",
      "|    total_timesteps   | 12971303  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000127  |\n",
      "|    n_updates         | 3230325   |\n",
      "------------------------------------\n",
      "Num timesteps: 13000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -195182.60\n",
      "No more requests.\n",
      "Total reward for this episode is -137640.0\n",
      "No more requests.\n",
      "Total reward for this episode is -137919.99999999974\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -14360.000000000002\n",
      "No more requests.\n",
      "Total reward for this episode is -85980.00000000095\n",
      "-----------------------------------\n",
      "| average_route_length | 2.67     |\n",
      "| blocked_contiguous   | 0.286    |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.25     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 6.33e+04 |\n",
      "|    ep_rew_mean       | -1.9e+05 |\n",
      "|    exploration_rate  | 0.164    |\n",
      "| time/                |          |\n",
      "|    episodes          | 268      |\n",
      "|    fps               | 1910     |\n",
      "|    time_elapsed      | 6905     |\n",
      "|    total_timesteps   | 13193773 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.109    |\n",
      "|    n_updates         | 3285943  |\n",
      "-----------------------------------\n",
      "Num timesteps: 13200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -189972.20\n",
      "No more requests.\n",
      "Total reward for this episode is -112440.00000000157\n",
      "No more requests.\n",
      "Total reward for this episode is -107500.00000000141\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -98760.00000000128\n",
      "Num timesteps: 13400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -187721.20\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -11660.000000000002\n",
      "------------------------------------\n",
      "| average_route_length | 3         |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.8       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.29e+04  |\n",
      "|    ep_rew_mean       | -1.84e+05 |\n",
      "|    exploration_rate  | 0.151     |\n",
      "| time/                |           |\n",
      "|    episodes          | 272       |\n",
      "|    fps               | 1908      |\n",
      "|    time_elapsed      | 7025      |\n",
      "|    total_timesteps   | 13408235  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 5.91e-05  |\n",
      "|    n_updates         | 3339558   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -8719.999999999998\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -12160.000000000002\n",
      "No more requests.\n",
      "Total reward for this episode is -133419.99999999916\n",
      "No more requests.\n",
      "Total reward for this episode is -123180.00000000042\n",
      "------------------------------------\n",
      "| average_route_length | 3.15      |\n",
      "| blocked_contiguous   | 0.5       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.26e+04  |\n",
      "|    ep_rew_mean       | -1.78e+05 |\n",
      "|    exploration_rate  | 0.14      |\n",
      "| time/                |           |\n",
      "|    episodes          | 276       |\n",
      "|    fps               | 1906      |\n",
      "|    time_elapsed      | 7119      |\n",
      "|    total_timesteps   | 13576641  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00248   |\n",
      "|    n_updates         | 3381660   |\n",
      "------------------------------------\n",
      "Num timesteps: 13600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -178131.40\n",
      "No more requests.\n",
      "Total reward for this episode is -129139.9999999999\n",
      "No more requests.\n",
      "Total reward for this episode is -103520.0000000015\n",
      "No more requests.\n",
      "Total reward for this episode is -75980.00000000054\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -15860.000000000002\n",
      "------------------------------------\n",
      "| average_route_length | 3         |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.95      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.27e+04  |\n",
      "|    ep_rew_mean       | -1.74e+05 |\n",
      "|    exploration_rate  | 0.126     |\n",
      "| time/                |           |\n",
      "|    episodes          | 280       |\n",
      "|    fps               | 1904      |\n",
      "|    time_elapsed      | 7241      |\n",
      "|    total_timesteps   | 13794208  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 7.02e-05  |\n",
      "|    n_updates         | 3436051   |\n",
      "------------------------------------\n",
      "Num timesteps: 13800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -174265.60\n",
      "No more requests.\n",
      "Total reward for this episode is -134040.0000000002\n",
      "No more requests.\n",
      "Total reward for this episode is -77280.00000000073\n",
      "Num timesteps: 14000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -172903.40\n",
      "No more requests.\n",
      "Total reward for this episode is -113340.0000000012\n",
      "No more requests.\n",
      "Total reward for this episode is -108040.000000001\n",
      "------------------------------------\n",
      "| average_route_length | 3.23      |\n",
      "| blocked_contiguous   | 0.429     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.36e+04  |\n",
      "|    ep_rew_mean       | -1.72e+05 |\n",
      "|    exploration_rate  | 0.109     |\n",
      "| time/                |           |\n",
      "|    episodes          | 284       |\n",
      "|    fps               | 1902      |\n",
      "|    time_elapsed      | 7397      |\n",
      "|    total_timesteps   | 14071080  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00643   |\n",
      "|    n_updates         | 3505269   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -140819.99999999913\n",
      "Num timesteps: 14200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -170977.20\n",
      "No more requests.\n",
      "Total reward for this episode is -65300.0000000004\n",
      "No more requests.\n",
      "Total reward for this episode is -99640.00000000081\n",
      "No more requests.\n",
      "Total reward for this episode is -77240.00000000045\n",
      "------------------------------------\n",
      "| average_route_length | 2.71      |\n",
      "| blocked_contiguous   | 0.357     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.33e+04  |\n",
      "|    ep_rew_mean       | -1.65e+05 |\n",
      "|    exploration_rate  | 0.0911    |\n",
      "| time/                |           |\n",
      "|    episodes          | 288       |\n",
      "|    fps               | 1899      |\n",
      "|    time_elapsed      | 7555      |\n",
      "|    total_timesteps   | 14351529  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00108   |\n",
      "|    n_updates         | 3575382   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -7919.999999999996\n",
      "Num timesteps: 14400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -163839.80\n",
      "No more requests.\n",
      "Total reward for this episode is -110960.00000000095\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -11860.0\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -15860.000000000002\n",
      "------------------------------------\n",
      "| average_route_length | 3         |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.95      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.19e+04  |\n",
      "|    ep_rew_mean       | -1.59e+05 |\n",
      "|    exploration_rate  | 0.0844    |\n",
      "| time/                |           |\n",
      "|    episodes          | 292       |\n",
      "|    fps               | 1898      |\n",
      "|    time_elapsed      | 7616      |\n",
      "|    total_timesteps   | 14457594  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 6.13e-05  |\n",
      "|    n_updates         | 3601898   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -104760.00000000118\n",
      "No more requests.\n",
      "Total reward for this episode is -72140.00000000048\n",
      "Num timesteps: 14600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -156213.80\n",
      "No more requests.\n",
      "Total reward for this episode is -31359.999999999873\n",
      "No more requests.\n",
      "Total reward for this episode is -133560.00000000023\n",
      "------------------------------------\n",
      "| average_route_length | 4.1       |\n",
      "| blocked_contiguous   | 0.429     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.5       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.19e+04  |\n",
      "|    ep_rew_mean       | -1.53e+05 |\n",
      "|    exploration_rate  | 0.0666    |\n",
      "| time/                |           |\n",
      "|    episodes          | 296       |\n",
      "|    fps               | 1895      |\n",
      "|    time_elapsed      | 7774      |\n",
      "|    total_timesteps   | 14737318  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00236   |\n",
      "|    n_updates         | 3671829   |\n",
      "------------------------------------\n",
      "Num timesteps: 14800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -152885.80\n",
      "No more requests.\n",
      "Total reward for this episode is -83520.00000000044\n",
      "No more requests.\n",
      "Total reward for this episode is -51980.000000000175\n",
      "No more requests.\n",
      "Total reward for this episode is -78120.00000000041\n",
      "Num timesteps: 15000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -146582.80\n",
      "No more requests.\n",
      "Total reward for this episode is -60680.000000000204\n",
      "------------------------------------\n",
      "| average_route_length | 2.71      |\n",
      "| blocked_contiguous   | 0.357     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.17e+04  |\n",
      "|    ep_rew_mean       | -1.44e+05 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 300       |\n",
      "|    fps               | 1892      |\n",
      "|    time_elapsed      | 7932      |\n",
      "|    total_timesteps   | 15013992  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00074   |\n",
      "|    n_updates         | 3740997   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -60100.00000000016\n",
      "No more requests.\n",
      "Total reward for this episode is -46140.00000000011\n",
      "Num timesteps: 15200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -141277.40\n",
      "No more requests.\n",
      "Total reward for this episode is -56580.00000000013\n",
      "No more requests.\n",
      "Total reward for this episode is -59260.00000000018\n",
      "------------------------------------\n",
      "| average_route_length | 2.64      |\n",
      "| blocked_contiguous   | 0.429     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.16e+04  |\n",
      "|    ep_rew_mean       | -1.38e+05 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 304       |\n",
      "|    fps               | 1889      |\n",
      "|    time_elapsed      | 8088      |\n",
      "|    total_timesteps   | 15286926  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00509   |\n",
      "|    n_updates         | 3809231   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -74300.00000000029\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -3479.9999999999955\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -16860.000000000004\n",
      "Num timesteps: 15400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -133538.40\n",
      "No more requests.\n",
      "Total reward for this episode is -40839.99999999995\n",
      "------------------------------------\n",
      "| average_route_length | 2.33      |\n",
      "| blocked_contiguous   | 0.286     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.06e+04  |\n",
      "|    ep_rew_mean       | -1.31e+05 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 308       |\n",
      "|    fps               | 1888      |\n",
      "|    time_elapsed      | 8182      |\n",
      "|    total_timesteps   | 15451355  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 4.62e-05  |\n",
      "|    n_updates         | 3850338   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -92000.00000000074\n",
      "No more requests.\n",
      "Total reward for this episode is -42680.000000000015\n",
      "Num timesteps: 15600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -126350.20\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -21659.99999999996\n",
      "No more requests.\n",
      "Total reward for this episode is -59300.000000000226\n",
      "------------------------------------\n",
      "| average_route_length | 2.36      |\n",
      "| blocked_contiguous   | 0.357     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.01e+04  |\n",
      "|    ep_rew_mean       | -1.22e+05 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 312       |\n",
      "|    fps               | 1885      |\n",
      "|    time_elapsed      | 8323      |\n",
      "|    total_timesteps   | 15697129  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000336  |\n",
      "|    n_updates         | 3911782   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -57340.00000000017\n",
      "Num timesteps: 15800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -119817.20\n",
      "No more requests.\n",
      "Total reward for this episode is -70280.00000000028\n",
      "No more requests.\n",
      "Total reward for this episode is -44380.000000000124\n",
      "No more requests.\n",
      "Total reward for this episode is -74800.00000000036\n",
      "------------------------------------\n",
      "| average_route_length | 3         |\n",
      "| blocked_contiguous   | 0.5       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6e+04     |\n",
      "|    ep_rew_mean       | -1.15e+05 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 316       |\n",
      "|    fps               | 1883      |\n",
      "|    time_elapsed      | 8483      |\n",
      "|    total_timesteps   | 15977768  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0681    |\n",
      "|    n_updates         | 3981941   |\n",
      "------------------------------------\n",
      "Num timesteps: 16000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -115463.40\n",
      "No more requests.\n",
      "Total reward for this episode is -75420.00000000033\n",
      "No more requests.\n",
      "Total reward for this episode is -26899.99999999997\n",
      "No more requests.\n",
      "Total reward for this episode is -61700.00000000026\n",
      "Num timesteps: 16200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -108629.20\n",
      "No more requests.\n",
      "Total reward for this episode is -75240.0000000003\n",
      "------------------------------------\n",
      "| average_route_length | 2.92      |\n",
      "| blocked_contiguous   | 0.429     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 5.98e+04  |\n",
      "|    ep_rew_mean       | -1.06e+05 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 320       |\n",
      "|    fps               | 1881      |\n",
      "|    time_elapsed      | 8640      |\n",
      "|    total_timesteps   | 16254516  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000111  |\n",
      "|    n_updates         | 4051128   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -62080.00000000031\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -15860.000000000002\n",
      "No more requests.\n",
      "Total reward for this episode is -55680.00000000015\n",
      "Num timesteps: 16400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -103502.20\n",
      "No more requests.\n",
      "Total reward for this episode is -89380.0000000004\n",
      "------------------------------------\n",
      "| average_route_length | 3.42      |\n",
      "| blocked_contiguous   | 0.5       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.4       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 5.95e+04  |\n",
      "|    ep_rew_mean       | -1.02e+05 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 324       |\n",
      "|    fps               | 1879      |\n",
      "|    time_elapsed      | 8760      |\n",
      "|    total_timesteps   | 16464924  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 6.55e-05  |\n",
      "|    n_updates         | 4103730   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -37819.999999999935\n",
      "Num timesteps: 16600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -102445.20\n",
      "No more requests.\n",
      "Total reward for this episode is -61960.00000000023\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -5979.999999999995\n",
      "No more requests.\n",
      "Total reward for this episode is -57440.000000000146\n",
      "------------------------------------\n",
      "| average_route_length | 2.71      |\n",
      "| blocked_contiguous   | 0.357     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 5.93e+04  |\n",
      "|    ep_rew_mean       | -9.75e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 328       |\n",
      "|    fps               | 1877      |\n",
      "|    time_elapsed      | 8888      |\n",
      "|    total_timesteps   | 16688274  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00205   |\n",
      "|    n_updates         | 4159568   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -43620.000000000065\n",
      "Num timesteps: 16800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -96875.60\n",
      "No more requests.\n",
      "Total reward for this episode is -58860.00000000013\n",
      "No more requests.\n",
      "Total reward for this episode is -82260.00000000047\n",
      "No more requests.\n",
      "Total reward for this episode is -43320.00000000006\n",
      "------------------------------------\n",
      "| average_route_length | 2.47      |\n",
      "| blocked_contiguous   | 0.357     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 5.94e+04  |\n",
      "|    ep_rew_mean       | -9.27e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 332       |\n",
      "|    fps               | 1875      |\n",
      "|    time_elapsed      | 9045      |\n",
      "|    total_timesteps   | 16963686  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 8.99      |\n",
      "|    n_updates         | 4228421   |\n",
      "------------------------------------\n",
      "Num timesteps: 17000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -92728.20\n",
      "No more requests.\n",
      "Total reward for this episode is -74200.00000000028\n",
      "No more requests.\n",
      "Total reward for this episode is -61660.000000000204\n",
      "No more requests.\n",
      "Total reward for this episode is -75460.00000000036\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -16860.000000000004\n",
      "------------------------------------\n",
      "| average_route_length | 4         |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.95      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 5.91e+04  |\n",
      "|    ep_rew_mean       | -8.77e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 336       |\n",
      "|    fps               | 1873      |\n",
      "|    time_elapsed      | 9165      |\n",
      "|    total_timesteps   | 17174784  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000218  |\n",
      "|    n_updates         | 4281195   |\n",
      "------------------------------------\n",
      "Num timesteps: 17200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -87727.40\n",
      "No more requests.\n",
      "Total reward for this episode is -75240.00000000035\n",
      "No more requests.\n",
      "Total reward for this episode is -93340.00000000047\n",
      "No more requests.\n",
      "Total reward for this episode is -55040.00000000016\n",
      "Num timesteps: 17400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -85756.20\n",
      "No more requests.\n",
      "Total reward for this episode is -98280.00000000071\n",
      "------------------------------------\n",
      "| average_route_length | 3.67      |\n",
      "| blocked_contiguous   | 0.429     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.4       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 5.94e+04  |\n",
      "|    ep_rew_mean       | -8.67e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 340       |\n",
      "|    fps               | 1871      |\n",
      "|    time_elapsed      | 9323      |\n",
      "|    total_timesteps   | 17451216  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000436  |\n",
      "|    n_updates         | 4350303   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -43540.00000000008\n",
      "No more requests.\n",
      "Total reward for this episode is -61260.000000000204\n",
      "Num timesteps: 17600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -83551.00\n",
      "No more requests.\n",
      "Total reward for this episode is -57280.00000000017\n",
      "No more requests.\n",
      "Total reward for this episode is -72660.00000000028\n",
      "------------------------------------\n",
      "| average_route_length | 2.92      |\n",
      "| blocked_contiguous   | 0.5       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 5.94e+04  |\n",
      "|    ep_rew_mean       | -8.04e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 344       |\n",
      "|    fps               | 1869      |\n",
      "|    time_elapsed      | 9481      |\n",
      "|    total_timesteps   | 17729373  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000633  |\n",
      "|    n_updates         | 4419843   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -47200.00000000004\n",
      "Num timesteps: 17800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -79110.80\n",
      "No more requests.\n",
      "Total reward for this episode is -58260.00000000018\n",
      "No more requests.\n",
      "Total reward for this episode is -78260.00000000042\n",
      "Num timesteps: 18000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -76742.00\n",
      "No more requests.\n",
      "Total reward for this episode is -59020.00000000024\n",
      "------------------------------------\n",
      "| average_route_length | 2.5       |\n",
      "| blocked_contiguous   | 0.429     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 5.93e+04  |\n",
      "|    ep_rew_mean       | -7.52e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 348       |\n",
      "|    fps               | 1868      |\n",
      "|    time_elapsed      | 9638      |\n",
      "|    total_timesteps   | 18005961  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00206   |\n",
      "|    n_updates         | 4488990   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -8779.99999999993\n",
      "No more requests.\n",
      "Total reward for this episode is -74960.00000000042\n",
      "Num timesteps: 18200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -74202.60\n",
      "No more requests.\n",
      "Total reward for this episode is -66920.00000000028\n",
      "No more requests.\n",
      "Total reward for this episode is -57420.00000000022\n",
      "-----------------------------------\n",
      "| average_route_length | 2.57     |\n",
      "| blocked_contiguous   | 0.429    |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.3      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 6.04e+04 |\n",
      "|    ep_rew_mean       | -7.4e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 352      |\n",
      "|    fps               | 1866     |\n",
      "|    time_elapsed      | 9794     |\n",
      "|    total_timesteps   | 18277934 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0312   |\n",
      "|    n_updates         | 4556983  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -76740.00000000036\n",
      "Num timesteps: 18400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -73180.60\n",
      "No more requests.\n",
      "Total reward for this episode is -54680.00000000015\n",
      "No more requests.\n",
      "Total reward for this episode is -71180.00000000032\n",
      "No more requests.\n",
      "Total reward for this episode is -62840.00000000031\n",
      "------------------------------------\n",
      "| average_route_length | 2.79      |\n",
      "| blocked_contiguous   | 0.286     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.04e+04  |\n",
      "|    ep_rew_mean       | -7.11e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 356       |\n",
      "|    fps               | 1864      |\n",
      "|    time_elapsed      | 9951      |\n",
      "|    total_timesteps   | 18553165  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.557     |\n",
      "|    n_updates         | 4625791   |\n",
      "------------------------------------\n",
      "Num timesteps: 18600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -71081.00\n",
      "No more requests.\n",
      "Total reward for this episode is -33239.99999999996\n",
      "No more requests.\n",
      "Total reward for this episode is -54960.00000000022\n",
      "No more requests.\n",
      "Total reward for this episode is -59960.00000000014\n",
      "Num timesteps: 18800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -68665.40\n",
      "No more requests.\n",
      "Total reward for this episode is -44660.000000000065\n",
      "------------------------------------\n",
      "| average_route_length | 2.6       |\n",
      "| blocked_contiguous   | 0.357     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.09e+04  |\n",
      "|    ep_rew_mean       | -6.73e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 360       |\n",
      "|    fps               | 1862      |\n",
      "|    time_elapsed      | 10108     |\n",
      "|    total_timesteps   | 18826105  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000181  |\n",
      "|    n_updates         | 4694026   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -88080.00000000042\n",
      "No more requests.\n",
      "Total reward for this episode is -58760.00000000015\n",
      "Num timesteps: 19000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -64893.40\n",
      "No more requests.\n",
      "Total reward for this episode is -12239.999999999922\n",
      "No more requests.\n",
      "Total reward for this episode is -108340.0000000007\n",
      "------------------------------------\n",
      "| average_route_length | 3.91      |\n",
      "| blocked_contiguous   | 0.429     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.45      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.14e+04  |\n",
      "|    ep_rew_mean       | -6.45e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 364       |\n",
      "|    fps               | 1860      |\n",
      "|    time_elapsed      | 10268     |\n",
      "|    total_timesteps   | 19107079  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0018    |\n",
      "|    n_updates         | 4764269   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -112860.00000000058\n",
      "Num timesteps: 19200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -64222.20\n",
      "No more requests.\n",
      "Total reward for this episode is -92860.00000000063\n",
      "No more requests.\n",
      "Total reward for this episode is -15999.999999999905\n",
      "No more requests.\n",
      "Total reward for this episode is -61680.00000000021\n",
      "------------------------------------\n",
      "| average_route_length | 2.79      |\n",
      "| blocked_contiguous   | 0.286     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.19e+04  |\n",
      "|    ep_rew_mean       | -6.36e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 368       |\n",
      "|    fps               | 1859      |\n",
      "|    time_elapsed      | 10426     |\n",
      "|    total_timesteps   | 19383813  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00883   |\n",
      "|    n_updates         | 4833453   |\n",
      "------------------------------------\n",
      "Num timesteps: 19400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -63550.60\n",
      "No more requests.\n",
      "Total reward for this episode is -29799.999999999956\n",
      "No more requests.\n",
      "Total reward for this episode is -56740.00000000021\n",
      "No more requests.\n",
      "Total reward for this episode is -61220.00000000024\n",
      "Num timesteps: 19600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -61741.80\n",
      "No more requests.\n",
      "Total reward for this episode is -80540.00000000035\n",
      "------------------------------------\n",
      "| average_route_length | 3.54      |\n",
      "| blocked_contiguous   | 0.357     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.25e+04  |\n",
      "|    ep_rew_mean       | -6.24e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 372       |\n",
      "|    fps               | 1857      |\n",
      "|    time_elapsed      | 10584     |\n",
      "|    total_timesteps   | 19662055  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00333   |\n",
      "|    n_updates         | 4903013   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -57940.00000000019\n",
      "Num timesteps: 19800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -62829.00\n",
      "No more requests.\n",
      "Total reward for this episode is -91360.00000000048\n",
      "No more requests.\n",
      "Total reward for this episode is -63780.000000000204\n",
      "No more requests.\n",
      "Total reward for this episode is -58560.000000000175\n",
      "------------------------------------\n",
      "| average_route_length | 2.57      |\n",
      "| blocked_contiguous   | 0.357     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.36e+04  |\n",
      "|    ep_rew_mean       | -6.23e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 376       |\n",
      "|    fps               | 1855      |\n",
      "|    time_elapsed      | 10742     |\n",
      "|    total_timesteps   | 19937308  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0776    |\n",
      "|    n_updates         | 4971826   |\n",
      "------------------------------------\n",
      "Num timesteps: 20000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -62284.00\n",
      "No more requests.\n",
      "Total reward for this episode is -58700.00000000017\n",
      "No more requests.\n",
      "Total reward for this episode is -89740.00000000044\n",
      "No more requests.\n",
      "Total reward for this episode is -61300.0000000002\n",
      "Num timesteps: 20200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -61400.00\n",
      "No more requests.\n",
      "Total reward for this episode is -72100.00000000029\n",
      "-----------------------------------\n",
      "| average_route_length | 2.77     |\n",
      "| blocked_contiguous   | 0.429    |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.35     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 6.42e+04 |\n",
      "|    ep_rew_mean       | -6.2e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 380      |\n",
      "|    fps               | 1854     |\n",
      "|    time_elapsed      | 10898    |\n",
      "|    total_timesteps   | 20211451 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00018  |\n",
      "|    n_updates         | 5040362  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -27239.9999999999\n",
      "No more requests.\n",
      "Total reward for this episode is -55300.00000000018\n",
      "Num timesteps: 20400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -60680.20\n",
      "No more requests.\n",
      "Total reward for this episode is -58780.00000000017\n",
      "No more requests.\n",
      "Total reward for this episode is -60400.00000000017\n",
      "------------------------------------\n",
      "| average_route_length | 2.86      |\n",
      "| blocked_contiguous   | 0.357     |\n",
      "| blocked_continuous   | 0.0714    |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.42e+04  |\n",
      "|    ep_rew_mean       | -5.97e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 384       |\n",
      "|    fps               | 1853      |\n",
      "|    time_elapsed      | 11055     |\n",
      "|    total_timesteps   | 20486815  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0478    |\n",
      "|    n_updates         | 5109203   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -42659.99999999999\n",
      "Num timesteps: 20600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -58676.60\n",
      "No more requests.\n",
      "Total reward for this episode is -31879.999999999894\n",
      "No more requests.\n",
      "Total reward for this episode is -59520.00000000023\n",
      "No more requests.\n",
      "Total reward for this episode is -75220.00000000036\n",
      "-----------------------------------\n",
      "| average_route_length | 2.85     |\n",
      "| blocked_contiguous   | 0.429    |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.35     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 6.41e+04 |\n",
      "|    ep_rew_mean       | -5.8e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 388      |\n",
      "|    fps               | 1851     |\n",
      "|    time_elapsed      | 11211    |\n",
      "|    total_timesteps   | 20761793 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000276 |\n",
      "|    n_updates         | 5177948  |\n",
      "-----------------------------------\n",
      "Num timesteps: 20800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -58026.00\n",
      "No more requests.\n",
      "Total reward for this episode is -69060.00000000028\n",
      "No more requests.\n",
      "Total reward for this episode is -92460.00000000063\n",
      "No more requests.\n",
      "Total reward for this episode is -25179.999999999924\n",
      "Num timesteps: 21000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -58596.80\n",
      "No more requests.\n",
      "Total reward for this episode is -61240.00000000023\n",
      "-----------------------------------\n",
      "| average_route_length | 2.86     |\n",
      "| blocked_contiguous   | 0.357    |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.3      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 6.58e+04 |\n",
      "|    ep_rew_mean       | -5.9e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 392      |\n",
      "|    fps               | 1850     |\n",
      "|    time_elapsed      | 11369    |\n",
      "|    total_timesteps   | 21039825 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0062   |\n",
      "|    n_updates         | 5247456  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -92940.00000000042\n",
      "No more requests.\n",
      "Total reward for this episode is -28679.999999999916\n",
      "Num timesteps: 21200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -58293.40\n",
      "No more requests.\n",
      "Total reward for this episode is -67840.00000000032\n",
      "No more requests.\n",
      "Total reward for this episode is -60100.00000000022\n",
      "------------------------------------\n",
      "| average_route_length | 2.64      |\n",
      "| blocked_contiguous   | 0.286     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.58e+04  |\n",
      "|    ep_rew_mean       | -5.79e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 396       |\n",
      "|    fps               | 1849      |\n",
      "|    time_elapsed      | 11529     |\n",
      "|    total_timesteps   | 21319880  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00813   |\n",
      "|    n_updates         | 5317469   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -43180.000000000044\n",
      "Num timesteps: 21400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -57625.20\n",
      "No more requests.\n",
      "Total reward for this episode is -22940.000000000036\n",
      "No more requests.\n",
      "Total reward for this episode is -77980.00000000042\n",
      "No more requests.\n",
      "Total reward for this episode is -73020.0000000003\n",
      "------------------------------------\n",
      "| average_route_length | 2.92      |\n",
      "| blocked_contiguous   | 0.5       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.58e+04  |\n",
      "|    ep_rew_mean       | -5.75e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 400       |\n",
      "|    fps               | 1847      |\n",
      "|    time_elapsed      | 11685     |\n",
      "|    total_timesteps   | 21591900  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0249    |\n",
      "|    n_updates         | 5385474   |\n",
      "------------------------------------\n",
      "Num timesteps: 21600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -57456.80\n",
      "No more requests.\n",
      "Total reward for this episode is -70200.00000000029\n",
      "No more requests.\n",
      "Total reward for this episode is -77160.00000000038\n",
      "Num timesteps: 21800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -57868.00\n",
      "No more requests.\n",
      "Total reward for this episode is -56840.00000000008\n",
      "No more requests.\n",
      "Total reward for this episode is -61020.00000000014\n",
      "------------------------------------\n",
      "| average_route_length | 2.79      |\n",
      "| blocked_contiguous   | 0.357     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.58e+04  |\n",
      "|    ep_rew_mean       | -5.78e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 404       |\n",
      "|    fps               | 1846      |\n",
      "|    time_elapsed      | 11843     |\n",
      "|    total_timesteps   | 21868181  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000529  |\n",
      "|    n_updates         | 5454545   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -60760.000000000204\n",
      "Num timesteps: 22000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -57647.80\n",
      "No more requests.\n",
      "Total reward for this episode is -57180.00000000015\n",
      "No more requests.\n",
      "Total reward for this episode is -39339.99999999995\n",
      "No more requests.\n",
      "Total reward for this episode is -43580.000000000044\n",
      "------------------------------------\n",
      "| average_route_length | 2.4       |\n",
      "| blocked_contiguous   | 0.286     |\n",
      "| blocked_continuous   | 0.0714    |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.69e+04  |\n",
      "|    ep_rew_mean       | -5.84e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 408       |\n",
      "|    fps               | 1845      |\n",
      "|    time_elapsed      | 12000     |\n",
      "|    total_timesteps   | 22142058  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.279     |\n",
      "|    n_updates         | 5523014   |\n",
      "------------------------------------\n",
      "Num timesteps: 22200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -58448.20\n",
      "No more requests.\n",
      "Total reward for this episode is -37199.999999999876\n",
      "No more requests.\n",
      "Total reward for this episode is -73780.00000000028\n",
      "No more requests.\n",
      "Total reward for this episode is -63240.00000000025\n",
      "Num timesteps: 22400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -58630.60\n",
      "No more requests.\n",
      "Total reward for this episode is -78200.00000000041\n",
      "------------------------------------\n",
      "| average_route_length | 3.08      |\n",
      "| blocked_contiguous   | 0.357     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.72e+04  |\n",
      "|    ep_rew_mean       | -5.88e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 412       |\n",
      "|    fps               | 1843      |\n",
      "|    time_elapsed      | 12157     |\n",
      "|    total_timesteps   | 22415622  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000715  |\n",
      "|    n_updates         | 5591405   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -71620.00000000029\n",
      "No more requests.\n",
      "Total reward for this episode is -41080.000000000044\n",
      "Num timesteps: 22600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -58880.40\n",
      "No more requests.\n",
      "Total reward for this episode is -44340.00000000001\n",
      "No more requests.\n",
      "Total reward for this episode is -51360.00000000005\n",
      "------------------------------------\n",
      "| average_route_length | 2.27      |\n",
      "| blocked_contiguous   | 0.357     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.71e+04  |\n",
      "|    ep_rew_mean       | -5.85e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 416       |\n",
      "|    fps               | 1842      |\n",
      "|    time_elapsed      | 12315     |\n",
      "|    total_timesteps   | 22692348  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.181     |\n",
      "|    n_updates         | 5660586   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -43540.00000000008\n",
      "Num timesteps: 22800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -58221.80\n",
      "No more requests.\n",
      "Total reward for this episode is -29279.999999999887\n",
      "No more requests.\n",
      "Total reward for this episode is -70560.0000000003\n",
      "No more requests.\n",
      "Total reward for this episode is -56120.000000000175\n",
      "------------------------------------\n",
      "| average_route_length | 2.5       |\n",
      "| blocked_contiguous   | 0.357     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.71e+04  |\n",
      "|    ep_rew_mean       | -5.81e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 420       |\n",
      "|    fps               | 1841      |\n",
      "|    time_elapsed      | 12472     |\n",
      "|    total_timesteps   | 22967964  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00247   |\n",
      "|    n_updates         | 5729490   |\n",
      "------------------------------------\n",
      "Num timesteps: 23000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -58143.00\n",
      "No more requests.\n",
      "Total reward for this episode is -63060.00000000032\n",
      "No more requests.\n",
      "Total reward for this episode is -73620.00000000023\n",
      "No more requests.\n",
      "Total reward for this episode is -58840.00000000019\n",
      "Num timesteps: 23200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -58767.60\n",
      "No more requests.\n",
      "Total reward for this episode is -87520.00000000042\n",
      "------------------------------------\n",
      "| average_route_length | 3.17      |\n",
      "| blocked_contiguous   | 0.429     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.4       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.77e+04  |\n",
      "|    ep_rew_mean       | -5.87e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 424       |\n",
      "|    fps               | 1840      |\n",
      "|    time_elapsed      | 12628     |\n",
      "|    total_timesteps   | 23239710  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000145  |\n",
      "|    n_updates         | 5797427   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -41980.00000000003\n",
      "No more requests.\n",
      "Total reward for this episode is -27439.999999999884\n",
      "Num timesteps: 23400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -58443.40\n",
      "No more requests.\n",
      "Total reward for this episode is -58820.00000000016\n",
      "No more requests.\n",
      "Total reward for this episode is -75260.0000000003\n",
      "-----------------------------------\n",
      "| average_route_length | 2.92     |\n",
      "| blocked_contiguous   | 0.357    |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.35     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 6.83e+04 |\n",
      "|    ep_rew_mean       | -5.9e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 428      |\n",
      "|    fps               | 1839     |\n",
      "|    time_elapsed      | 12789    |\n",
      "|    total_timesteps   | 23521501 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 1.78e-05 |\n",
      "|    n_updates         | 5867875  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -57320.00000000016\n",
      "Num timesteps: 23600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -59185.60\n",
      "No more requests.\n",
      "Total reward for this episode is -43580.000000000044\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -1479.9999999999955\n",
      "No more requests.\n",
      "Total reward for this episode is -41880.00000000011\n",
      "------------------------------------\n",
      "| average_route_length | 2.33      |\n",
      "| blocked_contiguous   | 0.357     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.79e+04  |\n",
      "|    ep_rew_mean       | -5.82e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 432       |\n",
      "|    fps               | 1838      |\n",
      "|    time_elapsed      | 12919     |\n",
      "|    total_timesteps   | 23749462  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 1.94e-05  |\n",
      "|    n_updates         | 5924865   |\n",
      "------------------------------------\n",
      "Num timesteps: 23800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -58205.00\n",
      "No more requests.\n",
      "Total reward for this episode is -41960.00000000005\n",
      "No more requests.\n",
      "Total reward for this episode is -29339.99999999997\n",
      "No more requests.\n",
      "Total reward for this episode is -59700.00000000017\n",
      "Num timesteps: 24000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -57401.80\n",
      "No more requests.\n",
      "Total reward for this episode is -23020.00000000001\n",
      "------------------------------------\n",
      "| average_route_length | 2.06      |\n",
      "| blocked_contiguous   | 0.286     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.2       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.85e+04  |\n",
      "|    ep_rew_mean       | -5.75e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 436       |\n",
      "|    fps               | 1837      |\n",
      "|    time_elapsed      | 13075     |\n",
      "|    total_timesteps   | 24023767  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00166   |\n",
      "|    n_updates         | 5993441   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -53040.00000000001\n",
      "No more requests.\n",
      "Total reward for this episode is -58440.000000000226\n",
      "Num timesteps: 24200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -56898.00\n",
      "No more requests.\n",
      "Total reward for this episode is -41720.00000000005\n",
      "No more requests.\n",
      "Total reward for this episode is -58680.000000000255\n",
      "------------------------------------\n",
      "| average_route_length | 2.43      |\n",
      "| blocked_contiguous   | 0.214     |\n",
      "| blocked_continuous   | 0.0714    |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.85e+04  |\n",
      "|    ep_rew_mean       | -5.63e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 440       |\n",
      "|    fps               | 1836      |\n",
      "|    time_elapsed      | 13235     |\n",
      "|    total_timesteps   | 24304021  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000723  |\n",
      "|    n_updates         | 6063505   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -26259.999999999884\n",
      "Num timesteps: 24400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -56196.00\n",
      "No more requests.\n",
      "Total reward for this episode is -37239.99999999993\n",
      "No more requests.\n",
      "Total reward for this episode is -39899.999999999956\n",
      "No more requests.\n",
      "Total reward for this episode is -74200.0000000003\n",
      "------------------------------------\n",
      "| average_route_length | 3         |\n",
      "| blocked_contiguous   | 0.429     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.85e+04  |\n",
      "|    ep_rew_mean       | -5.57e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 444       |\n",
      "|    fps               | 1835      |\n",
      "|    time_elapsed      | 13393     |\n",
      "|    total_timesteps   | 24578349  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000327  |\n",
      "|    n_updates         | 6132087   |\n",
      "------------------------------------\n",
      "Num timesteps: 24600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -55692.40\n",
      "No more requests.\n",
      "Total reward for this episode is -41100.00000000004\n",
      "No more requests.\n",
      "Total reward for this episode is -8060.000000000099\n",
      "No more requests.\n",
      "Total reward for this episode is -61800.00000000025\n",
      "Num timesteps: 24800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -54964.80\n",
      "No more requests.\n",
      "Total reward for this episode is -40239.99999999993\n",
      "------------------------------------\n",
      "| average_route_length | 2.33      |\n",
      "| blocked_contiguous   | 0.286     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.84e+04  |\n",
      "|    ep_rew_mean       | -5.48e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 448       |\n",
      "|    fps               | 1833      |\n",
      "|    time_elapsed      | 13549     |\n",
      "|    total_timesteps   | 24849748  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0091    |\n",
      "|    n_updates         | 6199936   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -73260.00000000032\n",
      "No more requests.\n",
      "Total reward for this episode is -57820.00000000013\n",
      "Num timesteps: 25000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -55252.40\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -1539.9999999999977\n",
      "No more requests.\n",
      "Total reward for this episode is -74080.00000000028\n",
      "------------------------------------\n",
      "| average_route_length | 3         |\n",
      "| blocked_contiguous   | 0.357     |\n",
      "| blocked_continuous   | 0.0714    |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.8e+04   |\n",
      "|    ep_rew_mean       | -5.47e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 452       |\n",
      "|    fps               | 1833      |\n",
      "|    time_elapsed      | 13681     |\n",
      "|    total_timesteps   | 25079849  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000111  |\n",
      "|    n_updates         | 6257462   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -27559.999999999898\n",
      "Num timesteps: 25200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -54162.80\n",
      "No more requests.\n",
      "Total reward for this episode is -74400.00000000028\n",
      "No more requests.\n",
      "Total reward for this episode is -55520.00000000003\n",
      "No more requests.\n",
      "Total reward for this episode is -56460.000000000175\n",
      "-----------------------------------\n",
      "| average_route_length | 2.57     |\n",
      "| blocked_contiguous   | 0.357    |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.3      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 6.8e+04  |\n",
      "|    ep_rew_mean       | -5.4e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 456      |\n",
      "|    fps               | 1832     |\n",
      "|    time_elapsed      | 13839    |\n",
      "|    total_timesteps   | 25355644 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0144   |\n",
      "|    n_updates         | 6326410  |\n",
      "-----------------------------------\n",
      "Num timesteps: 25400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -54036.60\n",
      "No more requests.\n",
      "Total reward for this episode is -23439.99999999992\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -6039.9999999999945\n",
      "No more requests.\n",
      "Total reward for this episode is -46320.00000000003\n",
      "No more requests.\n",
      "Total reward for this episode is -41299.99999999999\n",
      "------------------------------------\n",
      "| average_route_length | 2.4       |\n",
      "| blocked_contiguous   | 0.286     |\n",
      "| blocked_continuous   | 0.0714    |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.75e+04  |\n",
      "|    ep_rew_mean       | -5.33e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 460       |\n",
      "|    fps               | 1831      |\n",
      "|    time_elapsed      | 13968     |\n",
      "|    total_timesteps   | 25579951  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00127   |\n",
      "|    n_updates         | 6382487   |\n",
      "------------------------------------\n",
      "Num timesteps: 25600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -53273.80\n",
      "No more requests.\n",
      "Total reward for this episode is -84740.00000000044\n",
      "No more requests.\n",
      "Total reward for this episode is -94260.00000000054\n",
      "No more requests.\n",
      "Total reward for this episode is -73400.00000000032\n",
      "Num timesteps: 25800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -54312.00\n",
      "No more requests.\n",
      "Total reward for this episode is -64240.00000000021\n",
      "-----------------------------------\n",
      "| average_route_length | 2.93     |\n",
      "| blocked_contiguous   | 0.357    |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.3      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 6.75e+04 |\n",
      "|    ep_rew_mean       | -5.4e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 464      |\n",
      "|    fps               | 1830     |\n",
      "|    time_elapsed      | 14124    |\n",
      "|    total_timesteps   | 25853139 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.759    |\n",
      "|    n_updates         | 6450784  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -36079.99999999997\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -16860.000000000004\n",
      "No more requests.\n",
      "Total reward for this episode is -81060.00000000041\n",
      "Num timesteps: 26000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -53093.20\n",
      "No more requests.\n",
      "Total reward for this episode is -58840.00000000021\n",
      "------------------------------------\n",
      "| average_route_length | 2.64      |\n",
      "| blocked_contiguous   | 0.429     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.68e+04  |\n",
      "|    ep_rew_mean       | -5.32e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 468       |\n",
      "|    fps               | 1829      |\n",
      "|    time_elapsed      | 14244     |\n",
      "|    total_timesteps   | 26062743  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0117    |\n",
      "|    n_updates         | 6503185   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -53340.00000000001\n",
      "Num timesteps: 26200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -53300.20\n",
      "No more requests.\n",
      "Total reward for this episode is -77740.00000000032\n",
      "No more requests.\n",
      "Total reward for this episode is -47540.000000000124\n",
      "No more requests.\n",
      "Total reward for this episode is -74200.00000000029\n",
      "------------------------------------\n",
      "| average_route_length | 3         |\n",
      "| blocked_contiguous   | 0.5       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.68e+04  |\n",
      "|    ep_rew_mean       | -5.34e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 472       |\n",
      "|    fps               | 1828      |\n",
      "|    time_elapsed      | 14403     |\n",
      "|    total_timesteps   | 26338856  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000437  |\n",
      "|    n_updates         | 6572213   |\n",
      "------------------------------------\n",
      "Num timesteps: 26400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -53413.00\n",
      "No more requests.\n",
      "Total reward for this episode is -91460.00000000044\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -7479.999999999997\n",
      "No more requests.\n",
      "Total reward for this episode is -40940.00000000003\n",
      "No more requests.\n",
      "Total reward for this episode is -68180.0000000003\n",
      "------------------------------------\n",
      "| average_route_length | 2.62      |\n",
      "| blocked_contiguous   | 0.357     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.63e+04  |\n",
      "|    ep_rew_mean       | -5.28e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 476       |\n",
      "|    fps               | 1827      |\n",
      "|    time_elapsed      | 14531     |\n",
      "|    total_timesteps   | 26563589  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00159   |\n",
      "|    n_updates         | 6628397   |\n",
      "------------------------------------\n",
      "Num timesteps: 26600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -52771.60\n",
      "No more requests.\n",
      "Total reward for this episode is -41140.00000000009\n",
      "No more requests.\n",
      "Total reward for this episode is -72620.0000000003\n",
      "No more requests.\n",
      "Total reward for this episode is -43320.000000000065\n",
      "Num timesteps: 26800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -52140.00\n",
      "No more requests.\n",
      "Total reward for this episode is -54220.000000000175\n",
      "-----------------------------------\n",
      "| average_route_length | 2.43     |\n",
      "| blocked_contiguous   | 0.429    |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.3      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 6.63e+04 |\n",
      "|    ep_rew_mean       | -5.2e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 480      |\n",
      "|    fps               | 1827     |\n",
      "|    time_elapsed      | 14690    |\n",
      "|    total_timesteps   | 26840571 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0148   |\n",
      "|    n_updates         | 6697642  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -25059.99999999991\n",
      "No more requests.\n",
      "Total reward for this episode is -13499.999999999955\n",
      "Num timesteps: 27000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -51521.40\n",
      "No more requests.\n",
      "Total reward for this episode is -57440.00000000019\n",
      "No more requests.\n",
      "Total reward for this episode is -74380.00000000029\n",
      "------------------------------------\n",
      "| average_route_length | 3         |\n",
      "| blocked_contiguous   | 0.5       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.63e+04  |\n",
      "|    ep_rew_mean       | -5.16e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 484       |\n",
      "|    fps               | 1826      |\n",
      "|    time_elapsed      | 14846     |\n",
      "|    total_timesteps   | 27114198  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0925    |\n",
      "|    n_updates         | 6766049   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -60040.00000000021\n",
      "Num timesteps: 27200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -51819.60\n",
      "No more requests.\n",
      "Total reward for this episode is -40199.99999999998\n",
      "No more requests.\n",
      "Total reward for this episode is -27759.999999999887\n",
      "No more requests.\n",
      "Total reward for this episode is -56940.00000000013\n",
      "------------------------------------\n",
      "| average_route_length | 2.57      |\n",
      "| blocked_contiguous   | 0.429     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.63e+04  |\n",
      "|    ep_rew_mean       | -5.14e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 488       |\n",
      "|    fps               | 1825      |\n",
      "|    time_elapsed      | 15003     |\n",
      "|    total_timesteps   | 27388799  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00187   |\n",
      "|    n_updates         | 6834699   |\n",
      "------------------------------------\n",
      "Num timesteps: 27400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -51402.40\n",
      "No more requests.\n",
      "Total reward for this episode is -43259.99999999999\n",
      "No more requests.\n",
      "Total reward for this episode is -44020.00000000008\n",
      "No more requests.\n",
      "Total reward for this episode is -73040.0000000003\n",
      "Num timesteps: 27600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -51138.60\n",
      "No more requests.\n",
      "Total reward for this episode is -61540.000000000146\n",
      "------------------------------------\n",
      "| average_route_length | 2.86      |\n",
      "| blocked_contiguous   | 0.286     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.62e+04  |\n",
      "|    ep_rew_mean       | -5.12e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 492       |\n",
      "|    fps               | 1824      |\n",
      "|    time_elapsed      | 15159     |\n",
      "|    total_timesteps   | 27660470  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 2.39e-05  |\n",
      "|    n_updates         | 6902617   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -76320.0000000003\n",
      "No more requests.\n",
      "Total reward for this episode is -39799.99999999999\n",
      "Num timesteps: 27800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -51191.60\n",
      "No more requests.\n",
      "Total reward for this episode is -57400.00000000017\n",
      "No more requests.\n",
      "Total reward for this episode is -79620.00000000035\n",
      "------------------------------------\n",
      "| average_route_length | 3.15      |\n",
      "| blocked_contiguous   | 0.5       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.62e+04  |\n",
      "|    ep_rew_mean       | -5.14e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 496       |\n",
      "|    fps               | 1823      |\n",
      "|    time_elapsed      | 15317     |\n",
      "|    total_timesteps   | 27935844  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.12      |\n",
      "|    n_updates         | 6971460   |\n",
      "------------------------------------\n",
      "Num timesteps: 28000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -51387.40\n",
      "No more requests.\n",
      "Total reward for this episode is -55260.00000000014\n",
      "No more requests.\n",
      "Total reward for this episode is -37919.999999999854\n",
      "No more requests.\n",
      "Total reward for this episode is -55660.00000000015\n",
      "Num timesteps: 28200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -51434.80\n",
      "No more requests.\n",
      "Total reward for this episode is -59100.000000000204\n",
      "------------------------------------\n",
      "| average_route_length | 2.79      |\n",
      "| blocked_contiguous   | 0.357     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.62e+04  |\n",
      "|    ep_rew_mean       | -5.13e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 500       |\n",
      "|    fps               | 1822      |\n",
      "|    time_elapsed      | 15473     |\n",
      "|    total_timesteps   | 28208145  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00133   |\n",
      "|    n_updates         | 7039536   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -26279.999999999894\n",
      "No more requests.\n",
      "Total reward for this episode is -38719.999999999985\n",
      "Num timesteps: 28400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -50472.00\n",
      "No more requests.\n",
      "Total reward for this episode is -29439.999999999905\n",
      "No more requests.\n",
      "Total reward for this episode is -72340.00000000033\n",
      "------------------------------------\n",
      "| average_route_length | 2.77      |\n",
      "| blocked_contiguous   | 0.429     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.61e+04  |\n",
      "|    ep_rew_mean       | -5.04e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 504       |\n",
      "|    fps               | 1822      |\n",
      "|    time_elapsed      | 15629     |\n",
      "|    total_timesteps   | 28481634  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 5.65e-05  |\n",
      "|    n_updates         | 7107908   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -75180.00000000026\n",
      "Num timesteps: 28600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -50560.40\n",
      "No more requests.\n",
      "Total reward for this episode is -61220.00000000016\n",
      "No more requests.\n",
      "Total reward for this episode is -44260.00000000002\n",
      "No more requests.\n",
      "Total reward for this episode is -42180.00000000003\n",
      "------------------------------------\n",
      "| average_route_length | 2.47      |\n",
      "| blocked_contiguous   | 0.286     |\n",
      "| blocked_continuous   | 0.0714    |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.61e+04  |\n",
      "|    ep_rew_mean       | -5.06e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 508       |\n",
      "|    fps               | 1821      |\n",
      "|    time_elapsed      | 15786     |\n",
      "|    total_timesteps   | 28752671  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0111    |\n",
      "|    n_updates         | 7175667   |\n",
      "------------------------------------\n",
      "Num timesteps: 28800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -50636.00\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -13839.999999999958\n",
      "No more requests.\n",
      "Total reward for this episode is -56040.000000000146\n",
      "No more requests.\n",
      "Total reward for this episode is -25859.999999999905\n",
      "Num timesteps: 29000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -49847.60\n",
      "No more requests.\n",
      "Total reward for this episode is -71940.00000000028\n",
      "------------------------------------\n",
      "| average_route_length | 2.92      |\n",
      "| blocked_contiguous   | 0.5       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.59e+04  |\n",
      "|    ep_rew_mean       | -4.98e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 512       |\n",
      "|    fps               | 1820      |\n",
      "|    time_elapsed      | 15932     |\n",
      "|    total_timesteps   | 29008673  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000123  |\n",
      "|    n_updates         | 7239668   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -55940.00000000021\n",
      "No more requests.\n",
      "Total reward for this episode is -11719.999999999953\n",
      "Num timesteps: 29200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -49334.60\n",
      "No more requests.\n",
      "Total reward for this episode is -41099.99999999991\n",
      "No more requests.\n",
      "Total reward for this episode is -40960.00000000004\n",
      "------------------------------------\n",
      "| average_route_length | 2.33      |\n",
      "| blocked_contiguous   | 0.357     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.59e+04  |\n",
      "|    ep_rew_mean       | -4.93e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 516       |\n",
      "|    fps               | 1820      |\n",
      "|    time_elapsed      | 16088     |\n",
      "|    total_timesteps   | 29281745  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000234  |\n",
      "|    n_updates         | 7307936   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -79000.0000000004\n",
      "Num timesteps: 29400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -49655.80\n",
      "No more requests.\n",
      "Total reward for this episode is -43180.00000000007\n",
      "No more requests.\n",
      "Total reward for this episode is -37299.99999999991\n",
      "No more requests.\n",
      "Total reward for this episode is -39800.00000000004\n",
      "------------------------------------\n",
      "| average_route_length | 2.2       |\n",
      "| blocked_contiguous   | 0.357     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.59e+04  |\n",
      "|    ep_rew_mean       | -4.93e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 520       |\n",
      "|    fps               | 1819      |\n",
      "|    time_elapsed      | 16245     |\n",
      "|    total_timesteps   | 29556864  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0133    |\n",
      "|    n_updates         | 7376715   |\n",
      "------------------------------------\n",
      "Num timesteps: 29600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -49301.00\n",
      "No more requests.\n",
      "Total reward for this episode is -42360.00000000011\n",
      "No more requests.\n",
      "Total reward for this episode is -57840.00000000017\n",
      "No more requests.\n",
      "Total reward for this episode is -60600.000000000146\n",
      "Num timesteps: 29800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -48953.80\n",
      "No more requests.\n",
      "Total reward for this episode is -7519.999999999984\n",
      "------------------------------------\n",
      "| average_route_length | 1.88      |\n",
      "| blocked_contiguous   | 0.214     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 6.59e+04  |\n",
      "|    ep_rew_mean       | -4.82e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 524       |\n",
      "|    fps               | 1818      |\n",
      "|    time_elapsed      | 16402     |\n",
      "|    total_timesteps   | 29829523  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00158   |\n",
      "|    n_updates         | 7444880   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -51700.000000000015\n",
      "No more requests.\n",
      "Total reward for this episode is -42500.00000000002\n",
      "Num timesteps: 30000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -48403.60\n"
     ]
    }
   ],
   "source": [
    "# Create the callback: check every 1000 steps\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=200000, log_dir=log_dir)\n",
    "tensor_callback = TensorboardCallback()\n",
    "\n",
    "# create model\n",
    "\n",
    "nodeList, linkList = createPresetTopology(\"VSNL\", 5)\n",
    "requestList = generateRequests(nodeList, 20, 2, 30)\n",
    "\n",
    "user = User()\n",
    "env = game_gym(nodeList, linkList, requestList, user, dynamic=True)\n",
    "eveon = Monitor(env, log_dir)\n",
    "\n",
    "# check_env(eveon, warn=True)\n",
    "model = DQN('MlpPolicy', eveon, verbose=1, buffer_size=100000, device='cuda', \n",
    "learning_starts=50000, exploration_fraction=0.5, learning_rate=0.0001,\n",
    "gamma=0.8, tensorboard_log='./dqn_tensorboard/')\n",
    "\n",
    "# train\n",
    "model_name = \"Dict_15\"\n",
    "model.learn(total_timesteps=30000000, callback=[callback, tensor_callback])\n",
    "model.save(model_name)\n",
    "\n",
    "# tensorboard --logdir ./dqn_tensorboard/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VSNL Topology Selected\n",
      "Traffic load is: 15.0\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "No more requests.\n",
      "Total reward for this episode is -30000.0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "No more requests.\n",
      "Total reward for this episode is -79030.0\n",
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "No more requests.\n",
      "Total reward for this episode is -60638.0\n",
      "0\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# create env\n",
    "\n",
    "nodeList, linkList = createPresetTopology(\"VSNL\", 5)\n",
    "requestList = generateRequests(nodeList, 20, 2, 30)\n",
    "\n",
    "user = User()\n",
    "env = game_gym(nodeList, linkList, requestList, user, dynamic=True)\n",
    "\n",
    "# test\n",
    "model = DQN.load('Dict_15', env=env, device='cpu')\n",
    "\n",
    "obs = env.reset()\n",
    "while True :\n",
    "    action, states_ = model.predict(obs, deterministic=True )\n",
    "    # action = 6\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    # plt.imshow(obs)\n",
    "    # plt.show()\n",
    "    print(action)\n",
    "    if dones == True:\n",
    "        # print(env.reward)\n",
    "\n",
    "        # with open('info.json', 'w') as outfile:\n",
    "        #     json.dump(info, outfile)\n",
    "\n",
    "        env.reset()\n",
    "\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6e1680fa3f035ba33798eeb42b0c1edda6a758b70597993f042faeae31b18ae3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('4thYearprojectEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
