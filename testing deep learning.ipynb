{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.9.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from time import sleep\n",
    "from IPython.display import clear_output, display\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "import optical_network_game.game_gym\n",
    "importlib.reload(optical_network_game.game_gym)\n",
    "from optical_network_game.game_gym import *\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq:\n",
    "    :param log_dir: Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: Verbosity level.\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose: int = 1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, 'best_model')\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "          # Retrieve training reward\n",
    "          x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "          if len(x) > 0:\n",
    "              # Mean training reward over the last 100 episodes\n",
    "              mean_reward = np.mean(y[-100:])\n",
    "              if self.verbose > 0:\n",
    "                print(f\"Num timesteps: {self.num_timesteps}\")\n",
    "                print(f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\")\n",
    "\n",
    "              # New best model, you could save the agent here\n",
    "              if mean_reward > self.best_mean_reward:\n",
    "                  self.best_mean_reward = mean_reward\n",
    "                  # Example for saving best model\n",
    "                  if self.verbose > 0:\n",
    "                    print(f\"Saving new best model to {self.save_path}\")\n",
    "                  self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Create log dir\n",
    "log_dir = os.path.join(os.getcwd(), \"tmp/\")\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "# Create the callback: check every 1000 steps\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=10000, log_dir=log_dir)\n",
    "\n",
    "# create model\n",
    "\n",
    "nodeList, linkList = createTestTopology()\n",
    "requestList = generateRequests(nodeList, 30)\n",
    "\n",
    "user = User()\n",
    "env = game_gym(nodeList, linkList, requestList, user)\n",
    "eveon = Monitor(env, log_dir)\n",
    "\n",
    "# check_env(eveon, warn=True)\n",
    "model = DQN('MlpPolicy', eveon, verbose=1, buffer_size=10000, device='cuda', \n",
    "learning_starts=10000, exploration_fraction=0.9, learning_rate=0.000025)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256, 3)\n",
      "Total reward for this episode is 0.5700000000000001\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 1.8050000000000002\n",
      "(256, 256, 3)\n",
      "Num timesteps: 10000\n",
      "Best mean reward: -inf - Last mean reward per episode: 1.19\n",
      "Saving new best model to c:\\Users\\tkate\\Desktop\\Year 4\\project\\Game part\\4th-Year-Project\\tmp/best_model\n",
      "Total reward for this episode is 4.639999999999999\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 0.77\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 3.23e+03 |\n",
      "|    ep_rew_mean      | 1.95     |\n",
      "|    exploration_rate | 0.973    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 106      |\n",
      "|    time_elapsed     | 121      |\n",
      "|    total_timesteps  | 12914    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 0.000137 |\n",
      "|    n_updates        | 728      |\n",
      "----------------------------------\n",
      "Total reward for this episode is -2.1499999999999995\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 3.639999999999999\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -0.865\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -0.7649999999999997\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.45e+03 |\n",
      "|    ep_rew_mean      | 0.956    |\n",
      "|    exploration_rate | 0.959    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 80       |\n",
      "|    time_elapsed     | 243      |\n",
      "|    total_timesteps  | 19567    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 3.02e-05 |\n",
      "|    n_updates        | 2391     |\n",
      "----------------------------------\n",
      "Num timesteps: 20000\n",
      "Best mean reward: 1.19 - Last mean reward per episode: 0.96\n",
      "Total reward for this episode is -2.3000000000000003\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 1.8550000000000006\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 3.289999999999999\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -0.7649999999999995\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.18e+03 |\n",
      "|    ep_rew_mean      | 0.81     |\n",
      "|    exploration_rate | 0.945    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 71       |\n",
      "|    time_elapsed     | 365      |\n",
      "|    total_timesteps  | 26153    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 5.17e-05 |\n",
      "|    n_updates        | 4038     |\n",
      "----------------------------------\n",
      "Total reward for this episode is 1.7549999999999997\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -0.9649999999999999\n",
      "(256, 256, 3)\n",
      "Num timesteps: 30000\n",
      "Best mean reward: 1.19 - Last mean reward per episode: 0.75\n",
      "Total reward for this episode is -2.3500000000000005\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -3.785\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.05e+03 |\n",
      "|    ep_rew_mean      | 0.274    |\n",
      "|    exploration_rate | 0.931    |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 67       |\n",
      "|    time_elapsed     | 487      |\n",
      "|    total_timesteps  | 32813    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 1.65e-05 |\n",
      "|    n_updates        | 5703     |\n",
      "----------------------------------\n",
      "Total reward for this episode is 7.894999999999996\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -2.5\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 6.559999999999996\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 1.9050000000000014\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.97e+03 |\n",
      "|    ep_rew_mean      | 0.912    |\n",
      "|    exploration_rate | 0.917    |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 64       |\n",
      "|    time_elapsed     | 609      |\n",
      "|    total_timesteps  | 39315    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 0.00288  |\n",
      "|    n_updates        | 7328     |\n",
      "----------------------------------\n",
      "Num timesteps: 40000\n",
      "Best mean reward: 1.19 - Last mean reward per episode: 0.91\n",
      "Total reward for this episode is 4.824999999999997\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -2.3999999999999995\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 6.209999999999997\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 0.47000000000000003\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.91e+03 |\n",
      "|    ep_rew_mean      | 1.14     |\n",
      "|    exploration_rate | 0.903    |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 62       |\n",
      "|    time_elapsed     | 731      |\n",
      "|    total_timesteps  | 45942    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 0.0188   |\n",
      "|    n_updates        | 8985     |\n",
      "----------------------------------\n",
      "Total reward for this episode is 3.6399999999999997\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -2.3500000000000005\n",
      "(256, 256, 3)\n",
      "Num timesteps: 50000\n",
      "Best mean reward: 1.19 - Last mean reward per episode: 1.10\n",
      "Total reward for this episode is 0.5200000000000002\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -1.1650000000000005\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.87e+03 |\n",
      "|    ep_rew_mean      | 1        |\n",
      "|    exploration_rate | 0.889    |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 61       |\n",
      "|    time_elapsed     | 853      |\n",
      "|    total_timesteps  | 52362    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 7.22e-06 |\n",
      "|    n_updates        | 10590    |\n",
      "----------------------------------\n",
      "Total reward for this episode is -0.8650000000000002\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -3.8350000000000004\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 2.005\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -3.635\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.84e+03 |\n",
      "|    ep_rew_mean      | 0.677    |\n",
      "|    exploration_rate | 0.876    |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 60       |\n",
      "|    time_elapsed     | 975      |\n",
      "|    total_timesteps  | 58937    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 7.48e-06 |\n",
      "|    n_updates        | 12234    |\n",
      "----------------------------------\n",
      "Num timesteps: 60000\n",
      "Best mean reward: 1.19 - Last mean reward per episode: 0.68\n",
      "Total reward for this episode is 3.49\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 6.159999999999999\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 3.6399999999999992\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 3.6399999999999983\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.82e+03 |\n",
      "|    ep_rew_mean      | 1.07     |\n",
      "|    exploration_rate | 0.862    |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 1097     |\n",
      "|    total_timesteps  | 65489    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 0.00448  |\n",
      "|    n_updates        | 13872    |\n",
      "----------------------------------\n",
      "Total reward for this episode is -3.635\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 3.389999999999999\n",
      "(256, 256, 3)\n",
      "Num timesteps: 70000\n",
      "Best mean reward: 1.19 - Last mean reward per episode: 1.01\n",
      "Total reward for this episode is 0.5700000000000001\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 0.6200000000000008\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.8e+03  |\n",
      "|    ep_rew_mean      | 0.988    |\n",
      "|    exploration_rate | 0.848    |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 59       |\n",
      "|    time_elapsed     | 1221     |\n",
      "|    total_timesteps  | 72180    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 0.00288  |\n",
      "|    n_updates        | 15544    |\n",
      "----------------------------------\n",
      "Total reward for this episode is 4.974999999999999\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 0.5200000000000007\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 4.725\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 4.874999999999999\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.79e+03 |\n",
      "|    ep_rew_mean      | 1.24     |\n",
      "|    exploration_rate | 0.834    |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 58       |\n",
      "|    time_elapsed     | 1342     |\n",
      "|    total_timesteps  | 78700    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 0.00225  |\n",
      "|    n_updates        | 17174    |\n",
      "----------------------------------\n",
      "Num timesteps: 80000\n",
      "Best mean reward: 1.19 - Last mean reward per episode: 1.24\n",
      "Saving new best model to c:\\Users\\tkate\\Desktop\\Year 4\\project\\Game part\\4th-Year-Project\\tmp/best_model\n",
      "Total reward for this episode is -0.8649999999999995\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 0.5200000000000002\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 0.5200000000000005\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 3.3900000000000006\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.77e+03 |\n",
      "|    ep_rew_mean      | 1.21     |\n",
      "|    exploration_rate | 0.82     |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 58       |\n",
      "|    time_elapsed     | 1462     |\n",
      "|    total_timesteps  | 85162    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 0.0187   |\n",
      "|    n_updates        | 18790    |\n",
      "----------------------------------\n",
      "Total reward for this episode is -0.8149999999999995\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 0.7199999999999995\n",
      "(256, 256, 3)\n",
      "Num timesteps: 90000\n",
      "Best mean reward: 1.24 - Last mean reward per episode: 1.16\n",
      "Total reward for this episode is -0.7149999999999999\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 2.005000000000001\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.76e+03 |\n",
      "|    ep_rew_mean      | 1.14     |\n",
      "|    exploration_rate | 0.806    |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 57       |\n",
      "|    time_elapsed     | 1585     |\n",
      "|    total_timesteps  | 91734    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 0.00134  |\n",
      "|    n_updates        | 20433    |\n",
      "----------------------------------\n",
      "Total reward for this episode is 3.4399999999999986\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 1.9549999999999998\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -0.6649999999999998\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 1.9049999999999994\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.75e+03 |\n",
      "|    ep_rew_mean      | 1.18     |\n",
      "|    exploration_rate | 0.793    |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 57       |\n",
      "|    time_elapsed     | 1707     |\n",
      "|    total_timesteps  | 98021    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 0.0199   |\n",
      "|    n_updates        | 22005    |\n",
      "----------------------------------\n",
      "Total reward for this episode is 1.8549999999999998\n",
      "(256, 256, 3)\n",
      "Num timesteps: 100000\n",
      "Best mean reward: 1.24 - Last mean reward per episode: 1.19\n",
      "Total reward for this episode is 2.205\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 3.1900000000000004\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -0.8149999999999997\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.74e+03 |\n",
      "|    ep_rew_mean      | 1.21     |\n",
      "|    exploration_rate | 0.78     |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 57       |\n",
      "|    time_elapsed     | 1829     |\n",
      "|    total_timesteps  | 104386   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 1.49e-05 |\n",
      "|    n_updates        | 23596    |\n",
      "----------------------------------\n",
      "Total reward for this episode is 7.7449999999999966\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 0.37000000000000055\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 1.9549999999999992\n",
      "(256, 256, 3)\n",
      "Num timesteps: 110000\n",
      "Best mean reward: 1.24 - Last mean reward per episode: 1.31\n",
      "Saving new best model to c:\\Users\\tkate\\Desktop\\Year 4\\project\\Game part\\4th-Year-Project\\tmp/best_model\n",
      "Total reward for this episode is -2.35\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.73e+03 |\n",
      "|    ep_rew_mean      | 1.25     |\n",
      "|    exploration_rate | 0.766    |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 56       |\n",
      "|    time_elapsed     | 1950     |\n",
      "|    total_timesteps  | 110748   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 1.55e-05 |\n",
      "|    n_updates        | 25186    |\n",
      "----------------------------------\n",
      "Total reward for this episode is -1.065\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 0.6200000000000001\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -1.0650000000000004\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 2.054999999999999\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.72e+03 |\n",
      "|    ep_rew_mean      | 1.19     |\n",
      "|    exploration_rate | 0.753    |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 56       |\n",
      "|    time_elapsed     | 2073     |\n",
      "|    total_timesteps  | 117143   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 0.0029   |\n",
      "|    n_updates        | 26785    |\n",
      "----------------------------------\n",
      "Total reward for this episode is -1.015\n",
      "(256, 256, 3)\n",
      "Num timesteps: 120000\n",
      "Best mean reward: 1.31 - Last mean reward per episode: 1.16\n",
      "Total reward for this episode is 3.4399999999999995\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 1.9050000000000005\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 3.1899999999999986\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.72e+03 |\n",
      "|    ep_rew_mean      | 1.23     |\n",
      "|    exploration_rate | 0.739    |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 56       |\n",
      "|    time_elapsed     | 2194     |\n",
      "|    total_timesteps  | 123517   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 1.45e-05 |\n",
      "|    n_updates        | 28379    |\n",
      "----------------------------------\n",
      "Total reward for this episode is -0.915\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 2.155\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -0.7649999999999995\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 1.855\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.71e+03 |\n",
      "|    ep_rew_mean      | 1.19     |\n",
      "|    exploration_rate | 0.726    |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 56       |\n",
      "|    time_elapsed     | 2315     |\n",
      "|    total_timesteps  | 129915   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 7.73e-06 |\n",
      "|    n_updates        | 29978    |\n",
      "----------------------------------\n",
      "Num timesteps: 130000\n",
      "Best mean reward: 1.31 - Last mean reward per episode: 1.19\n",
      "Total reward for this episode is 3.389999999999999\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -2.249999999999999\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 2.0549999999999997\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 3.4900000000000015\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.7e+03  |\n",
      "|    ep_rew_mean      | 1.22     |\n",
      "|    exploration_rate | 0.713    |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 55       |\n",
      "|    time_elapsed     | 2436     |\n",
      "|    total_timesteps  | 136175   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 9.12e-06 |\n",
      "|    n_updates        | 31543    |\n",
      "----------------------------------\n",
      "Total reward for this episode is -0.9649999999999999\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 0.4699999999999998\n",
      "(256, 256, 3)\n",
      "Num timesteps: 140000\n",
      "Best mean reward: 1.31 - Last mean reward per episode: 1.18\n",
      "Total reward for this episode is 3.4900000000000007\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 2.105\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.7e+03  |\n",
      "|    ep_rew_mean      | 1.22     |\n",
      "|    exploration_rate | 0.699    |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 55       |\n",
      "|    time_elapsed     | 2558     |\n",
      "|    total_timesteps  | 142440   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 0.0194   |\n",
      "|    n_updates        | 33109    |\n",
      "----------------------------------\n",
      "Total reward for this episode is -2.05\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -0.8649999999999998\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 4.875\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -1.0150000000000003\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.69e+03 |\n",
      "|    ep_rew_mean      | 1.17     |\n",
      "|    exploration_rate | 0.686    |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 55       |\n",
      "|    time_elapsed     | 2679     |\n",
      "|    total_timesteps  | 148532   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 5.29e-06 |\n",
      "|    n_updates        | 34632    |\n",
      "----------------------------------\n",
      "Num timesteps: 150000\n",
      "Best mean reward: 1.31 - Last mean reward per episode: 1.17\n",
      "Total reward for this episode is 0.7200000000000002\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -0.8149999999999997\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 3.339999999999998\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 0.3699999999999999\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.68e+03 |\n",
      "|    ep_rew_mean      | 1.16     |\n",
      "|    exploration_rate | 0.673    |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 55       |\n",
      "|    time_elapsed     | 2801     |\n",
      "|    total_timesteps  | 154848   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 0.0158   |\n",
      "|    n_updates        | 36211    |\n",
      "----------------------------------\n",
      "Total reward for this episode is 1.9050000000000002\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 2.204999999999997\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 0.7200000000000002\n",
      "(256, 256, 3)\n",
      "Num timesteps: 160000\n",
      "Best mean reward: 1.31 - Last mean reward per episode: 1.18\n",
      "Total reward for this episode is -1.015000000000001\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.68e+03 |\n",
      "|    ep_rew_mean      | 1.15     |\n",
      "|    exploration_rate | 0.66     |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 55       |\n",
      "|    time_elapsed     | 2923     |\n",
      "|    total_timesteps  | 161143   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 1.1e-05  |\n",
      "|    n_updates        | 37785    |\n",
      "----------------------------------\n",
      "Total reward for this episode is 0.4700000000000003\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -2.25\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 3.489999999999999\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 3.4399999999999995\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.68e+03 |\n",
      "|    ep_rew_mean      | 1.16     |\n",
      "|    exploration_rate | 0.646    |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 55       |\n",
      "|    time_elapsed     | 3045     |\n",
      "|    total_timesteps  | 167534   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 1.14e-05 |\n",
      "|    n_updates        | 39383    |\n",
      "----------------------------------\n",
      "Total reward for this episode is 3.3899999999999983\n",
      "(256, 256, 3)\n",
      "Num timesteps: 170000\n",
      "Best mean reward: 1.31 - Last mean reward per episode: 1.19\n",
      "Total reward for this episode is 1.9549999999999994\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 2.0549999999999997\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 2.1549999999999994\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.61e+03 |\n",
      "|    ep_rew_mean      | 1.18     |\n",
      "|    exploration_rate | 0.633    |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 54       |\n",
      "|    time_elapsed     | 3168     |\n",
      "|    total_timesteps  | 173905   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 1.5e-05  |\n",
      "|    n_updates        | 40976    |\n",
      "----------------------------------\n",
      "Total reward for this episode is -2.15\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 0.6700000000000008\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -0.8149999999999993\n",
      "(256, 256, 3)\n",
      "Num timesteps: 180000\n",
      "Best mean reward: 1.31 - Last mean reward per episode: 1.15\n",
      "Total reward for this episode is 1.8549999999999998\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.61e+03 |\n",
      "|    ep_rew_mean      | 1.17     |\n",
      "|    exploration_rate | 0.62     |\n",
      "| time/               |          |\n",
      "|    episodes         | 108      |\n",
      "|    fps              | 54       |\n",
      "|    time_elapsed     | 3290     |\n",
      "|    total_timesteps  | 180236   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 1.22e-05 |\n",
      "|    n_updates        | 42558    |\n",
      "----------------------------------\n",
      "Total reward for this episode is 3.689999999999998\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 4.874999999999998\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 1.9550000000000005\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 3.3399999999999985\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.6e+03  |\n",
      "|    ep_rew_mean      | 1.29     |\n",
      "|    exploration_rate | 0.606    |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 54       |\n",
      "|    time_elapsed     | 3411     |\n",
      "|    total_timesteps  | 186399   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 0.00171  |\n",
      "|    n_updates        | 44099    |\n",
      "----------------------------------\n",
      "Total reward for this episode is 0.470000000000001\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 0.47\n",
      "(256, 256, 3)\n",
      "Num timesteps: 190000\n",
      "Best mean reward: 1.31 - Last mean reward per episode: 1.29\n",
      "Total reward for this episode is -0.865\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 1.9550000000000012\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.6e+03  |\n",
      "|    ep_rew_mean      | 1.37     |\n",
      "|    exploration_rate | 0.594    |\n",
      "| time/               |          |\n",
      "|    episodes         | 116      |\n",
      "|    fps              | 54       |\n",
      "|    time_elapsed     | 3534     |\n",
      "|    total_timesteps  | 192390   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 0.0177   |\n",
      "|    n_updates        | 45597    |\n",
      "----------------------------------\n",
      "Total reward for this episode is -2.2500000000000004\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 2.0050000000000003\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -0.9149999999999991\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 2.104999999999999\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.59e+03 |\n",
      "|    ep_rew_mean      | 1.24     |\n",
      "|    exploration_rate | 0.581    |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 54       |\n",
      "|    time_elapsed     | 3655     |\n",
      "|    total_timesteps  | 198588   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 9.94e-06 |\n",
      "|    n_updates        | 47146    |\n",
      "----------------------------------\n",
      "Num timesteps: 200000\n",
      "Best mean reward: 1.31 - Last mean reward per episode: 1.24\n",
      "Total reward for this episode is 0.5700000000000003\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -5.170000000000001\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 1.9550000000000003\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -0.7649999999999998\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.59e+03 |\n",
      "|    ep_rew_mean      | 1.11     |\n",
      "|    exploration_rate | 0.568    |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 54       |\n",
      "|    time_elapsed     | 3778     |\n",
      "|    total_timesteps  | 204785   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 0.00172  |\n",
      "|    n_updates        | 48696    |\n",
      "----------------------------------\n",
      "Total reward for this episode is 4.925\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 1.955\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 4.8249999999999975\n",
      "(256, 256, 3)\n",
      "Num timesteps: 210000\n",
      "Best mean reward: 1.31 - Last mean reward per episode: 1.21\n",
      "Total reward for this episode is 0.5699999999999992\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.58e+03 |\n",
      "|    ep_rew_mean      | 1.23     |\n",
      "|    exploration_rate | 0.555    |\n",
      "| time/               |          |\n",
      "|    episodes         | 128      |\n",
      "|    fps              | 54       |\n",
      "|    time_elapsed     | 3900     |\n",
      "|    total_timesteps  | 210852   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 4.84e-06 |\n",
      "|    n_updates        | 50212    |\n",
      "----------------------------------\n",
      "Total reward for this episode is 3.289999999999999\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 0.7699999999999982\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 0.47000000000000025\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 3.589999999999999\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.58e+03 |\n",
      "|    ep_rew_mean      | 1.37     |\n",
      "|    exploration_rate | 0.541    |\n",
      "| time/               |          |\n",
      "|    episodes         | 132      |\n",
      "|    fps              | 54       |\n",
      "|    time_elapsed     | 4022     |\n",
      "|    total_timesteps  | 217346   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 0.00448  |\n",
      "|    n_updates        | 51836    |\n",
      "----------------------------------\n",
      "Total reward for this episode is 0.5200000000000002\n",
      "(256, 256, 3)\n",
      "Num timesteps: 220000\n",
      "Best mean reward: 1.31 - Last mean reward per episode: 1.34\n",
      "Saving new best model to c:\\Users\\tkate\\Desktop\\Year 4\\project\\Game part\\4th-Year-Project\\tmp/best_model\n",
      "Total reward for this episode is 2.1049999999999978\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 4.974999999999998\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -1.015\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.58e+03 |\n",
      "|    ep_rew_mean      | 1.27     |\n",
      "|    exploration_rate | 0.528    |\n",
      "| time/               |          |\n",
      "|    episodes         | 136      |\n",
      "|    fps              | 53       |\n",
      "|    time_elapsed     | 4144     |\n",
      "|    total_timesteps  | 223727   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 0.00228  |\n",
      "|    n_updates        | 53431    |\n",
      "----------------------------------\n",
      "Total reward for this episode is -1.0649999999999993\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 1.905\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 2.0050000000000003\n",
      "(256, 256, 3)\n",
      "Num timesteps: 230000\n",
      "Best mean reward: 1.34 - Last mean reward per episode: 1.29\n",
      "Total reward for this episode is 0.6700000000000002\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.58e+03 |\n",
      "|    ep_rew_mean      | 1.29     |\n",
      "|    exploration_rate | 0.514    |\n",
      "| time/               |          |\n",
      "|    episodes         | 140      |\n",
      "|    fps              | 53       |\n",
      "|    time_elapsed     | 4266     |\n",
      "|    total_timesteps  | 230137   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 6.24e-06 |\n",
      "|    n_updates        | 55034    |\n",
      "----------------------------------\n",
      "Total reward for this episode is 0.6700000000000002\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 2.005\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 0.31999999999999995\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 0.52\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.58e+03 |\n",
      "|    ep_rew_mean      | 1.18     |\n",
      "|    exploration_rate | 0.501    |\n",
      "| time/               |          |\n",
      "|    episodes         | 144      |\n",
      "|    fps              | 53       |\n",
      "|    time_elapsed     | 4387     |\n",
      "|    total_timesteps  | 236548   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 6.93e-06 |\n",
      "|    n_updates        | 56636    |\n",
      "----------------------------------\n",
      "Total reward for this episode is 5.124999999999997\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 0.31999999999999973\n",
      "(256, 256, 3)\n",
      "Num timesteps: 240000\n",
      "Best mean reward: 1.34 - Last mean reward per episode: 1.24\n",
      "Total reward for this episode is 0.72\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -2.55\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.58e+03 |\n",
      "|    ep_rew_mean      | 1.18     |\n",
      "|    exploration_rate | 0.487    |\n",
      "| time/               |          |\n",
      "|    episodes         | 148      |\n",
      "|    fps              | 53       |\n",
      "|    time_elapsed     | 4508     |\n",
      "|    total_timesteps  | 242892   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 1.4e-06  |\n",
      "|    n_updates        | 58222    |\n",
      "----------------------------------\n",
      "Total reward for this episode is 1.7550000000000001\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -2.750000000000001\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 0.5699999999999998\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 4.874999999999998\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57e+03 |\n",
      "|    ep_rew_mean      | 1.21     |\n",
      "|    exploration_rate | 0.474    |\n",
      "| time/               |          |\n",
      "|    episodes         | 152      |\n",
      "|    fps              | 53       |\n",
      "|    time_elapsed     | 4629     |\n",
      "|    total_timesteps  | 249102   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 2.8e-06  |\n",
      "|    n_updates        | 59775    |\n",
      "----------------------------------\n",
      "Num timesteps: 250000\n",
      "Best mean reward: 1.34 - Last mean reward per episode: 1.21\n",
      "Total reward for this episode is 2.2550000000000003\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -1.0650000000000006\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -2.3500000000000005\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 0.4199999999999996\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.58e+03 |\n",
      "|    ep_rew_mean      | 1.14     |\n",
      "|    exploration_rate | 0.46     |\n",
      "| time/               |          |\n",
      "|    episodes         | 156      |\n",
      "|    fps              | 53       |\n",
      "|    time_elapsed     | 4751     |\n",
      "|    total_timesteps  | 255572   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 0.00401  |\n",
      "|    n_updates        | 61392    |\n",
      "----------------------------------\n",
      "Total reward for this episode is -0.8649999999999998\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 0.41999999999999993\n",
      "(256, 256, 3)\n",
      "Num timesteps: 260000\n",
      "Best mean reward: 1.34 - Last mean reward per episode: 1.09\n",
      "Total reward for this episode is 2.004999999999999\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 3.5899999999999985\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.58e+03 |\n",
      "|    ep_rew_mean      | 1.12     |\n",
      "|    exploration_rate | 0.447    |\n",
      "| time/               |          |\n",
      "|    episodes         | 160      |\n",
      "|    fps              | 53       |\n",
      "|    time_elapsed     | 4872     |\n",
      "|    total_timesteps  | 262023   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 0.0157   |\n",
      "|    n_updates        | 63005    |\n",
      "----------------------------------\n",
      "Total reward for this episode is -0.7150000000000001\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 3.4899999999999984\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -0.9649999999999999\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 0.46999999999999975\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.58e+03 |\n",
      "|    ep_rew_mean      | 1.07     |\n",
      "|    exploration_rate | 0.433    |\n",
      "| time/               |          |\n",
      "|    episodes         | 164      |\n",
      "|    fps              | 53       |\n",
      "|    time_elapsed     | 4994     |\n",
      "|    total_timesteps  | 268393   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 7.16e-06 |\n",
      "|    n_updates        | 64598    |\n",
      "----------------------------------\n",
      "Num timesteps: 270000\n",
      "Best mean reward: 1.34 - Last mean reward per episode: 1.07\n",
      "Total reward for this episode is 4.924999999999999\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 6.509999999999996\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 6.359999999999996\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 0.47000000000000003\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.58e+03 |\n",
      "|    ep_rew_mean      | 1.25     |\n",
      "|    exploration_rate | 0.42     |\n",
      "| time/               |          |\n",
      "|    episodes         | 168      |\n",
      "|    fps              | 53       |\n",
      "|    time_elapsed     | 5115     |\n",
      "|    total_timesteps  | 274814   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 4.26e-06 |\n",
      "|    n_updates        | 66203    |\n",
      "----------------------------------\n",
      "Total reward for this episode is 1.9050000000000002\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 1.9049999999999998\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 0.6700000000000002\n",
      "(256, 256, 3)\n",
      "Num timesteps: 280000\n",
      "Best mean reward: 1.34 - Last mean reward per episode: 1.25\n",
      "Total reward for this episode is 3.4900000000000007\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.58e+03 |\n",
      "|    ep_rew_mean      | 1.25     |\n",
      "|    exploration_rate | 0.406    |\n",
      "| time/               |          |\n",
      "|    episodes         | 172      |\n",
      "|    fps              | 53       |\n",
      "|    time_elapsed     | 5237     |\n",
      "|    total_timesteps  | 281223   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 1.1e-05  |\n",
      "|    n_updates        | 67805    |\n",
      "----------------------------------\n",
      "Total reward for this episode is -3.8850000000000007\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 3.4400000000000013\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 1.9050000000000005\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 0.5700000000000005\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57e+03 |\n",
      "|    ep_rew_mean      | 1.25     |\n",
      "|    exploration_rate | 0.393    |\n",
      "| time/               |          |\n",
      "|    episodes         | 176      |\n",
      "|    fps              | 53       |\n",
      "|    time_elapsed     | 5358     |\n",
      "|    total_timesteps  | 287320   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 0.00173  |\n",
      "|    n_updates        | 69329    |\n",
      "----------------------------------\n",
      "Total reward for this episode is -2.500000000000001\n",
      "(256, 256, 3)\n",
      "Num timesteps: 290000\n",
      "Best mean reward: 1.34 - Last mean reward per episode: 1.19\n",
      "Total reward for this episode is -2.3999999999999995\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 1.9050000000000005\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -0.9649999999999999\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57e+03 |\n",
      "|    ep_rew_mean      | 1.14     |\n",
      "|    exploration_rate | 0.38     |\n",
      "| time/               |          |\n",
      "|    episodes         | 180      |\n",
      "|    fps              | 53       |\n",
      "|    time_elapsed     | 5481     |\n",
      "|    total_timesteps  | 293645   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 1.98e-06 |\n",
      "|    n_updates        | 70911    |\n",
      "----------------------------------\n",
      "Total reward for this episode is 0.470000000000001\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 2.1050000000000004\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 0.5700000000000001\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -1.0149999999999997\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57e+03 |\n",
      "|    ep_rew_mean      | 1.11     |\n",
      "|    exploration_rate | 0.367    |\n",
      "| time/               |          |\n",
      "|    episodes         | 184      |\n",
      "|    fps              | 53       |\n",
      "|    time_elapsed     | 5603     |\n",
      "|    total_timesteps  | 299919   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 5.96e-06 |\n",
      "|    n_updates        | 72479    |\n",
      "----------------------------------\n",
      "Num timesteps: 300000\n",
      "Best mean reward: 1.34 - Last mean reward per episode: 1.11\n",
      "Total reward for this episode is 0.1700000000000001\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -2.3500000000000005\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 0.7200000000000006\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -3.785\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.58e+03 |\n",
      "|    ep_rew_mean      | 1.05     |\n",
      "|    exploration_rate | 0.354    |\n",
      "| time/               |          |\n",
      "|    episodes         | 188      |\n",
      "|    fps              | 53       |\n",
      "|    time_elapsed     | 5724     |\n",
      "|    total_timesteps  | 306180   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 4.12e-06 |\n",
      "|    n_updates        | 74044    |\n",
      "----------------------------------\n",
      "Total reward for this episode is 4.774999999999997\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -2.1499999999999995\n",
      "(256, 256, 3)\n",
      "Num timesteps: 310000\n",
      "Best mean reward: 1.34 - Last mean reward per episode: 1.08\n",
      "Total reward for this episode is 0.41999999999999993\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 1.9050000000000002\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57e+03 |\n",
      "|    ep_rew_mean      | 1.06     |\n",
      "|    exploration_rate | 0.341    |\n",
      "| time/               |          |\n",
      "|    episodes         | 192      |\n",
      "|    fps              | 53       |\n",
      "|    time_elapsed     | 5846     |\n",
      "|    total_timesteps  | 312290   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 3.36e-06 |\n",
      "|    n_updates        | 75572    |\n",
      "----------------------------------\n",
      "Total reward for this episode is -0.815\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 0.6200000000000006\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -5.12\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 0.5700000000000001\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57e+03 |\n",
      "|    ep_rew_mean      | 0.979    |\n",
      "|    exploration_rate | 0.327    |\n",
      "| time/               |          |\n",
      "|    episodes         | 196      |\n",
      "|    fps              | 53       |\n",
      "|    time_elapsed     | 5970     |\n",
      "|    total_timesteps  | 318580   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 0.00176  |\n",
      "|    n_updates        | 77144    |\n",
      "----------------------------------\n",
      "Num timesteps: 320000\n",
      "Best mean reward: 1.34 - Last mean reward per episode: 0.98\n",
      "Total reward for this episode is 0.6700000000000002\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -0.8150000000000004\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -5.22\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 0.5700000000000003\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57e+03 |\n",
      "|    ep_rew_mean      | 0.879    |\n",
      "|    exploration_rate | 0.314    |\n",
      "| time/               |          |\n",
      "|    episodes         | 200      |\n",
      "|    fps              | 53       |\n",
      "|    time_elapsed     | 6093     |\n",
      "|    total_timesteps  | 324906   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 0.007    |\n",
      "|    n_updates        | 78726    |\n",
      "----------------------------------\n",
      "Total reward for this episode is -2.15\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 0.5700000000000003\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 2.1550000000000002\n",
      "(256, 256, 3)\n",
      "Num timesteps: 330000\n",
      "Best mean reward: 1.34 - Last mean reward per episode: 0.81\n",
      "Total reward for this episode is -0.7649999999999995\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57e+03 |\n",
      "|    ep_rew_mean      | 0.782    |\n",
      "|    exploration_rate | 0.301    |\n",
      "| time/               |          |\n",
      "|    episodes         | 204      |\n",
      "|    fps              | 53       |\n",
      "|    time_elapsed     | 6216     |\n",
      "|    total_timesteps  | 331015   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 0.00587  |\n",
      "|    n_updates        | 80253    |\n",
      "----------------------------------\n",
      "Total reward for this episode is 3.339999999999999\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 2.155\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -0.865\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -2.450000000000001\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57e+03 |\n",
      "|    ep_rew_mean      | 0.808    |\n",
      "|    exploration_rate | 0.288    |\n",
      "| time/               |          |\n",
      "|    episodes         | 208      |\n",
      "|    fps              | 53       |\n",
      "|    time_elapsed     | 6337     |\n",
      "|    total_timesteps  | 337152   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 0.00462  |\n",
      "|    n_updates        | 81787    |\n",
      "----------------------------------\n",
      "Total reward for this episode is 0.8699999999999999\n",
      "(256, 256, 3)\n",
      "Num timesteps: 340000\n",
      "Best mean reward: 1.34 - Last mean reward per episode: 0.78\n",
      "Total reward for this episode is -2.0500000000000003\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 3.4900000000000007\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -0.8650000000000002\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57e+03 |\n",
      "|    ep_rew_mean      | 0.684    |\n",
      "|    exploration_rate | 0.275    |\n",
      "| time/               |          |\n",
      "|    episodes         | 212      |\n",
      "|    fps              | 53       |\n",
      "|    time_elapsed     | 6458     |\n",
      "|    total_timesteps  | 343266   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 1.08e-05 |\n",
      "|    n_updates        | 83316    |\n",
      "----------------------------------\n",
      "Total reward for this episode is 2.205000000000001\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 2.255\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -0.9649999999999996\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -2.2499999999999996\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57e+03 |\n",
      "|    ep_rew_mean      | 0.676    |\n",
      "|    exploration_rate | 0.262    |\n",
      "| time/               |          |\n",
      "|    episodes         | 216      |\n",
      "|    fps              | 53       |\n",
      "|    time_elapsed     | 6581     |\n",
      "|    total_timesteps  | 349479   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 1.17e-05 |\n",
      "|    n_updates        | 84869    |\n",
      "----------------------------------\n",
      "Num timesteps: 350000\n",
      "Best mean reward: 1.34 - Last mean reward per episode: 0.68\n",
      "Total reward for this episode is 0.6199999999999979\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -3.835000000000001\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 0.5700000000000001\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 0.6700000000000008\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57e+03 |\n",
      "|    ep_rew_mean      | 0.647    |\n",
      "|    exploration_rate | 0.249    |\n",
      "| time/               |          |\n",
      "|    episodes         | 220      |\n",
      "|    fps              | 53       |\n",
      "|    time_elapsed     | 6704     |\n",
      "|    total_timesteps  | 355761   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 0.0333   |\n",
      "|    n_updates        | 86440    |\n",
      "----------------------------------\n",
      "Total reward for this episode is -0.9149999999999998\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 3.39\n",
      "(256, 256, 3)\n",
      "Num timesteps: 360000\n",
      "Best mean reward: 1.34 - Last mean reward per episode: 0.72\n",
      "Total reward for this episode is -3.6350000000000007\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -0.8649999999999998\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57e+03 |\n",
      "|    ep_rew_mean      | 0.66     |\n",
      "|    exploration_rate | 0.236    |\n",
      "| time/               |          |\n",
      "|    episodes         | 224      |\n",
      "|    fps              | 53       |\n",
      "|    time_elapsed     | 6826     |\n",
      "|    total_timesteps  | 362031   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 0.00231  |\n",
      "|    n_updates        | 88007    |\n",
      "----------------------------------\n",
      "Total reward for this episode is 2.104999999999998\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 0.5200000000000005\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -0.9149999999999996\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 0.7199999999999993\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57e+03 |\n",
      "|    ep_rew_mean      | 0.562    |\n",
      "|    exploration_rate | 0.222    |\n",
      "| time/               |          |\n",
      "|    episodes         | 228      |\n",
      "|    fps              | 53       |\n",
      "|    time_elapsed     | 6949     |\n",
      "|    total_timesteps  | 368333   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 1.49e-06 |\n",
      "|    n_updates        | 89583    |\n",
      "----------------------------------\n",
      "Total reward for this episode is 4.874999999999998\n",
      "(256, 256, 3)\n",
      "Num timesteps: 370000\n",
      "Best mean reward: 1.34 - Last mean reward per episode: 0.58\n",
      "Total reward for this episode is 0.52\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -0.8149999999999995\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 0.5700000000000005\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57e+03 |\n",
      "|    ep_rew_mean      | 0.532    |\n",
      "|    exploration_rate | 0.209    |\n",
      "| time/               |          |\n",
      "|    episodes         | 232      |\n",
      "|    fps              | 52       |\n",
      "|    time_elapsed     | 7071     |\n",
      "|    total_timesteps  | 374450   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 6.99e-06 |\n",
      "|    n_updates        | 91112    |\n",
      "----------------------------------\n",
      "Total reward for this episode is -5.170000000000001\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -2.1999999999999997\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 0.6699999999999995\n",
      "(256, 256, 3)\n",
      "Num timesteps: 380000\n",
      "Best mean reward: 1.34 - Last mean reward per episode: 0.39\n",
      "Total reward for this episode is -3.9350000000000005\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.57e+03 |\n",
      "|    ep_rew_mean      | 0.36     |\n",
      "|    exploration_rate | 0.197    |\n",
      "| time/               |          |\n",
      "|    episodes         | 236      |\n",
      "|    fps              | 52       |\n",
      "|    time_elapsed     | 7192     |\n",
      "|    total_timesteps  | 380280   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 1.58e-06 |\n",
      "|    n_updates        | 92569    |\n",
      "----------------------------------\n",
      "Total reward for this episode is -3.6350000000000007\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 0.31999999999999984\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -2.1999999999999993\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -0.8650000000000002\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.56e+03 |\n",
      "|    ep_rew_mean      | 0.261    |\n",
      "|    exploration_rate | 0.184    |\n",
      "| time/               |          |\n",
      "|    episodes         | 240      |\n",
      "|    fps              | 52       |\n",
      "|    time_elapsed     | 7315     |\n",
      "|    total_timesteps  | 386469   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 0.0311   |\n",
      "|    n_updates        | 94117    |\n",
      "----------------------------------\n",
      "Total reward for this episode is 2.305\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -0.7650000000000001\n",
      "(256, 256, 3)\n",
      "Num timesteps: 390000\n",
      "Best mean reward: 1.34 - Last mean reward per episode: 0.25\n",
      "Total reward for this episode is -2.3500000000000005\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -2.2\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.56e+03 |\n",
      "|    ep_rew_mean      | 0.196    |\n",
      "|    exploration_rate | 0.171    |\n",
      "| time/               |          |\n",
      "|    episodes         | 244      |\n",
      "|    fps              | 52       |\n",
      "|    time_elapsed     | 7436     |\n",
      "|    total_timesteps  | 392523   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 0.00297  |\n",
      "|    n_updates        | 95630    |\n",
      "----------------------------------\n",
      "Total reward for this episode is -2.15\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -3.635\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -4.819999999999999\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -2.3\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.56e+03 |\n",
      "|    ep_rew_mean      | 0.0307   |\n",
      "|    exploration_rate | 0.158    |\n",
      "| time/               |          |\n",
      "|    episodes         | 248      |\n",
      "|    fps              | 52       |\n",
      "|    time_elapsed     | 7559     |\n",
      "|    total_timesteps  | 398665   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 0.00236  |\n",
      "|    n_updates        | 97166    |\n",
      "----------------------------------\n",
      "Num timesteps: 400000\n",
      "Best mean reward: 1.34 - Last mean reward per episode: 0.03\n",
      "Total reward for this episode is -6.405\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -6.505\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -3.785\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is 0.6200000000000001\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.56e+03 |\n",
      "|    ep_rew_mean      | -0.175   |\n",
      "|    exploration_rate | 0.146    |\n",
      "| time/               |          |\n",
      "|    episodes         | 252      |\n",
      "|    fps              | 52       |\n",
      "|    time_elapsed     | 7681     |\n",
      "|    total_timesteps  | 404751   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 0.00338  |\n",
      "|    n_updates        | 98687    |\n",
      "----------------------------------\n",
      "Total reward for this episode is -0.665\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -1.9999999999999996\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -0.7650000000000003\n",
      "(256, 256, 3)\n",
      "Num timesteps: 410000\n",
      "Best mean reward: 1.34 - Last mean reward per episode: -0.20\n",
      "Total reward for this episode is -0.665\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.55e+03 |\n",
      "|    ep_rew_mean      | -0.208   |\n",
      "|    exploration_rate | 0.133    |\n",
      "| time/               |          |\n",
      "|    episodes         | 256      |\n",
      "|    fps              | 52       |\n",
      "|    time_elapsed     | 7803     |\n",
      "|    total_timesteps  | 410662   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 0.00526  |\n",
      "|    n_updates        | 100165   |\n",
      "----------------------------------\n",
      "Total reward for this episode is -0.9649999999999999\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -0.6649999999999998\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -2.099999999999999\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -0.8149999999999995\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.55e+03 |\n",
      "|    ep_rew_mean      | -0.305   |\n",
      "|    exploration_rate | 0.12     |\n",
      "| time/               |          |\n",
      "|    episodes         | 260      |\n",
      "|    fps              | 52       |\n",
      "|    time_elapsed     | 7925     |\n",
      "|    total_timesteps  | 416652   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 0.0186   |\n",
      "|    n_updates        | 101662   |\n",
      "----------------------------------\n",
      "Total reward for this episode is -0.8650000000000009\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -2.250000000000001\n",
      "(256, 256, 3)\n",
      "Num timesteps: 420000\n",
      "Best mean reward: 1.34 - Last mean reward per episode: -0.36\n",
      "Total reward for this episode is -3.585000000000001\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -0.5649999999999997\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.54e+03 |\n",
      "|    ep_rew_mean      | -0.4     |\n",
      "|    exploration_rate | 0.108    |\n",
      "| time/               |          |\n",
      "|    episodes         | 264      |\n",
      "|    fps              | 52       |\n",
      "|    time_elapsed     | 8048     |\n",
      "|    total_timesteps  | 422656   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 2.36e-06 |\n",
      "|    n_updates        | 103163   |\n",
      "----------------------------------\n",
      "Total reward for this episode is 0.8200000000000005\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -4.92\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -9.274999999999999\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -0.5649999999999995\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.54e+03 |\n",
      "|    ep_rew_mean      | -0.723   |\n",
      "|    exploration_rate | 0.0951   |\n",
      "| time/               |          |\n",
      "|    episodes         | 268      |\n",
      "|    fps              | 52       |\n",
      "|    time_elapsed     | 8170     |\n",
      "|    total_timesteps  | 428616   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 2.98e-06 |\n",
      "|    n_updates        | 104653   |\n",
      "----------------------------------\n",
      "Num timesteps: 430000\n",
      "Best mean reward: 1.34 - Last mean reward per episode: -0.72\n",
      "Total reward for this episode is -4.92\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -9.125000000000002\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -6.355000000000001\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -13.029999999999998\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.53e+03 |\n",
      "|    ep_rew_mean      | -1.14    |\n",
      "|    exploration_rate | 0.0824   |\n",
      "| time/               |          |\n",
      "|    episodes         | 272      |\n",
      "|    fps              | 52       |\n",
      "|    time_elapsed     | 8293     |\n",
      "|    total_timesteps  | 434635   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 1.8e-05  |\n",
      "|    n_updates        | 106158   |\n",
      "----------------------------------\n",
      "Total reward for this episode is -7.990000000000001\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -6.455\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -7.940000000000002\n",
      "(256, 256, 3)\n",
      "Num timesteps: 440000\n",
      "Best mean reward: 1.34 - Last mean reward per episode: -1.38\n",
      "Total reward for this episode is -6.605000000000002\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.53e+03 |\n",
      "|    ep_rew_mean      | -1.45    |\n",
      "|    exploration_rate | 0.0698   |\n",
      "| time/               |          |\n",
      "|    episodes         | 276      |\n",
      "|    fps              | 52       |\n",
      "|    time_elapsed     | 8416     |\n",
      "|    total_timesteps  | 440642   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 0.0311   |\n",
      "|    n_updates        | 107660   |\n",
      "----------------------------------\n",
      "Total reward for this episode is -4.820000000000001\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -7.890000000000002\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -6.355000000000001\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -6.405\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.53e+03 |\n",
      "|    ep_rew_mean      | -1.66    |\n",
      "|    exploration_rate | 0.0573   |\n",
      "| time/               |          |\n",
      "|    episodes         | 280      |\n",
      "|    fps              | 52       |\n",
      "|    time_elapsed     | 8538     |\n",
      "|    total_timesteps  | 446548   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 4.38e-06 |\n",
      "|    n_updates        | 109136   |\n",
      "----------------------------------\n",
      "Total reward for this episode is -4.47\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -10.66\n",
      "(256, 256, 3)\n",
      "Num timesteps: 450000\n",
      "Best mean reward: 1.34 - Last mean reward per episode: -1.84\n",
      "Total reward for this episode is -6.255000000000001\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -13.28\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.53e+03 |\n",
      "|    ep_rew_mean      | -2.03    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 284      |\n",
      "|    fps              | 52       |\n",
      "|    time_elapsed     | 8659     |\n",
      "|    total_timesteps  | 452431   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 0.0179   |\n",
      "|    n_updates        | 110607   |\n",
      "----------------------------------\n",
      "Total reward for this episode is -5.0200000000000005\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -3.3350000000000026\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -3.7350000000000003\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -7.540000000000004\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.52e+03 |\n",
      "|    ep_rew_mean      | -2.17    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 288      |\n",
      "|    fps              | 52       |\n",
      "|    time_elapsed     | 8782     |\n",
      "|    total_timesteps  | 458389   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 0.0155   |\n",
      "|    n_updates        | 112097   |\n",
      "----------------------------------\n",
      "Total reward for this episode is -12.045000000000002\n",
      "(256, 256, 3)\n",
      "Num timesteps: 460000\n",
      "Best mean reward: 1.34 - Last mean reward per episode: -2.34\n",
      "Total reward for this episode is -9.325000000000001\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -4.870000000000001\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -5.120000000000002\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.52e+03 |\n",
      "|    ep_rew_mean      | -2.54    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 292      |\n",
      "|    fps              | 52       |\n",
      "|    time_elapsed     | 8903     |\n",
      "|    total_timesteps  | 464199   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 0.00237  |\n",
      "|    n_updates        | 113549   |\n",
      "----------------------------------\n",
      "Total reward for this episode is -6.455000000000001\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -9.175\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -7.940000000000002\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -10.56\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.51e+03 |\n",
      "|    ep_rew_mean      | -2.83    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 296      |\n",
      "|    fps              | 52       |\n",
      "|    time_elapsed     | 9025     |\n",
      "|    total_timesteps  | 469954   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 0.0018   |\n",
      "|    n_updates        | 114988   |\n",
      "----------------------------------\n",
      "Num timesteps: 470000\n",
      "Best mean reward: 1.34 - Last mean reward per episode: -2.83\n",
      "Total reward for this episode is -9.075000000000001\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -7.590000000000001\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -7.740000000000002\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -7.840000000000001\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.51e+03 |\n",
      "|    ep_rew_mean      | -3.11    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 300      |\n",
      "|    fps              | 52       |\n",
      "|    time_elapsed     | 9147     |\n",
      "|    total_timesteps  | 475791   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 0.0338   |\n",
      "|    n_updates        | 116447   |\n",
      "----------------------------------\n",
      "Total reward for this episode is -6.105000000000002\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -6.5550000000000015\n",
      "(256, 256, 3)\n",
      "Num timesteps: 480000\n",
      "Best mean reward: 1.34 - Last mean reward per episode: -3.22\n",
      "Total reward for this episode is -7.540000000000003\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -4.77\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.51e+03 |\n",
      "|    ep_rew_mean      | -3.35    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 304      |\n",
      "|    fps              | 51       |\n",
      "|    time_elapsed     | 9269     |\n",
      "|    total_timesteps  | 481641   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 0.003    |\n",
      "|    n_updates        | 117910   |\n",
      "----------------------------------\n",
      "Total reward for this episode is -4.770000000000001\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -11.995\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -12.094999999999997\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -6.155000000000001\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.5e+03  |\n",
      "|    ep_rew_mean      | -3.72    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 308      |\n",
      "|    fps              | 51       |\n",
      "|    time_elapsed     | 9393     |\n",
      "|    total_timesteps  | 487637   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 0.0155   |\n",
      "|    n_updates        | 119409   |\n",
      "----------------------------------\n",
      "Total reward for this episode is -7.8900000000000015\n",
      "(256, 256, 3)\n",
      "Num timesteps: 490000\n",
      "Best mean reward: 1.34 - Last mean reward per episode: -3.81\n",
      "Total reward for this episode is -6.205000000000001\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -10.46\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -3.6350000000000002\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.5e+03  |\n",
      "|    ep_rew_mean      | -4.02    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 312      |\n",
      "|    fps              | 51       |\n",
      "|    time_elapsed     | 9515     |\n",
      "|    total_timesteps  | 493509   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 0.00306  |\n",
      "|    n_updates        | 120877   |\n",
      "----------------------------------\n",
      "Total reward for this episode is -3.484999999999999\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -9.275\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -9.124999999999998\n",
      "(256, 256, 3)\n",
      "Total reward for this episode is -3.4849999999999994\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.5e+03  |\n",
      "|    ep_rew_mean      | -4.29    |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 316      |\n",
      "|    fps              | 51       |\n",
      "|    time_elapsed     | 9638     |\n",
      "|    total_timesteps  | 499389   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 2.5e-05  |\n",
      "|    loss             | 0.0188   |\n",
      "|    n_updates        | 122347   |\n",
      "----------------------------------\n",
      "Num timesteps: 500000\n",
      "Best mean reward: 1.34 - Last mean reward per episode: -4.29\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "model.learn(total_timesteps=500000, callback=callback)\n",
    "model.save(\"DQNEveon12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x144 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot rewards\n",
    "plot_results([log_dir], 2000000, results_plotter.X_TIMESTEPS, \"DQN EON\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n",
      "(256, 256, 3)\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "final_model = 'DQNEveon12'\n",
    "best_model = 'tmp/best_model.zip'\n",
    "model = DQN.load(final_model, env=env, device='cpu')\n",
    "\n",
    "obs = env.reset()\n",
    "while True :\n",
    "    # clear_output(wait=True)\n",
    "    action, states_ = model.predict(obs, deterministic=True)\n",
    "    # action = 6\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    # plt.imshow(obs)\n",
    "    # plt.show()\n",
    "    print(action)\n",
    "    if dones == True:\n",
    "        print(env.reward)\n",
    "\n",
    "        # with open('info.json', 'w') as outfile:\n",
    "        #     json.dump(info, outfile)\n",
    "\n",
    "        env.reset()\n",
    "\n",
    "    env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6e1680fa3f035ba33798eeb42b0c1edda6a758b70597993f042faeae31b18ae3"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('4thYearprojectEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
