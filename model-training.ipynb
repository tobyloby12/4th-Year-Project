{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.3 (SDL 2.0.16, Python 3.9.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "from IPython.display import clear_output, display\n",
    "import matplotlib.pyplot as plt\n",
    "import importlib\n",
    "\n",
    "import optical_network_game.game_gym\n",
    "importlib.reload(optical_network_game.game_gym)\n",
    "from optical_network_game.game_gym import *\n",
    "\n",
    "from optical_network_game.requests import *\n",
    "from optical_network_game.topology_generation import *\n",
    "\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
    "from stable_baselines3.common.callbacks import BaseCallback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callback Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq:\n",
    "    :param log_dir: Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: Verbosity level.\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose: int = 1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = None\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "          # Retrieve training reward\n",
    "          x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "          if len(x) > 0:\n",
    "              # Mean training reward over the last 100 episodes\n",
    "              mean_reward = np.mean(y[-100:])\n",
    "              if self.verbose > 0:\n",
    "                print(f\"Num timesteps: {self.num_timesteps}\")\n",
    "                print(f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\")\n",
    "\n",
    "              # # New best model, you could save the agent here\n",
    "              # if mean_reward > self.best_mean_reward:\n",
    "              #     self.best_mean_reward = mean_reward\n",
    "              #     # Example for saving best model\n",
    "              #     if self.verbose > 0:\n",
    "              #       print(f\"Saving new best model to {self.save_path}\")\n",
    "              #     self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorboardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Custom callback for plotting additional values in tensorboard.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, verbose=0):\n",
    "        super(TensorboardCallback, self).__init__(verbose)\n",
    "    \n",
    "    def _on_step(self) -> bool:\n",
    "        block_ep = self.locals[\"infos\"][0].get('bp')\n",
    "        avg_path_len = self.locals[\"infos\"][0].get('avg_length')\n",
    "        blocked_continuous = self.locals[\"infos\"][0].get('blocked_continuous')\n",
    "        blocked_contiguous = self.locals[\"infos\"][0].get('blocked_contiguous')\n",
    "        self.logger.record('blocking_ratio', block_ep)\n",
    "        self.logger.record('average_route_length', avg_path_len)\n",
    "        self.logger.record('blocked_continuous', blocked_continuous)\n",
    "        self.logger.record('blocked_contiguous', blocked_contiguous)\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Create log dir\n",
    "log_dir = os.path.join(os.getcwd(), \"10_Traffic_Load_Model_Log/\")\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters for model training environment\n",
    "Holdtime = 20 #(For a traffic load of 10)\n",
    "#Number of connection requests = 20\n",
    "num_req = 20\n",
    "#request interval = 2 seconds\n",
    "req_int = 2\n",
    "#time limit for each connection request Time_limit = req_interval +1\n",
    "time_limit = req_int + 1\n",
    "#bandwidth per link = 5\n",
    "link_BW = 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traffic Load 10 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VSNL Topology Selected\n",
      "Traffic load is: 10.0\n",
      "Using cuda device\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "# Create the callback: check every 1000 steps\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=200000, log_dir=log_dir)\n",
    "tensor_callback = TensorboardCallback()\n",
    "\n",
    "# create model\n",
    "\n",
    "nodeList, linkList = createPresetTopology(\"VSNL\", num_slots=link_BW)\n",
    "requestList = generateRequests(nodeList, numberOfRequests=num_req, req_interval=req_int, hold_time=Holdtime, time_limit=time_limit)\n",
    "\n",
    "user = User()\n",
    "env = game_gym(nodeList, linkList, requestList, user, dynamic=True)\n",
    "eveon = Monitor(env, log_dir)\n",
    "\n",
    "# check_env(eveon, warn=True)\n",
    "model = DQN('MlpPolicy', eveon, verbose=1, buffer_size=100000, device='cuda', \n",
    "learning_starts=50000, exploration_fraction=0.5, learning_rate=0.0001,\n",
    "gamma=0.8, tensorboard_log='./10_traffic_load_model_log/')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training agent on 30 000 000 timesteps\n",
    "model_name = \"DQN_VSNL_TL_10_ver2\"\n",
    "model.learn(total_timesteps=30000000, callback=[callback, tensor_callback])\n",
    "model.save(model_name)\n",
    "\n",
    "# tensorboard --logdir ./10_traffic_load_model_log/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traffic Load 5 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create log dir\n",
    "log_dir = os.path.join(os.getcwd(), \"5_Traffic_Load_Model_Log/\")\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VSNL Topology Selected\n",
      "Traffic load is: 5.0\n",
      "Using cuda device\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "# Create the callback: check every 1000 steps\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=200000, log_dir=log_dir)\n",
    "tensor_callback = TensorboardCallback()\n",
    "\n",
    "#hold time for traffic load of 5\n",
    "Holdtime_TL_5 = 10\n",
    "\n",
    "# create model\n",
    "\n",
    "nodeList, linkList = createPresetTopology(\"VSNL\", num_slots=link_BW)\n",
    "requestList = generateRequests(nodeList, numberOfRequests=num_req, req_interval=req_int, hold_time=Holdtime_TL_5, time_limit=time_limit)\n",
    "\n",
    "user = User()\n",
    "env = game_gym(nodeList, linkList, requestList, user, dynamic=True)\n",
    "eveon = Monitor(env, log_dir)\n",
    "\n",
    "# check_env(eveon, warn=True)\n",
    "model = DQN('MlpPolicy', eveon, verbose=1, buffer_size=100000, device='cuda', \n",
    "learning_starts=50000, exploration_fraction=0.5, learning_rate=0.0001,\n",
    "gamma=0.8, tensorboard_log='./5_traffic_load_model_log/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./5_traffic_load_model_log/DQN_1\n",
      "No more requests.\n",
      "Total reward for this episode is -25200.0\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -57700.000000000386\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -30859.999999999924\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -8979.999999999967\n",
      "------------------------------------\n",
      "| average_route_length | 5.11      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.55      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.77e+04  |\n",
      "|    ep_rew_mean       | -3.08e+04 |\n",
      "|    exploration_rate  | 0.996     |\n",
      "| time/                |           |\n",
      "|    episodes          | 4         |\n",
      "|    fps               | 897       |\n",
      "|    time_elapsed      | 78        |\n",
      "|    total_timesteps   | 70780     |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 1.04      |\n",
      "|    n_updates         | 5194      |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -10459.999999999976\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -29979.99999999983\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -21999.99999999996\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -17560.00000000001\n",
      "------------------------------------\n",
      "| average_route_length | 5.33      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.55      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.62e+04  |\n",
      "|    ep_rew_mean       | -2.54e+04 |\n",
      "|    exploration_rate  | 0.992     |\n",
      "| time/                |           |\n",
      "|    episodes          | 8         |\n",
      "|    fps               | 772       |\n",
      "|    time_elapsed      | 167       |\n",
      "|    total_timesteps   | 129503    |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00998   |\n",
      "|    n_updates         | 19875     |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 1100.0000000000796\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -11859.99999999997\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -43340.00000000002\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -21379.99999999996\n",
      "------------------------------------\n",
      "| average_route_length | 5.25      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.6       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.53e+04  |\n",
      "|    ep_rew_mean       | -2.32e+04 |\n",
      "|    exploration_rate  | 0.988     |\n",
      "| time/                |           |\n",
      "|    episodes          | 12        |\n",
      "|    fps               | 737       |\n",
      "|    time_elapsed      | 248       |\n",
      "|    total_timesteps   | 183323    |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0033    |\n",
      "|    n_updates         | 33330     |\n",
      "------------------------------------\n",
      "Num timesteps: 200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -23171.67\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -48580.00000000059\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -15859.999999999985\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -10679.999999999998\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -16300.000000000015\n",
      "------------------------------------\n",
      "| average_route_length | 5.71      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.49e+04  |\n",
      "|    ep_rew_mean       | -2.31e+04 |\n",
      "|    exploration_rate  | 0.985     |\n",
      "| time/                |           |\n",
      "|    episodes          | 16        |\n",
      "|    fps               | 729       |\n",
      "|    time_elapsed      | 326       |\n",
      "|    total_timesteps   | 238441    |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00236   |\n",
      "|    n_updates         | 47110     |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -34760.00000000022\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -55320.00000000045\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -3599.999999999952\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -8959.999999999896\n",
      "------------------------------------\n",
      "| average_route_length | 4.46      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.56e+04  |\n",
      "|    ep_rew_mean       | -2.36e+04 |\n",
      "|    exploration_rate  | 0.98      |\n",
      "| time/                |           |\n",
      "|    episodes          | 20        |\n",
      "|    fps               | 728       |\n",
      "|    time_elapsed      | 427       |\n",
      "|    total_timesteps   | 311805    |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00156   |\n",
      "|    n_updates         | 65451     |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -52380.00000000036\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -15839.999999999933\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -6439.999999999976\n",
      "No more requests.\n",
      "Total reward for this episode is -55860.00000000088\n",
      "------------------------------------\n",
      "| average_route_length | 4.78      |\n",
      "| blocked_contiguous   | 0.111     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.1       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.59e+04  |\n",
      "|    ep_rew_mean       | -2.51e+04 |\n",
      "|    exploration_rate  | 0.976     |\n",
      "| time/                |           |\n",
      "|    episodes          | 24        |\n",
      "|    fps               | 728       |\n",
      "|    time_elapsed      | 522       |\n",
      "|    total_timesteps   | 380449    |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00154   |\n",
      "|    n_updates         | 82612     |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -2479.9999999999673\n",
      "Num timesteps: 400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -24196.00\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -6139.9999999999545\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -11159.99999999999\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -18359.999999999894\n",
      "------------------------------------\n",
      "| average_route_length | 4.18      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.45      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.58e+04  |\n",
      "|    ep_rew_mean       | -2.29e+04 |\n",
      "|    exploration_rate  | 0.972     |\n",
      "| time/                |           |\n",
      "|    episodes          | 28        |\n",
      "|    fps               | 727       |\n",
      "|    time_elapsed      | 608       |\n",
      "|    total_timesteps   | 442319    |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.008     |\n",
      "|    n_updates         | 98079     |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -109300.00000000243\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -52520.00000000042\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -61400.000000000604\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -7899.999999999995\n",
      "------------------------------------\n",
      "| average_route_length | 5.71      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.63e+04  |\n",
      "|    ep_rew_mean       | -2.72e+04 |\n",
      "|    exploration_rate  | 0.967     |\n",
      "| time/                |           |\n",
      "|    episodes          | 32        |\n",
      "|    fps               | 726       |\n",
      "|    time_elapsed      | 718       |\n",
      "|    total_timesteps   | 521907    |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00582   |\n",
      "|    n_updates         | 117976    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -17059.999999999985\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -26439.99999999992\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -29559.999999999887\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -14059.999999999956\n",
      "------------------------------------\n",
      "| average_route_length | 4.29      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.58e+04  |\n",
      "|    ep_rew_mean       | -2.66e+04 |\n",
      "|    exploration_rate  | 0.964     |\n",
      "| time/                |           |\n",
      "|    episodes          | 36        |\n",
      "|    fps               | 725       |\n",
      "|    time_elapsed      | 783       |\n",
      "|    total_timesteps   | 568336    |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00303   |\n",
      "|    n_updates         | 129583    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -10319.99999999996\n",
      "Num timesteps: 600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -26188.11\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -104660.00000000196\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -58400.00000000083\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -16260.000000000011\n",
      "------------------------------------\n",
      "| average_route_length | 6.29      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.59e+04  |\n",
      "|    ep_rew_mean       | -2.87e+04 |\n",
      "|    exploration_rate  | 0.96      |\n",
      "| time/                |           |\n",
      "|    episodes          | 40        |\n",
      "|    fps               | 724       |\n",
      "|    time_elapsed      | 878       |\n",
      "|    total_timesteps   | 636480    |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 1.48      |\n",
      "|    n_updates         | 146619    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -25299.999999999927\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -18459.99999999998\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -20200.000000000004\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -74720.00000000106\n",
      "------------------------------------\n",
      "| average_route_length | 6         |\n",
      "| blocked_contiguous   | 0.111     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.4       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.57e+04  |\n",
      "|    ep_rew_mean       | -2.92e+04 |\n",
      "|    exploration_rate  | 0.956     |\n",
      "| time/                |           |\n",
      "|    episodes          | 44        |\n",
      "|    fps               | 722       |\n",
      "|    time_elapsed      | 958       |\n",
      "|    total_timesteps   | 692323    |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0291    |\n",
      "|    n_updates         | 160580    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -82240.0000000016\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -3019.9999999999504\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -41000.00000000038\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -9999.999999999975\n",
      "------------------------------------\n",
      "| average_route_length | 4.86      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.59e+04  |\n",
      "|    ep_rew_mean       | -2.96e+04 |\n",
      "|    exploration_rate  | 0.952     |\n",
      "| time/                |           |\n",
      "|    episodes          | 48        |\n",
      "|    fps               | 721       |\n",
      "|    time_elapsed      | 1055      |\n",
      "|    total_timesteps   | 761592    |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00531   |\n",
      "|    n_updates         | 177897    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 6720.000000000078\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -3839.999999999965\n",
      "Num timesteps: 800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -28391.20\n",
      "No more requests.\n",
      "Total reward for this episode is -50840.00000000039\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -10000.000000000004\n",
      "------------------------------------\n",
      "| average_route_length | 5.71      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.6e+04   |\n",
      "|    ep_rew_mean       | -2.85e+04 |\n",
      "|    exploration_rate  | 0.947     |\n",
      "| time/                |           |\n",
      "|    episodes          | 52        |\n",
      "|    fps               | 720       |\n",
      "|    time_elapsed      | 1156      |\n",
      "|    total_timesteps   | 833097    |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.356     |\n",
      "|    n_updates         | 195774    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -38540.00000000017\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -30119.99999999983\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -14319.999999999998\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -100360.00000000224\n",
      "------------------------------------\n",
      "| average_route_length | 5.23      |\n",
      "| blocked_contiguous   | 0.111     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.63e+04  |\n",
      "|    ep_rew_mean       | -2.97e+04 |\n",
      "|    exploration_rate  | 0.942     |\n",
      "| time/                |           |\n",
      "|    episodes          | 56        |\n",
      "|    fps               | 718       |\n",
      "|    time_elapsed      | 1271      |\n",
      "|    total_timesteps   | 913735    |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 3.24      |\n",
      "|    n_updates         | 215933    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 6120.000000000126\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -17219.999999999858\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -9879.999999999969\n",
      "No more requests.\n",
      "Total reward for this episode is -89540.00000000169\n",
      "------------------------------------\n",
      "| average_route_length | 5.18      |\n",
      "| blocked_contiguous   | 0.167     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.66e+04  |\n",
      "|    ep_rew_mean       | -2.96e+04 |\n",
      "|    exploration_rate  | 0.937     |\n",
      "| time/                |           |\n",
      "|    episodes          | 60        |\n",
      "|    fps               | 717       |\n",
      "|    time_elapsed      | 1388      |\n",
      "|    total_timesteps   | 995999    |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0114    |\n",
      "|    n_updates         | 236499    |\n",
      "------------------------------------\n",
      "Num timesteps: 1000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -29579.67\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -10819.99999999996\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -18919.99999999996\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -14939.999999999967\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -23439.999999999905\n",
      "------------------------------------\n",
      "| average_route_length | 4.25      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.6       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.63e+04  |\n",
      "|    ep_rew_mean       | -2.88e+04 |\n",
      "|    exploration_rate  | 0.934     |\n",
      "| time/                |           |\n",
      "|    episodes          | 64        |\n",
      "|    fps               | 715       |\n",
      "|    time_elapsed      | 1459      |\n",
      "|    total_timesteps   | 1044300   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0139    |\n",
      "|    n_updates         | 248574    |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -62440.00000000083\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -20679.999999999935\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -51700.00000000041\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -74260.00000000102\n",
      "------------------------------------\n",
      "| average_route_length | 5.83      |\n",
      "| blocked_contiguous   | 0.111     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.4       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.65e+04  |\n",
      "|    ep_rew_mean       | -3.02e+04 |\n",
      "|    exploration_rate  | 0.929     |\n",
      "| time/                |           |\n",
      "|    episodes          | 68        |\n",
      "|    fps               | 714       |\n",
      "|    time_elapsed      | 1567      |\n",
      "|    total_timesteps   | 1119408   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.012     |\n",
      "|    n_updates         | 267351    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -28079.999999999818\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -64700.00000000074\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -13679.99999999993\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -5879.999999999948\n",
      "------------------------------------\n",
      "| average_route_length | 4.89      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.55      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.65e+04  |\n",
      "|    ep_rew_mean       | -3.01e+04 |\n",
      "|    exploration_rate  | 0.925     |\n",
      "| time/                |           |\n",
      "|    episodes          | 72        |\n",
      "|    fps               | 713       |\n",
      "|    time_elapsed      | 1667      |\n",
      "|    total_timesteps   | 1189272   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00602   |\n",
      "|    n_updates         | 284817    |\n",
      "------------------------------------\n",
      "Num timesteps: 1200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -30057.78\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 1460.0000000000193\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -28959.99999999978\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -1079.9999999999472\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -76800.00000000125\n",
      "------------------------------------\n",
      "| average_route_length | 5.09      |\n",
      "| blocked_contiguous   | 0.111     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.45      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.66e+04  |\n",
      "|    ep_rew_mean       | -2.99e+04 |\n",
      "|    exploration_rate  | 0.92      |\n",
      "| time/                |           |\n",
      "|    episodes          | 76        |\n",
      "|    fps               | 711       |\n",
      "|    time_elapsed      | 1768      |\n",
      "|    total_timesteps   | 1259242   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00534   |\n",
      "|    n_updates         | 302310    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -25739.999999999913\n",
      "No more requests.\n",
      "Total reward for this episode is 3820.0000000001937\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -43680.000000000175\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -10059.999999999935\n",
      "------------------------------------\n",
      "| average_route_length | 4.44      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.55      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.66e+04  |\n",
      "|    ep_rew_mean       | -2.93e+04 |\n",
      "|    exploration_rate  | 0.916     |\n",
      "| time/                |           |\n",
      "|    episodes          | 80        |\n",
      "|    fps               | 710       |\n",
      "|    time_elapsed      | 1868      |\n",
      "|    total_timesteps   | 1328440   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00751   |\n",
      "|    n_updates         | 319609    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -46780.00000000041\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -49900.00000000034\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -44660.00000000038\n",
      "Num timesteps: 1400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -29957.11\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -68000.00000000086\n",
      "------------------------------------\n",
      "| average_route_length | 5.2       |\n",
      "| blocked_contiguous   | 0.111     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.68e+04  |\n",
      "|    ep_rew_mean       | -3.04e+04 |\n",
      "|    exploration_rate  | 0.911     |\n",
      "| time/                |           |\n",
      "|    episodes          | 84        |\n",
      "|    fps               | 709       |\n",
      "|    time_elapsed      | 1984      |\n",
      "|    total_timesteps   | 1407890   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.003     |\n",
      "|    n_updates         | 339472    |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -88640.00000000132\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 6460.000000000034\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -34259.99999999985\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -27719.9999999999\n",
      "------------------------------------\n",
      "| average_route_length | 5         |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.6       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.69e+04  |\n",
      "|    ep_rew_mean       | -3.07e+04 |\n",
      "|    exploration_rate  | 0.906     |\n",
      "| time/                |           |\n",
      "|    episodes          | 88        |\n",
      "|    fps               | 708       |\n",
      "|    time_elapsed      | 2094      |\n",
      "|    total_timesteps   | 1483385   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.035     |\n",
      "|    n_updates         | 358346    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -103000.0000000021\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -12299.999999999965\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -25019.999999999858\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -94300.00000000195\n",
      "------------------------------------\n",
      "| average_route_length | 5.45      |\n",
      "| blocked_contiguous   | 0.111     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.45      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.69e+04  |\n",
      "|    ep_rew_mean       | -3.19e+04 |\n",
      "|    exploration_rate  | 0.902     |\n",
      "| time/                |           |\n",
      "|    episodes          | 92        |\n",
      "|    fps               | 707       |\n",
      "|    time_elapsed      | 2198      |\n",
      "|    total_timesteps   | 1554365   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00155   |\n",
      "|    n_updates         | 376091    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -8419.999999999978\n",
      "Stop hitting yourself\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -50160.00000000086\n",
      "Num timesteps: 1600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -31826.60\n",
      "No more requests.\n",
      "Total reward for this episode is -54880.00000000054\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -21339.999999999935\n",
      "-----------------------------------\n",
      "| average_route_length | 4        |\n",
      "| blocked_contiguous   | 0        |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.65     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.69e+04 |\n",
      "|    ep_rew_mean       | -3.2e+04 |\n",
      "|    exploration_rate  | 0.897    |\n",
      "| time/                |          |\n",
      "|    episodes          | 96       |\n",
      "|    fps               | 705      |\n",
      "|    time_elapsed      | 2303     |\n",
      "|    total_timesteps   | 1625277  |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 33.8     |\n",
      "|    n_updates         | 393819   |\n",
      "-----------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -122240.0000000003\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -6440.000000000004\n",
      "No more requests.\n",
      "Total reward for this episode is -30239.99999999985\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -18579.999999999967\n",
      "------------------------------------\n",
      "| average_route_length | 5.11      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.55      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.7e+04   |\n",
      "|    ep_rew_mean       | -3.25e+04 |\n",
      "|    exploration_rate  | 0.892     |\n",
      "| time/                |           |\n",
      "|    episodes          | 100       |\n",
      "|    fps               | 704       |\n",
      "|    time_elapsed      | 2411      |\n",
      "|    total_timesteps   | 1698560   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00638   |\n",
      "|    n_updates         | 412139    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -38020.00000000007\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 880.0000000000125\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -21459.99999999982\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -14939.999999999967\n",
      "-----------------------------------\n",
      "| average_route_length | 4.57     |\n",
      "| blocked_contiguous   | 0        |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.65     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.7e+04  |\n",
      "|    ep_rew_mean       | -3.2e+04 |\n",
      "|    exploration_rate  | 0.888    |\n",
      "| time/                |          |\n",
      "|    episodes          | 104      |\n",
      "|    fps               | 703      |\n",
      "|    time_elapsed      | 2512     |\n",
      "|    total_timesteps   | 1766918  |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00377  |\n",
      "|    n_updates         | 429229   |\n",
      "-----------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -11099.999999999995\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -7379.999999999991\n",
      "Num timesteps: 1800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -31744.60\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -24639.999999999836\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -539.9999999999835\n",
      "------------------------------------\n",
      "| average_route_length | 4.89      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.55      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.69e+04  |\n",
      "|    ep_rew_mean       | -3.16e+04 |\n",
      "|    exploration_rate  | 0.885     |\n",
      "| time/                |           |\n",
      "|    episodes          | 108       |\n",
      "|    fps               | 701       |\n",
      "|    time_elapsed      | 2592      |\n",
      "|    total_timesteps   | 1819716   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00135   |\n",
      "|    n_updates         | 442428    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -9279.999999999969\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -22639.999999999964\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 3880.0000000000864\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -7079.999999999981\n",
      "------------------------------------\n",
      "| average_route_length | 5.5       |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.6       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.69e+04  |\n",
      "|    ep_rew_mean       | -3.12e+04 |\n",
      "|    exploration_rate  | 0.882     |\n",
      "| time/                |           |\n",
      "|    episodes          | 112       |\n",
      "|    fps               | 701       |\n",
      "|    time_elapsed      | 2667      |\n",
      "|    total_timesteps   | 1869886   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00168   |\n",
      "|    n_updates         | 454971    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -68620.00000000105\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -19159.999999999884\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -3459.9999999999227\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -15299.999999999938\n",
      "------------------------------------\n",
      "| average_route_length | 4.17      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.4       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.71e+04  |\n",
      "|    ep_rew_mean       | -3.13e+04 |\n",
      "|    exploration_rate  | 0.877     |\n",
      "| time/                |           |\n",
      "|    episodes          | 116       |\n",
      "|    fps               | 699       |\n",
      "|    time_elapsed      | 2779      |\n",
      "|    total_timesteps   | 1944786   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0035    |\n",
      "|    n_updates         | 473696    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -27039.999999999913\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -18720.000000000004\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -26859.999999999935\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -7919.9999999999545\n",
      "------------------------------------\n",
      "| average_route_length | 4.57      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.68e+04  |\n",
      "|    ep_rew_mean       | -3.11e+04 |\n",
      "|    exploration_rate  | 0.874     |\n",
      "| time/                |           |\n",
      "|    episodes          | 120       |\n",
      "|    fps               | 698       |\n",
      "|    time_elapsed      | 2855      |\n",
      "|    total_timesteps   | 1995688   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00256   |\n",
      "|    n_updates         | 486421    |\n",
      "------------------------------------\n",
      "Num timesteps: 2000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -31127.40\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -17399.999999999894\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -54680.00000000027\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -14679.999999999942\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -54440.00000000053\n",
      "------------------------------------\n",
      "| average_route_length | 5.75      |\n",
      "| blocked_contiguous   | 0.0556    |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.6       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.68e+04  |\n",
      "|    ep_rew_mean       | -3.12e+04 |\n",
      "|    exploration_rate  | 0.87      |\n",
      "| time/                |           |\n",
      "|    episodes          | 124       |\n",
      "|    fps               | 697       |\n",
      "|    time_elapsed      | 2952      |\n",
      "|    total_timesteps   | 2059760   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0024    |\n",
      "|    n_updates         | 502439    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -20079.99999999995\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -63440.00000000093\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -16680.0\n",
      "No more requests.\n",
      "Total reward for this episode is -80660.0000000011\n",
      "------------------------------------\n",
      "| average_route_length | 5.18      |\n",
      "| blocked_contiguous   | 0.167     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.69e+04  |\n",
      "|    ep_rew_mean       | -3.27e+04 |\n",
      "|    exploration_rate  | 0.865     |\n",
      "| time/                |           |\n",
      "|    episodes          | 128       |\n",
      "|    fps               | 696       |\n",
      "|    time_elapsed      | 3059      |\n",
      "|    total_timesteps   | 2131667   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00398   |\n",
      "|    n_updates         | 520416    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -2739.999999999941\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -99420.00000000246\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -40119.999999999935\n",
      "Num timesteps: 2200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -31852.00\n",
      "No more requests.\n",
      "Total reward for this episode is -74300.00000000095\n",
      "------------------------------------\n",
      "| average_route_length | 4.47      |\n",
      "| blocked_contiguous   | 0.158     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.7e+04   |\n",
      "|    ep_rew_mean       | -3.25e+04 |\n",
      "|    exploration_rate  | 0.859     |\n",
      "| time/                |           |\n",
      "|    episodes          | 132       |\n",
      "|    fps               | 695       |\n",
      "|    time_elapsed      | 3192      |\n",
      "|    total_timesteps   | 2220084   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00449   |\n",
      "|    n_updates         | 542520    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 5020.000000000005\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -12019.99999999996\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -10659.999999999915\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -67480.0000000012\n",
      "------------------------------------\n",
      "| average_route_length | 5.57      |\n",
      "| blocked_contiguous   | 0.105     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.72e+04  |\n",
      "|    ep_rew_mean       | -3.25e+04 |\n",
      "|    exploration_rate  | 0.855     |\n",
      "| time/                |           |\n",
      "|    episodes          | 136       |\n",
      "|    fps               | 694       |\n",
      "|    time_elapsed      | 3293      |\n",
      "|    total_timesteps   | 2287175   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 14.3      |\n",
      "|    n_updates         | 559293    |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -87580.00000000169\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -40520.00000000011\n",
      "No more requests.\n",
      "Total reward for this episode is -47360.0000000003\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -11739.999999999984\n",
      "------------------------------------\n",
      "| average_route_length | 4.86      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.73e+04  |\n",
      "|    ep_rew_mean       | -3.25e+04 |\n",
      "|    exploration_rate  | 0.85      |\n",
      "| time/                |           |\n",
      "|    episodes          | 140       |\n",
      "|    fps               | 693       |\n",
      "|    time_elapsed      | 3415      |\n",
      "|    total_timesteps   | 2368223   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00336   |\n",
      "|    n_updates         | 579555    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -55820.000000000946\n",
      "Num timesteps: 2400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -32793.80\n",
      "No more requests.\n",
      "Total reward for this episode is -50900.00000000043\n",
      "No more requests.\n",
      "Total reward for this episode is -54620.00000000068\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -3059.999999999988\n",
      "------------------------------------\n",
      "| average_route_length | 4.57      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.76e+04  |\n",
      "|    ep_rew_mean       | -3.28e+04 |\n",
      "|    exploration_rate  | 0.845     |\n",
      "| time/                |           |\n",
      "|    episodes          | 144       |\n",
      "|    fps               | 692       |\n",
      "|    time_elapsed      | 3543      |\n",
      "|    total_timesteps   | 2452789   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00147   |\n",
      "|    n_updates         | 600697    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -34459.99999999993\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -38880.00000000002\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -23739.999999999956\n",
      "No more requests.\n",
      "Total reward for this episode is -62600.00000000119\n",
      "-----------------------------------\n",
      "| average_route_length | 4.89     |\n",
      "| blocked_contiguous   | 0.105    |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.76e+04 |\n",
      "|    ep_rew_mean       | -3.3e+04 |\n",
      "|    exploration_rate  | 0.84     |\n",
      "| time/                |          |\n",
      "|    episodes          | 148      |\n",
      "|    fps               | 691      |\n",
      "|    time_elapsed      | 3648     |\n",
      "|    total_timesteps   | 2521683  |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.928    |\n",
      "|    n_updates         | 617920   |\n",
      "-----------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -114660.0000000014\n",
      "Stop hitting yourself\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -44580.000000000335\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -40680.000000000095\n",
      "Num timesteps: 2600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -34510.80\n",
      "No more requests.\n",
      "Total reward for this episode is -118100.00000000118\n",
      "------------------------------------\n",
      "| average_route_length | 4.94      |\n",
      "| blocked_contiguous   | 0.158     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.78e+04  |\n",
      "|    ep_rew_mean       | -3.56e+04 |\n",
      "|    exploration_rate  | 0.835     |\n",
      "| time/                |           |\n",
      "|    episodes          | 152       |\n",
      "|    fps               | 689       |\n",
      "|    time_elapsed      | 3786      |\n",
      "|    total_timesteps   | 2611401   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00769   |\n",
      "|    n_updates         | 640350    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -25559.999999999945\n",
      "No more requests.\n",
      "Total reward for this episode is 16200.00000000012\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -67560.0000000009\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -15599.99999999993\n",
      "------------------------------------\n",
      "| average_route_length | 4.22      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.55      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.77e+04  |\n",
      "|    ep_rew_mean       | -3.47e+04 |\n",
      "|    exploration_rate  | 0.83      |\n",
      "| time/                |           |\n",
      "|    episodes          | 156       |\n",
      "|    fps               | 688       |\n",
      "|    time_elapsed      | 3896      |\n",
      "|    total_timesteps   | 2683117   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0148    |\n",
      "|    n_updates         | 658279    |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -38580.000000000575\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -6419.999999999982\n",
      "No more requests.\n",
      "Total reward for this episode is -6879.999999999852\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -36020.00000000001\n",
      "------------------------------------\n",
      "| average_route_length | 5.29      |\n",
      "| blocked_contiguous   | 0.0526    |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.77e+04  |\n",
      "|    ep_rew_mean       | -3.45e+04 |\n",
      "|    exploration_rate  | 0.825     |\n",
      "| time/                |           |\n",
      "|    episodes          | 160       |\n",
      "|    fps               | 687       |\n",
      "|    time_elapsed      | 4021      |\n",
      "|    total_timesteps   | 2765705   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00869   |\n",
      "|    n_updates         | 678926    |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -51180.00000000054\n",
      "Num timesteps: 2800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -34883.40\n",
      "No more requests.\n",
      "Total reward for this episode is -84220.00000000147\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -819.9999999999903\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -5419.9999999999345\n",
      "------------------------------------\n",
      "| average_route_length | 4.67      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.55      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.8e+04   |\n",
      "|    ep_rew_mean       | -3.52e+04 |\n",
      "|    exploration_rate  | 0.82      |\n",
      "| time/                |           |\n",
      "|    episodes          | 164       |\n",
      "|    fps               | 686       |\n",
      "|    time_elapsed      | 4138      |\n",
      "|    total_timesteps   | 2841733   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00759   |\n",
      "|    n_updates         | 697933    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -2599.9999999999673\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -45680.00000000046\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -12479.999999999962\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -89620.00000000183\n",
      "------------------------------------\n",
      "| average_route_length | 4.94      |\n",
      "| blocked_contiguous   | 0.105     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.79e+04  |\n",
      "|    ep_rew_mean       | -3.46e+04 |\n",
      "|    exploration_rate  | 0.816     |\n",
      "| time/                |           |\n",
      "|    episodes          | 168       |\n",
      "|    fps               | 685       |\n",
      "|    time_elapsed      | 4245      |\n",
      "|    total_timesteps   | 2910721   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0259    |\n",
      "|    n_updates         | 715180    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -2639.9999999999623\n",
      "No more requests.\n",
      "Total reward for this episode is -51620.000000000655\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -16919.999999999967\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -5399.999999999965\n",
      "------------------------------------\n",
      "| average_route_length | 4.57      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.78e+04  |\n",
      "|    ep_rew_mean       | -3.43e+04 |\n",
      "|    exploration_rate  | 0.812     |\n",
      "| time/                |           |\n",
      "|    episodes          | 172       |\n",
      "|    fps               | 684       |\n",
      "|    time_elapsed      | 4339      |\n",
      "|    total_timesteps   | 2972066   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00708   |\n",
      "|    n_updates         | 730516    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -26199.99999999995\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -18180.0\n",
      "Num timesteps: 3000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -34450.40\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -12439.99999999998\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -21479.999999999927\n",
      "-----------------------------------\n",
      "| average_route_length | 4.25     |\n",
      "| blocked_contiguous   | 0        |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.6      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.76e+04 |\n",
      "|    ep_rew_mean       | -3.4e+04 |\n",
      "|    exploration_rate  | 0.809    |\n",
      "| time/                |          |\n",
      "|    episodes          | 176      |\n",
      "|    fps               | 684      |\n",
      "|    time_elapsed      | 4408     |\n",
      "|    total_timesteps   | 3015670  |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00724  |\n",
      "|    n_updates         | 741417   |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -45140.00000000023\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -34200.0\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -1919.9999999999704\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -40800.0000000004\n",
      "------------------------------------\n",
      "| average_route_length | 4.71      |\n",
      "| blocked_contiguous   | 0.0526    |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.76e+04  |\n",
      "|    ep_rew_mean       | -3.45e+04 |\n",
      "|    exploration_rate  | 0.804     |\n",
      "| time/                |           |\n",
      "|    episodes          | 180       |\n",
      "|    fps               | 683       |\n",
      "|    time_elapsed      | 4524      |\n",
      "|    total_timesteps   | 3090610   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 60.9      |\n",
      "|    n_updates         | 760152    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -63880.00000000068\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -13359.999999999975\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -86860.0000000017\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 1640.0000000000887\n",
      "-----------------------------------\n",
      "| average_route_length | 4.17     |\n",
      "| blocked_contiguous   | 0        |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.4      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.75e+04 |\n",
      "|    ep_rew_mean       | -3.4e+04 |\n",
      "|    exploration_rate  | 0.8      |\n",
      "| time/                |          |\n",
      "|    episodes          | 184      |\n",
      "|    fps               | 681      |\n",
      "|    time_elapsed      | 4633     |\n",
      "|    total_timesteps   | 3159922  |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0286   |\n",
      "|    n_updates         | 777480   |\n",
      "-----------------------------------\n",
      "Stop hitting yourself\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -41360.000000000444\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -6439.999999999968\n",
      "Num timesteps: 3200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -33662.20\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 3300.0000000000114\n",
      "No more requests.\n",
      "Total reward for this episode is -64900.00000000066\n",
      "------------------------------------\n",
      "| average_route_length | 4.71      |\n",
      "| blocked_contiguous   | 0.158     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.75e+04  |\n",
      "|    ep_rew_mean       | -3.37e+04 |\n",
      "|    exploration_rate  | 0.795     |\n",
      "| time/                |           |\n",
      "|    episodes          | 188       |\n",
      "|    fps               | 681       |\n",
      "|    time_elapsed      | 4744      |\n",
      "|    total_timesteps   | 3231056   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 26.9      |\n",
      "|    n_updates         | 795263    |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -2779.9999999998604\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 14400.000000000016\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -26899.999999999716\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -43820.00000000031\n",
      "------------------------------------\n",
      "| average_route_length | 5         |\n",
      "| blocked_contiguous   | 0.0526    |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.77e+04  |\n",
      "|    ep_rew_mean       | -3.19e+04 |\n",
      "|    exploration_rate  | 0.79      |\n",
      "| time/                |           |\n",
      "|    episodes          | 192       |\n",
      "|    fps               | 680       |\n",
      "|    time_elapsed      | 4882      |\n",
      "|    total_timesteps   | 3320565   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0113    |\n",
      "|    n_updates         | 817641    |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -132559.99999999953\n",
      "No more requests.\n",
      "Total reward for this episode is -639.9999999998971\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -13839.999999999969\n",
      "Num timesteps: 3400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -32255.80\n",
      "No more requests.\n",
      "Total reward for this episode is -73460.00000000089\n",
      "------------------------------------\n",
      "| average_route_length | 5.53      |\n",
      "| blocked_contiguous   | 0.158     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.78e+04  |\n",
      "|    ep_rew_mean       | -3.28e+04 |\n",
      "|    exploration_rate  | 0.784     |\n",
      "| time/                |           |\n",
      "|    episodes          | 196       |\n",
      "|    fps               | 679       |\n",
      "|    time_elapsed      | 5019      |\n",
      "|    total_timesteps   | 3409056   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00704   |\n",
      "|    n_updates         | 839763    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -19380.00000000001\n",
      "No more requests.\n",
      "Total reward for this episode is -37099.99999999983\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -65020.000000000764\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -39940.00000000017\n",
      "------------------------------------\n",
      "| average_route_length | 5.57      |\n",
      "| blocked_contiguous   | 0.0526    |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.79e+04  |\n",
      "|    ep_rew_mean       | -3.26e+04 |\n",
      "|    exploration_rate  | 0.779     |\n",
      "| time/                |           |\n",
      "|    episodes          | 200       |\n",
      "|    fps               | 678       |\n",
      "|    time_elapsed      | 5138      |\n",
      "|    total_timesteps   | 3486256   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00927   |\n",
      "|    n_updates         | 859063    |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -46700.000000000255\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -49380.00000000012\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -239.99999999998352\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -60660.0000000008\n",
      "------------------------------------\n",
      "| average_route_length | 5.23      |\n",
      "| blocked_contiguous   | 0.105     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.8e+04   |\n",
      "|    ep_rew_mean       | -3.35e+04 |\n",
      "|    exploration_rate  | 0.774     |\n",
      "| time/                |           |\n",
      "|    episodes          | 204       |\n",
      "|    fps               | 677       |\n",
      "|    time_elapsed      | 5262      |\n",
      "|    total_timesteps   | 3565880   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00864   |\n",
      "|    n_updates         | 878969    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -2119.9999999999063\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -10979.999999999993\n",
      "Num timesteps: 3600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -33408.20\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 2940.0000000001482\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -53760.000000000335\n",
      "------------------------------------\n",
      "| average_route_length | 5.69      |\n",
      "| blocked_contiguous   | 0.105     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.81e+04  |\n",
      "|    ep_rew_mean       | -3.37e+04 |\n",
      "|    exploration_rate  | 0.77      |\n",
      "| time/                |           |\n",
      "|    episodes          | 208       |\n",
      "|    fps               | 676       |\n",
      "|    time_elapsed      | 5369      |\n",
      "|    total_timesteps   | 3633663   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00899   |\n",
      "|    n_updates         | 895915    |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -23239.99999999982\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -6159.999999999873\n",
      "No more requests.\n",
      "Total reward for this episode is -65280.00000000057\n",
      "No more requests.\n",
      "Total reward for this episode is -53440.000000000706\n",
      "------------------------------------\n",
      "| average_route_length | 4.44      |\n",
      "| blocked_contiguous   | 0.105     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.1       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.86e+04  |\n",
      "|    ep_rew_mean       | -3.48e+04 |\n",
      "|    exploration_rate  | 0.764     |\n",
      "| time/                |           |\n",
      "|    episodes          | 212       |\n",
      "|    fps               | 675       |\n",
      "|    time_elapsed      | 5514      |\n",
      "|    total_timesteps   | 3727083   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00344   |\n",
      "|    n_updates         | 919270    |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -52720.000000000655\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -10159.99999999998\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -35119.99999999996\n",
      "Num timesteps: 3800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -34884.60\n",
      "No more requests.\n",
      "Total reward for this episode is -110280.00000000188\n",
      "------------------------------------\n",
      "| average_route_length | 5.62      |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.2       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.87e+04  |\n",
      "|    ep_rew_mean       | -3.58e+04 |\n",
      "|    exploration_rate  | 0.759     |\n",
      "| time/                |           |\n",
      "|    episodes          | 216       |\n",
      "|    fps               | 675       |\n",
      "|    time_elapsed      | 5644      |\n",
      "|    total_timesteps   | 3810654   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00315   |\n",
      "|    n_updates         | 940163    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -49440.00000000029\n",
      "No more requests.\n",
      "Total reward for this episode is -30059.99999999981\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -29579.999999999774\n",
      "No more requests.\n",
      "Total reward for this episode is -9119.999999999894\n",
      "------------------------------------\n",
      "| average_route_length | 3.79      |\n",
      "| blocked_contiguous   | 0.0526    |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.05      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.91e+04  |\n",
      "|    ep_rew_mean       | -3.62e+04 |\n",
      "|    exploration_rate  | 0.753     |\n",
      "| time/                |           |\n",
      "|    episodes          | 220       |\n",
      "|    fps               | 674       |\n",
      "|    time_elapsed      | 5790      |\n",
      "|    total_timesteps   | 3903750   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 13.4      |\n",
      "|    n_updates         | 963437    |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 10500.00000000014\n",
      "No more requests.\n",
      "Total reward for this episode is -60820.000000001004\n",
      "No more requests.\n",
      "Total reward for this episode is -115300.00000000214\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -26099.999999999887\n",
      "------------------------------------\n",
      "| average_route_length | 4.33      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.4       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.94e+04  |\n",
      "|    ep_rew_mean       | -3.67e+04 |\n",
      "|    exploration_rate  | 0.747     |\n",
      "| time/                |           |\n",
      "|    episodes          | 224       |\n",
      "|    fps               | 673       |\n",
      "|    time_elapsed      | 5934      |\n",
      "|    total_timesteps   | 3995385   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0414    |\n",
      "|    n_updates         | 986346    |\n",
      "------------------------------------\n",
      "Num timesteps: 4000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -36749.80\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -90880.00000000121\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -15559.999999999982\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -19899.999999999993\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -53640.00000000053\n",
      "------------------------------------\n",
      "| average_route_length | 6.25      |\n",
      "| blocked_contiguous   | 0.0526    |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.6       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.92e+04  |\n",
      "|    ep_rew_mean       | -3.67e+04 |\n",
      "|    exploration_rate  | 0.743     |\n",
      "| time/                |           |\n",
      "|    episodes          | 228       |\n",
      "|    fps               | 672       |\n",
      "|    time_elapsed      | 6029      |\n",
      "|    total_timesteps   | 4054575   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0169    |\n",
      "|    n_updates         | 1001143   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -95880.00000000173\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -42900.00000000024\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -11999.999999999989\n",
      "No more requests.\n",
      "Total reward for this episode is -66720.0000000007\n",
      "------------------------------------\n",
      "| average_route_length | 5.29      |\n",
      "| blocked_contiguous   | 0.158     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.92e+04  |\n",
      "|    ep_rew_mean       | -3.68e+04 |\n",
      "|    exploration_rate  | 0.738     |\n",
      "| time/                |           |\n",
      "|    episodes          | 232       |\n",
      "|    fps               | 671       |\n",
      "|    time_elapsed      | 6158      |\n",
      "|    total_timesteps   | 4136342   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00811   |\n",
      "|    n_updates         | 1021585   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -11839.999999999944\n",
      "No more requests.\n",
      "Total reward for this episode is -106620.00000000226\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -31339.999999999884\n",
      "Num timesteps: 4200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -38077.20\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -66060.00000000071\n",
      "------------------------------------\n",
      "| average_route_length | 5.17      |\n",
      "| blocked_contiguous   | 0.105     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.4       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.92e+04  |\n",
      "|    ep_rew_mean       | -3.81e+04 |\n",
      "|    exploration_rate  | 0.734     |\n",
      "| time/                |           |\n",
      "|    episodes          | 236       |\n",
      "|    fps               | 670       |\n",
      "|    time_elapsed      | 6267      |\n",
      "|    total_timesteps   | 4205172   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.249     |\n",
      "|    n_updates         | 1038792   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -124200.00000000064\n",
      "No more requests.\n",
      "Total reward for this episode is -63620.00000000066\n",
      "No more requests.\n",
      "Total reward for this episode is -59480.00000000072\n",
      "No more requests.\n",
      "Total reward for this episode is -107440.00000000201\n",
      "------------------------------------\n",
      "| average_route_length | 5.12      |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.2       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.94e+04  |\n",
      "|    ep_rew_mean       | -3.97e+04 |\n",
      "|    exploration_rate  | 0.727     |\n",
      "| time/                |           |\n",
      "|    episodes          | 240       |\n",
      "|    fps               | 670       |\n",
      "|    time_elapsed      | 6423      |\n",
      "|    total_timesteps   | 4304295   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0051    |\n",
      "|    n_updates         | 1063573   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -88640.0000000019\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -19219.999999999993\n",
      "No more requests.\n",
      "Total reward for this episode is -75460.00000000097\n",
      "No more requests.\n",
      "Total reward for this episode is -90240.00000000157\n",
      "------------------------------------\n",
      "| average_route_length | 4.94      |\n",
      "| blocked_contiguous   | 0.158     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.94e+04  |\n",
      "|    ep_rew_mean       | -4.08e+04 |\n",
      "|    exploration_rate  | 0.722     |\n",
      "| time/                |           |\n",
      "|    episodes          | 244       |\n",
      "|    fps               | 669       |\n",
      "|    time_elapsed      | 6558      |\n",
      "|    total_timesteps   | 4389954   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00649   |\n",
      "|    n_updates         | 1084988   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -9440.000000000005\n",
      "Num timesteps: 4400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -40594.60\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -4419.999999999971\n",
      "No more requests.\n",
      "Total reward for this episode is -57900.000000000524\n",
      "No more requests.\n",
      "Total reward for this episode is -99840.0000000019\n",
      "-----------------------------------\n",
      "| average_route_length | 5.25     |\n",
      "| blocked_contiguous   | 0.211    |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.2      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.94e+04 |\n",
      "|    ep_rew_mean       | -4.1e+04 |\n",
      "|    exploration_rate  | 0.718    |\n",
      "| time/                |          |\n",
      "|    episodes          | 248      |\n",
      "|    fps               | 668      |\n",
      "|    time_elapsed      | 6669     |\n",
      "|    total_timesteps   | 4459700  |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00161  |\n",
      "|    n_updates         | 1102424  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -68660.00000000074\n",
      "No more requests.\n",
      "Total reward for this episode is -56280.00000000096\n",
      "No more requests.\n",
      "Total reward for this episode is -70040.000000001\n",
      "No more requests.\n",
      "Total reward for this episode is -57580.000000001055\n",
      "------------------------------------\n",
      "| average_route_length | 5.11      |\n",
      "| blocked_contiguous   | 0.105     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.1       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.95e+04  |\n",
      "|    ep_rew_mean       | -4.03e+04 |\n",
      "|    exploration_rate  | 0.711     |\n",
      "| time/                |           |\n",
      "|    episodes          | 252       |\n",
      "|    fps               | 667       |\n",
      "|    time_elapsed      | 6825      |\n",
      "|    total_timesteps   | 4558643   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00963   |\n",
      "|    n_updates         | 1127160   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -108760.00000000175\n",
      "Num timesteps: 4600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -41169.40\n",
      "No more requests.\n",
      "Total reward for this episode is -103280.00000000166\n",
      "No more requests.\n",
      "Total reward for this episode is -57880.00000000051\n",
      "Stop hitting yourself\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -41920.00000000018\n",
      "------------------------------------\n",
      "| average_route_length | 5.64      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.45      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.97e+04  |\n",
      "|    ep_rew_mean       | -4.25e+04 |\n",
      "|    exploration_rate  | 0.706     |\n",
      "| time/                |           |\n",
      "|    episodes          | 256       |\n",
      "|    fps               | 667       |\n",
      "|    time_elapsed      | 6968      |\n",
      "|    total_timesteps   | 4648836   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00786   |\n",
      "|    n_updates         | 1149708   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -10839.999999999962\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -29139.99999999971\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -44560.00000000032\n",
      "No more requests.\n",
      "Total reward for this episode is -67320.00000000068\n",
      "------------------------------------\n",
      "| average_route_length | 4.71      |\n",
      "| blocked_contiguous   | 0.158     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.96e+04  |\n",
      "|    ep_rew_mean       | -4.32e+04 |\n",
      "|    exploration_rate  | 0.701     |\n",
      "| time/                |           |\n",
      "|    episodes          | 260       |\n",
      "|    fps               | 666       |\n",
      "|    time_elapsed      | 7089      |\n",
      "|    total_timesteps   | 4724413   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00361   |\n",
      "|    n_updates         | 1168603   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -87640.00000000143\n",
      "No more requests.\n",
      "Total reward for this episode is -68940.00000000065\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -17299.999999999927\n",
      "Num timesteps: 4800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -43546.80\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -38680.000000000444\n",
      "------------------------------------\n",
      "| average_route_length | 4.67      |\n",
      "| blocked_contiguous   | 0.0526    |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.1       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.98e+04  |\n",
      "|    ep_rew_mean       | -4.39e+04 |\n",
      "|    exploration_rate  | 0.695     |\n",
      "| time/                |           |\n",
      "|    episodes          | 264       |\n",
      "|    fps               | 665       |\n",
      "|    time_elapsed      | 7240      |\n",
      "|    total_timesteps   | 4818972   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00369   |\n",
      "|    n_updates         | 1192242   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 1580.0000000000427\n",
      "No more requests.\n",
      "Total reward for this episode is -38640.00000000018\n",
      "No more requests.\n",
      "Total reward for this episode is -24899.999999999727\n",
      "No more requests.\n",
      "Total reward for this episode is -80460.00000000124\n",
      "------------------------------------\n",
      "| average_route_length | 4.82      |\n",
      "| blocked_contiguous   | 0.158     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.01e+04  |\n",
      "|    ep_rew_mean       | -4.38e+04 |\n",
      "|    exploration_rate  | 0.689     |\n",
      "| time/                |           |\n",
      "|    episodes          | 268       |\n",
      "|    fps               | 664       |\n",
      "|    time_elapsed      | 7396      |\n",
      "|    total_timesteps   | 4916574   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0039    |\n",
      "|    n_updates         | 1216643   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -84280.00000000118\n",
      "No more requests.\n",
      "Total reward for this episode is -58320.0000000005\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -3239.999999999928\n",
      "Num timesteps: 5000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -44574.40\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -76320.00000000109\n",
      "------------------------------------\n",
      "| average_route_length | 4.82      |\n",
      "| blocked_contiguous   | 0.105     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.04e+04  |\n",
      "|    ep_rew_mean       | -4.53e+04 |\n",
      "|    exploration_rate  | 0.683     |\n",
      "| time/                |           |\n",
      "|    episodes          | 272       |\n",
      "|    fps               | 663       |\n",
      "|    time_elapsed      | 7546      |\n",
      "|    total_timesteps   | 5009829   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00192   |\n",
      "|    n_updates         | 1239957   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -25919.999999999807\n",
      "No more requests.\n",
      "Total reward for this episode is -71900.00000000081\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -104280.00000000188\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -46620.00000000014\n",
      "------------------------------------\n",
      "| average_route_length | 4.67      |\n",
      "| blocked_contiguous   | 0.05      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.1       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.09e+04  |\n",
      "|    ep_rew_mean       | -4.69e+04 |\n",
      "|    exploration_rate  | 0.677     |\n",
      "| time/                |           |\n",
      "|    episodes          | 276       |\n",
      "|    fps               | 662       |\n",
      "|    time_elapsed      | 7704      |\n",
      "|    total_timesteps   | 5107157   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0346    |\n",
      "|    n_updates         | 1264289   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -21099.999999999876\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -59280.00000000054\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -19859.999999999884\n",
      "No more requests.\n",
      "Total reward for this episode is -65820.0000000009\n",
      "------------------------------------\n",
      "| average_route_length | 4.71      |\n",
      "| blocked_contiguous   | 0.15      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.11e+04  |\n",
      "|    ep_rew_mean       | -4.73e+04 |\n",
      "|    exploration_rate  | 0.671     |\n",
      "| time/                |           |\n",
      "|    episodes          | 280       |\n",
      "|    fps               | 662       |\n",
      "|    time_elapsed      | 7851      |\n",
      "|    total_timesteps   | 5198575   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00295   |\n",
      "|    n_updates         | 1287143   |\n",
      "------------------------------------\n",
      "Num timesteps: 5200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -47257.00\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -91580.00000000135\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -29059.99999999973\n",
      "No more requests.\n",
      "Total reward for this episode is -34959.99999999967\n",
      "No more requests.\n",
      "Total reward for this episode is -67900.00000000073\n",
      "------------------------------------\n",
      "| average_route_length | 5.18      |\n",
      "| blocked_contiguous   | 0.15      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.14e+04  |\n",
      "|    ep_rew_mean       | -4.79e+04 |\n",
      "|    exploration_rate  | 0.665     |\n",
      "| time/                |           |\n",
      "|    episodes          | 284       |\n",
      "|    fps               | 661       |\n",
      "|    time_elapsed      | 8007      |\n",
      "|    total_timesteps   | 5295003   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0027    |\n",
      "|    n_updates         | 1311250   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -17359.999999999884\n",
      "No more requests.\n",
      "Total reward for this episode is -74520.00000000102\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -63500.00000000074\n",
      "No more requests.\n",
      "Total reward for this episode is -71700.00000000074\n",
      "------------------------------------\n",
      "| average_route_length | 5.41      |\n",
      "| blocked_contiguous   | 0.15      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.16e+04  |\n",
      "|    ep_rew_mean       | -4.91e+04 |\n",
      "|    exploration_rate  | 0.659     |\n",
      "| time/                |           |\n",
      "|    episodes          | 288       |\n",
      "|    fps               | 660       |\n",
      "|    time_elapsed      | 8157      |\n",
      "|    total_timesteps   | 5388082   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0325    |\n",
      "|    n_updates         | 1334520   |\n",
      "------------------------------------\n",
      "Num timesteps: 5400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -49072.20\n",
      "No more requests.\n",
      "Total reward for this episode is -59480.00000000033\n",
      "No more requests.\n",
      "Total reward for this episode is -6819.999999999817\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -8039.999999999998\n",
      "No more requests.\n",
      "Total reward for this episode is -46020.00000000046\n",
      "------------------------------------\n",
      "| average_route_length | 4.67      |\n",
      "| blocked_contiguous   | 0.1       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.1       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.15e+04  |\n",
      "|    ep_rew_mean       | -4.97e+04 |\n",
      "|    exploration_rate  | 0.654     |\n",
      "| time/                |           |\n",
      "|    episodes          | 292       |\n",
      "|    fps               | 659       |\n",
      "|    time_elapsed      | 8285      |\n",
      "|    total_timesteps   | 5466744   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0136    |\n",
      "|    n_updates         | 1354185   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -15679.999999999885\n",
      "No more requests.\n",
      "Total reward for this episode is -899.9999999999707\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -33699.9999999998\n",
      "No more requests.\n",
      "Total reward for this episode is -73260.00000000116\n",
      "------------------------------------\n",
      "| average_route_length | 4.82      |\n",
      "| blocked_contiguous   | 0.15      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.15e+04  |\n",
      "|    ep_rew_mean       | -4.87e+04 |\n",
      "|    exploration_rate  | 0.648     |\n",
      "| time/                |           |\n",
      "|    episodes          | 296       |\n",
      "|    fps               | 659       |\n",
      "|    time_elapsed      | 8431      |\n",
      "|    total_timesteps   | 5556965   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00919   |\n",
      "|    n_updates         | 1376741   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -50040.000000000546\n",
      "Num timesteps: 5600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -49036.60\n",
      "No more requests.\n",
      "Total reward for this episode is -30639.999999999753\n",
      "No more requests.\n",
      "Total reward for this episode is -30000.000000000102\n",
      "No more requests.\n",
      "Total reward for this episode is -67240.00000000102\n",
      "------------------------------------\n",
      "| average_route_length | 4.24      |\n",
      "| blocked_contiguous   | 0.15      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.17e+04  |\n",
      "|    ep_rew_mean       | -4.89e+04 |\n",
      "|    exploration_rate  | 0.642     |\n",
      "| time/                |           |\n",
      "|    episodes          | 300       |\n",
      "|    fps               | 658       |\n",
      "|    time_elapsed      | 8588      |\n",
      "|    total_timesteps   | 5653892   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00568   |\n",
      "|    n_updates         | 1400972   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -1379.999999999849\n",
      "No more requests.\n",
      "Total reward for this episode is -102360.00000000194\n",
      "No more requests.\n",
      "Total reward for this episode is -78640.0000000013\n",
      "No more requests.\n",
      "Total reward for this episode is -39640.000000000175\n",
      "------------------------------------\n",
      "| average_route_length | 4.67      |\n",
      "| blocked_contiguous   | 0.1       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.1       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.18e+04  |\n",
      "|    ep_rew_mean       | -4.96e+04 |\n",
      "|    exploration_rate  | 0.636     |\n",
      "| time/                |           |\n",
      "|    episodes          | 304       |\n",
      "|    fps               | 657       |\n",
      "|    time_elapsed      | 8735      |\n",
      "|    total_timesteps   | 5744311   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0249    |\n",
      "|    n_updates         | 1423577   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -34080.000000000226\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -93040.00000000148\n",
      "Num timesteps: 5800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -50719.00\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -8979.999999999938\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -15719.999999999967\n",
      "------------------------------------\n",
      "| average_route_length | 4.75      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.6       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.18e+04  |\n",
      "|    ep_rew_mean       | -5.05e+04 |\n",
      "|    exploration_rate  | 0.632     |\n",
      "| time/                |           |\n",
      "|    episodes          | 308       |\n",
      "|    fps               | 656       |\n",
      "|    time_elapsed      | 8848      |\n",
      "|    total_timesteps   | 5813010   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00269   |\n",
      "|    n_updates         | 1440752   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -39140.00000000016\n",
      "No more requests.\n",
      "Total reward for this episode is -90600.00000000121\n",
      "No more requests.\n",
      "Total reward for this episode is -59920.00000000078\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -22519.99999999989\n",
      "------------------------------------\n",
      "| average_route_length | 4.43      |\n",
      "| blocked_contiguous   | 0.05      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.18e+04  |\n",
      "|    ep_rew_mean       | -5.11e+04 |\n",
      "|    exploration_rate  | 0.626     |\n",
      "| time/                |           |\n",
      "|    episodes          | 312       |\n",
      "|    fps               | 656       |\n",
      "|    time_elapsed      | 8998      |\n",
      "|    total_timesteps   | 5904941   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00417   |\n",
      "|    n_updates         | 1463735   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -91380.00000000141\n",
      "No more requests.\n",
      "Total reward for this episode is -1059.999999999933\n",
      "No more requests.\n",
      "Total reward for this episode is -101160.00000000185\n",
      "Num timesteps: 6000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -52065.60\n",
      "No more requests.\n",
      "Total reward for this episode is -88360.0000000014\n",
      "------------------------------------\n",
      "| average_route_length | 6         |\n",
      "| blocked_contiguous   | 0.2       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.2       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.19e+04  |\n",
      "|    ep_rew_mean       | -5.18e+04 |\n",
      "|    exploration_rate  | 0.62      |\n",
      "| time/                |           |\n",
      "|    episodes          | 316       |\n",
      "|    fps               | 655       |\n",
      "|    time_elapsed      | 9154      |\n",
      "|    total_timesteps   | 6000650   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0259    |\n",
      "|    n_updates         | 1487662   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -99380.00000000167\n",
      "No more requests.\n",
      "Total reward for this episode is -68440.0000000011\n",
      "No more requests.\n",
      "Total reward for this episode is -72200.00000000106\n",
      "No more requests.\n",
      "Total reward for this episode is -88100.00000000154\n",
      "-----------------------------------\n",
      "| average_route_length | 4.75     |\n",
      "| blocked_contiguous   | 0.2      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.2      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.19e+04 |\n",
      "|    ep_rew_mean       | -5.4e+04 |\n",
      "|    exploration_rate  | 0.614    |\n",
      "| time/                |          |\n",
      "|    episodes          | 320      |\n",
      "|    fps               | 654      |\n",
      "|    time_elapsed      | 9310     |\n",
      "|    total_timesteps   | 6096471  |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0651   |\n",
      "|    n_updates         | 1511617  |\n",
      "-----------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -29739.99999999993\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -30019.99999999988\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -27039.999999999873\n",
      "No more requests.\n",
      "Total reward for this episode is -85160.00000000131\n",
      "------------------------------------\n",
      "| average_route_length | 5.12      |\n",
      "| blocked_contiguous   | 0.2       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.2       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.17e+04  |\n",
      "|    ep_rew_mean       | -5.37e+04 |\n",
      "|    exploration_rate  | 0.609     |\n",
      "| time/                |           |\n",
      "|    episodes          | 324       |\n",
      "|    fps               | 654       |\n",
      "|    time_elapsed      | 9425      |\n",
      "|    total_timesteps   | 6165851   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0234    |\n",
      "|    n_updates         | 1528962   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -60860.00000000064\n",
      "Num timesteps: 6200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -53447.80\n",
      "No more requests.\n",
      "Total reward for this episode is -80940.00000000097\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -62320.0000000007\n",
      "No more requests.\n",
      "Total reward for this episode is -59480.0000000006\n",
      "------------------------------------\n",
      "| average_route_length | 5.29      |\n",
      "| blocked_contiguous   | 0.15      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.21e+04  |\n",
      "|    ep_rew_mean       | -5.46e+04 |\n",
      "|    exploration_rate  | 0.603     |\n",
      "| time/                |           |\n",
      "|    episodes          | 328       |\n",
      "|    fps               | 653       |\n",
      "|    time_elapsed      | 9581      |\n",
      "|    total_timesteps   | 6260544   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00345   |\n",
      "|    n_updates         | 1552635   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -60220.00000000048\n",
      "No more requests.\n",
      "Total reward for this episode is -55520.000000000866\n",
      "No more requests.\n",
      "Total reward for this episode is -55520.00000000047\n",
      "No more requests.\n",
      "Total reward for this episode is -35060.000000000226\n",
      "------------------------------------\n",
      "| average_route_length | 4.78      |\n",
      "| blocked_contiguous   | 0.1       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.1       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.22e+04  |\n",
      "|    ep_rew_mean       | -5.45e+04 |\n",
      "|    exploration_rate  | 0.597     |\n",
      "| time/                |           |\n",
      "|    episodes          | 332       |\n",
      "|    fps               | 652       |\n",
      "|    time_elapsed      | 9738      |\n",
      "|    total_timesteps   | 6355655   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0058    |\n",
      "|    n_updates         | 1576413   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 3420.0000000001223\n",
      "Num timesteps: 6400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -54353.40\n",
      "No more requests.\n",
      "Total reward for this episode is -89240.00000000121\n",
      "No more requests.\n",
      "Total reward for this episode is -66340.00000000074\n",
      "No more requests.\n",
      "Total reward for this episode is -52820.000000000415\n",
      "------------------------------------\n",
      "| average_route_length | 5.18      |\n",
      "| blocked_contiguous   | 0.15      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.25e+04  |\n",
      "|    ep_rew_mean       | -5.44e+04 |\n",
      "|    exploration_rate  | 0.591     |\n",
      "| time/                |           |\n",
      "|    episodes          | 336       |\n",
      "|    fps               | 651       |\n",
      "|    time_elapsed      | 9894      |\n",
      "|    total_timesteps   | 6450363   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0328    |\n",
      "|    n_updates         | 1600090   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -75800.00000000095\n",
      "No more requests.\n",
      "Total reward for this episode is -49300.00000000031\n",
      "No more requests.\n",
      "Total reward for this episode is -20099.99999999985\n",
      "No more requests.\n",
      "Total reward for this episode is -63940.00000000063\n",
      "-----------------------------------\n",
      "| average_route_length | 4.59     |\n",
      "| blocked_contiguous   | 0.15     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.15     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.24e+04 |\n",
      "|    ep_rew_mean       | -5.3e+04 |\n",
      "|    exploration_rate  | 0.585    |\n",
      "| time/                |          |\n",
      "|    episodes          | 340      |\n",
      "|    fps               | 651      |\n",
      "|    time_elapsed      | 10050    |\n",
      "|    total_timesteps   | 6545316  |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00107  |\n",
      "|    n_updates         | 1623828  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -23799.99999999985\n",
      "No more requests.\n",
      "Total reward for this episode is -29439.999999999924\n",
      "Num timesteps: 6600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -52411.80\n",
      "No more requests.\n",
      "Total reward for this episode is -85540.00000000109\n",
      "No more requests.\n",
      "Total reward for this episode is -37740.00000000013\n",
      "-----------------------------------\n",
      "| average_route_length | 5.22     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.25e+04 |\n",
      "|    ep_rew_mean       | -5.2e+04 |\n",
      "|    exploration_rate  | 0.579    |\n",
      "| time/                |          |\n",
      "|    episodes          | 344      |\n",
      "|    fps               | 650      |\n",
      "|    time_elapsed      | 10207    |\n",
      "|    total_timesteps   | 6640648  |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0329   |\n",
      "|    n_updates         | 1647661  |\n",
      "-----------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -35539.999999999825\n",
      "No more requests.\n",
      "Total reward for this episode is -11859.999999999814\n",
      "No more requests.\n",
      "Total reward for this episode is -34499.99999999966\n",
      "No more requests.\n",
      "Total reward for this episode is -47420.00000000032\n",
      "------------------------------------\n",
      "| average_route_length | 4.82      |\n",
      "| blocked_contiguous   | 0.15      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.28e+04  |\n",
      "|    ep_rew_mean       | -5.15e+04 |\n",
      "|    exploration_rate  | 0.573     |\n",
      "| time/                |           |\n",
      "|    episodes          | 348       |\n",
      "|    fps               | 649       |\n",
      "|    time_elapsed      | 10365     |\n",
      "|    total_timesteps   | 6736311   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00345   |\n",
      "|    n_updates         | 1671577   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -46420.00000000027\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -25319.999999999854\n",
      "Num timesteps: 6800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -50933.40\n",
      "No more requests.\n",
      "Total reward for this episode is -2679.9999999999263\n",
      "No more requests.\n",
      "Total reward for this episode is -55100.000000000386\n",
      "------------------------------------\n",
      "| average_route_length | 5.53      |\n",
      "| blocked_contiguous   | 0.15      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.27e+04  |\n",
      "|    ep_rew_mean       | -5.02e+04 |\n",
      "|    exploration_rate  | 0.568     |\n",
      "| time/                |           |\n",
      "|    episodes          | 352       |\n",
      "|    fps               | 649       |\n",
      "|    time_elapsed      | 10511     |\n",
      "|    total_timesteps   | 6824834   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0294    |\n",
      "|    n_updates         | 1693708   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -59420.00000000064\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -61920.00000000046\n",
      "No more requests.\n",
      "Total reward for this episode is -88600.00000000143\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 10120.000000000095\n",
      "------------------------------------\n",
      "| average_route_length | 4.47      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.26e+04  |\n",
      "|    ep_rew_mean       | -4.91e+04 |\n",
      "|    exploration_rate  | 0.562     |\n",
      "| time/                |           |\n",
      "|    episodes          | 356       |\n",
      "|    fps               | 648       |\n",
      "|    time_elapsed      | 10659     |\n",
      "|    total_timesteps   | 6913140   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00691   |\n",
      "|    n_updates         | 1715784   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -6119.999999999892\n",
      "No more requests.\n",
      "Total reward for this episode is -78460.00000000102\n",
      "No more requests.\n",
      "Total reward for this episode is -82820.00000000108\n",
      "Num timesteps: 7000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -49949.00\n",
      "No more requests.\n",
      "Total reward for this episode is -37519.99999999981\n",
      "------------------------------------\n",
      "| average_route_length | 4         |\n",
      "| blocked_contiguous   | 0.1       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.1       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.28e+04  |\n",
      "|    ep_rew_mean       | -4.97e+04 |\n",
      "|    exploration_rate  | 0.557     |\n",
      "| time/                |           |\n",
      "|    episodes          | 360       |\n",
      "|    fps               | 647       |\n",
      "|    time_elapsed      | 10805     |\n",
      "|    total_timesteps   | 7000937   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.027     |\n",
      "|    n_updates         | 1737734   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -58460.0000000006\n",
      "No more requests.\n",
      "Total reward for this episode is 13840.000000000064\n",
      "No more requests.\n",
      "Total reward for this episode is 2740.0000000000446\n",
      "No more requests.\n",
      "Total reward for this episode is -50580.00000000041\n",
      "------------------------------------\n",
      "| average_route_length | 5.29      |\n",
      "| blocked_contiguous   | 0.15      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.28e+04  |\n",
      "|    ep_rew_mean       | -4.85e+04 |\n",
      "|    exploration_rate  | 0.551     |\n",
      "| time/                |           |\n",
      "|    episodes          | 364       |\n",
      "|    fps               | 647       |\n",
      "|    time_elapsed      | 10961     |\n",
      "|    total_timesteps   | 7094004   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00383   |\n",
      "|    n_updates         | 1761000   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -24739.999999999844\n",
      "No more requests.\n",
      "Total reward for this episode is 15300.000000000031\n",
      "No more requests.\n",
      "Total reward for this episode is -50920.00000000054\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -40320.000000000255\n",
      "-----------------------------------\n",
      "| average_route_length | 4.93     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.25     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.27e+04 |\n",
      "|    ep_rew_mean       | -4.8e+04 |\n",
      "|    exploration_rate  | 0.545    |\n",
      "| time/                |          |\n",
      "|    episodes          | 368      |\n",
      "|    fps               | 646      |\n",
      "|    time_elapsed      | 11115    |\n",
      "|    total_timesteps   | 7185812  |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0117   |\n",
      "|    n_updates         | 1783952  |\n",
      "-----------------------------------\n",
      "Num timesteps: 7200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -48032.60\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -31319.999999999858\n",
      "No more requests.\n",
      "Total reward for this episode is -24399.999999999865\n",
      "No more requests.\n",
      "Total reward for this episode is -60980.0000000007\n",
      "No more requests.\n",
      "Total reward for this episode is -56320.00000000052\n",
      "------------------------------------\n",
      "| average_route_length | 4.59      |\n",
      "| blocked_contiguous   | 0.15      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.26e+04  |\n",
      "|    ep_rew_mean       | -4.75e+04 |\n",
      "|    exploration_rate  | 0.539     |\n",
      "| time/                |           |\n",
      "|    episodes          | 372       |\n",
      "|    fps               | 645       |\n",
      "|    time_elapsed      | 11263     |\n",
      "|    total_timesteps   | 7272930   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 4.23      |\n",
      "|    n_updates         | 1805732   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -35119.99999999991\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -30759.999999999898\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -72580.00000000105\n",
      "No more requests.\n",
      "Total reward for this episode is -58680.000000000575\n",
      "------------------------------------\n",
      "| average_route_length | 4.82      |\n",
      "| blocked_contiguous   | 0.15      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.24e+04  |\n",
      "|    ep_rew_mean       | -4.71e+04 |\n",
      "|    exploration_rate  | 0.535     |\n",
      "| time/                |           |\n",
      "|    episodes          | 376       |\n",
      "|    fps               | 645       |\n",
      "|    time_elapsed      | 11389     |\n",
      "|    total_timesteps   | 7347389   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 20.1      |\n",
      "|    n_updates         | 1824347   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -55160.000000000604\n",
      "No more requests.\n",
      "Total reward for this episode is -49280.000000000575\n",
      "Num timesteps: 7400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -47464.80\n",
      "No more requests.\n",
      "Total reward for this episode is -21619.99999999982\n",
      "No more requests.\n",
      "Total reward for this episode is -24119.999999999825\n",
      "------------------------------------\n",
      "| average_route_length | 4.78      |\n",
      "| blocked_contiguous   | 0.1       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.1       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.24e+04  |\n",
      "|    ep_rew_mean       | -4.71e+04 |\n",
      "|    exploration_rate  | 0.529     |\n",
      "| time/                |           |\n",
      "|    episodes          | 380       |\n",
      "|    fps               | 644       |\n",
      "|    time_elapsed      | 11545     |\n",
      "|    total_timesteps   | 7440138   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00572   |\n",
      "|    n_updates         | 1847534   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -42760.000000000175\n",
      "No more requests.\n",
      "Total reward for this episode is -6279.999999999866\n",
      "No more requests.\n",
      "Total reward for this episode is -27499.99999999984\n",
      "No more requests.\n",
      "Total reward for this episode is -64820.000000000655\n",
      "------------------------------------\n",
      "| average_route_length | 5.76      |\n",
      "| blocked_contiguous   | 0.15      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.24e+04  |\n",
      "|    ep_rew_mean       | -4.62e+04 |\n",
      "|    exploration_rate  | 0.523     |\n",
      "| time/                |           |\n",
      "|    episodes          | 384       |\n",
      "|    fps               | 643       |\n",
      "|    time_elapsed      | 11701     |\n",
      "|    total_timesteps   | 7532022   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0795    |\n",
      "|    n_updates         | 1870505   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -32059.999999999833\n",
      "No more requests.\n",
      "Total reward for this episode is -19119.99999999993\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -14420.000000000002\n",
      "Num timesteps: 7600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -45351.80\n",
      "No more requests.\n",
      "Total reward for this episode is -49660.000000000306\n",
      "------------------------------------\n",
      "| average_route_length | 4.82      |\n",
      "| blocked_contiguous   | 0.15      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.22e+04  |\n",
      "|    ep_rew_mean       | -4.51e+04 |\n",
      "|    exploration_rate  | 0.518     |\n",
      "| time/                |           |\n",
      "|    episodes          | 388       |\n",
      "|    fps               | 643       |\n",
      "|    time_elapsed      | 11823     |\n",
      "|    total_timesteps   | 7603985   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0505    |\n",
      "|    n_updates         | 1888496   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -37479.99999999992\n",
      "No more requests.\n",
      "Total reward for this episode is -46600.00000000035\n",
      "No more requests.\n",
      "Total reward for this episode is -38720.000000000095\n",
      "No more requests.\n",
      "Total reward for this episode is -44480.000000000146\n",
      "------------------------------------\n",
      "| average_route_length | 4.82      |\n",
      "| blocked_contiguous   | 0.15      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.23e+04  |\n",
      "|    ep_rew_mean       | -4.56e+04 |\n",
      "|    exploration_rate  | 0.513     |\n",
      "| time/                |           |\n",
      "|    episodes          | 392       |\n",
      "|    fps               | 642       |\n",
      "|    time_elapsed      | 11980     |\n",
      "|    total_timesteps   | 7697124   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 11.2      |\n",
      "|    n_updates         | 1911780   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -94960.00000000147\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -92780.00000000122\n",
      "No more requests.\n",
      "Total reward for this episode is -45940.00000000011\n",
      "No more requests.\n",
      "Total reward for this episode is -62720.000000000524\n",
      "------------------------------------\n",
      "| average_route_length | 5.18      |\n",
      "| blocked_contiguous   | 0.15      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.23e+04  |\n",
      "|    ep_rew_mean       | -4.73e+04 |\n",
      "|    exploration_rate  | 0.507     |\n",
      "| time/                |           |\n",
      "|    episodes          | 396       |\n",
      "|    fps               | 641       |\n",
      "|    time_elapsed      | 12136     |\n",
      "|    total_timesteps   | 7789500   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00194   |\n",
      "|    n_updates         | 1934874   |\n",
      "------------------------------------\n",
      "Num timesteps: 7800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -47338.40\n",
      "No more requests.\n",
      "Total reward for this episode is -70040.00000000079\n",
      "No more requests.\n",
      "Total reward for this episode is 9300.000000000025\n",
      "No more requests.\n",
      "Total reward for this episode is -26779.999999999825\n",
      "No more requests.\n",
      "Total reward for this episode is 9059.999999999985\n",
      "------------------------------------\n",
      "| average_route_length | 4         |\n",
      "| blocked_contiguous   | 0.05      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.05      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.23e+04  |\n",
      "|    ep_rew_mean       | -4.63e+04 |\n",
      "|    exploration_rate  | 0.501     |\n",
      "| time/                |           |\n",
      "|    episodes          | 400       |\n",
      "|    fps               | 641       |\n",
      "|    time_elapsed      | 12292     |\n",
      "|    total_timesteps   | 7881747   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00288   |\n",
      "|    n_updates         | 1957936   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -19179.999999999916\n",
      "No more requests.\n",
      "Total reward for this episode is -75160.00000000097\n",
      "No more requests.\n",
      "Total reward for this episode is -33739.99999999974\n",
      "No more requests.\n",
      "Total reward for this episode is -49340.00000000027\n",
      "------------------------------------\n",
      "| average_route_length | 4.94      |\n",
      "| blocked_contiguous   | 0.15      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.23e+04  |\n",
      "|    ep_rew_mean       | -4.59e+04 |\n",
      "|    exploration_rate  | 0.495     |\n",
      "| time/                |           |\n",
      "|    episodes          | 404       |\n",
      "|    fps               | 640       |\n",
      "|    time_elapsed      | 12446     |\n",
      "|    total_timesteps   | 7972368   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00609   |\n",
      "|    n_updates         | 1980591   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -43680.000000000204\n",
      "Num timesteps: 8000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -45995.80\n",
      "No more requests.\n",
      "Total reward for this episode is -16479.999999999913\n",
      "No more requests.\n",
      "Total reward for this episode is -31900.0\n",
      "No more requests.\n",
      "Total reward for this episode is -18479.999999999858\n",
      "------------------------------------\n",
      "| average_route_length | 4.44      |\n",
      "| blocked_contiguous   | 0.1       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.1       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.25e+04  |\n",
      "|    ep_rew_mean       | -4.55e+04 |\n",
      "|    exploration_rate  | 0.489     |\n",
      "| time/                |           |\n",
      "|    episodes          | 408       |\n",
      "|    fps               | 639       |\n",
      "|    time_elapsed      | 12602     |\n",
      "|    total_timesteps   | 8064168   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00104   |\n",
      "|    n_updates         | 2003541   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -9359.99999999994\n",
      "No more requests.\n",
      "Total reward for this episode is -40400.00000000013\n",
      "No more requests.\n",
      "Total reward for this episode is -40619.99999999998\n",
      "No more requests.\n",
      "Total reward for this episode is -50880.000000000255\n",
      "------------------------------------\n",
      "| average_route_length | 5.06      |\n",
      "| blocked_contiguous   | 0.15      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.25e+04  |\n",
      "|    ep_rew_mean       | -4.48e+04 |\n",
      "|    exploration_rate  | 0.484     |\n",
      "| time/                |           |\n",
      "|    episodes          | 412       |\n",
      "|    fps               | 639       |\n",
      "|    time_elapsed      | 12751     |\n",
      "|    total_timesteps   | 8151191   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0125    |\n",
      "|    n_updates         | 2025297   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -27599.99999999983\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -42039.9999999999\n",
      "Num timesteps: 8200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -44555.40\n",
      "No more requests.\n",
      "Total reward for this episode is -42280.000000000204\n",
      "No more requests.\n",
      "Total reward for this episode is -30639.99999999978\n",
      "------------------------------------\n",
      "| average_route_length | 5         |\n",
      "| blocked_contiguous   | 0.1       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.1       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.24e+04  |\n",
      "|    ep_rew_mean       | -4.34e+04 |\n",
      "|    exploration_rate  | 0.478     |\n",
      "| time/                |           |\n",
      "|    episodes          | 416       |\n",
      "|    fps               | 638       |\n",
      "|    time_elapsed      | 12904     |\n",
      "|    total_timesteps   | 8240901   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.005     |\n",
      "|    n_updates         | 2047725   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -71700.00000000076\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -11040.0\n",
      "No more requests.\n",
      "Total reward for this episode is -24199.9999999999\n",
      "No more requests.\n",
      "Total reward for this episode is -35579.99999999999\n",
      "------------------------------------\n",
      "| average_route_length | 4.24      |\n",
      "| blocked_contiguous   | 0.15      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.22e+04  |\n",
      "|    ep_rew_mean       | -4.15e+04 |\n",
      "|    exploration_rate  | 0.474     |\n",
      "| time/                |           |\n",
      "|    episodes          | 420       |\n",
      "|    fps               | 638       |\n",
      "|    time_elapsed      | 13028     |\n",
      "|    total_timesteps   | 8313071   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.789     |\n",
      "|    n_updates         | 2065767   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -16860.000000000004\n",
      "No more requests.\n",
      "Total reward for this episode is -101320.00000000135\n",
      "No more requests.\n",
      "Total reward for this episode is 1960.0000000000173\n",
      "No more requests.\n",
      "Total reward for this episode is 4920.000000000152\n",
      "------------------------------------\n",
      "| average_route_length | 3.79      |\n",
      "| blocked_contiguous   | 0.05      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.05      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.22e+04  |\n",
      "|    ep_rew_mean       | -4.09e+04 |\n",
      "|    exploration_rate  | 0.469     |\n",
      "| time/                |           |\n",
      "|    episodes          | 424       |\n",
      "|    fps               | 637       |\n",
      "|    time_elapsed      | 13148     |\n",
      "|    total_timesteps   | 8383187   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 11.1      |\n",
      "|    n_updates         | 2083296   |\n",
      "------------------------------------\n",
      "Num timesteps: 8400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -40932.40\n",
      "No more requests.\n",
      "Total reward for this episode is 10399.999999999996\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -28019.999999999727\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -75080.00000000092\n",
      "No more requests.\n",
      "Total reward for this episode is -12579.999999999978\n",
      "------------------------------------\n",
      "| average_route_length | 4.95      |\n",
      "| blocked_contiguous   | 0.05      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.05      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.21e+04  |\n",
      "|    ep_rew_mean       | -3.93e+04 |\n",
      "|    exploration_rate  | 0.463     |\n",
      "| time/                |           |\n",
      "|    episodes          | 428       |\n",
      "|    fps               | 636       |\n",
      "|    time_elapsed      | 13304     |\n",
      "|    total_timesteps   | 8473895   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00886   |\n",
      "|    n_updates         | 2105973   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -14139.999999999884\n",
      "No more requests.\n",
      "Total reward for this episode is -34700.00000000029\n",
      "No more requests.\n",
      "Total reward for this episode is -37720.00000000016\n",
      "No more requests.\n",
      "Total reward for this episode is -46420.00000000018\n",
      "------------------------------------\n",
      "| average_route_length | 5.18      |\n",
      "| blocked_contiguous   | 0.15      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.21e+04  |\n",
      "|    ep_rew_mean       | -3.86e+04 |\n",
      "|    exploration_rate  | 0.458     |\n",
      "| time/                |           |\n",
      "|    episodes          | 432       |\n",
      "|    fps               | 636       |\n",
      "|    time_elapsed      | 13460     |\n",
      "|    total_timesteps   | 8564794   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0122    |\n",
      "|    n_updates         | 2128698   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -38879.99999999991\n",
      "Num timesteps: 8600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -39038.80\n",
      "No more requests.\n",
      "Total reward for this episode is -27119.999999999858\n",
      "No more requests.\n",
      "Total reward for this episode is 11000.000000000096\n",
      "No more requests.\n",
      "Total reward for this episode is -18699.99999999986\n",
      "------------------------------------\n",
      "| average_route_length | 4.56      |\n",
      "| blocked_contiguous   | 0.1       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.1       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.21e+04  |\n",
      "|    ep_rew_mean       | -3.73e+04 |\n",
      "|    exploration_rate  | 0.452     |\n",
      "| time/                |           |\n",
      "|    episodes          | 436       |\n",
      "|    fps               | 635       |\n",
      "|    time_elapsed      | 13616     |\n",
      "|    total_timesteps   | 8655546   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00215   |\n",
      "|    n_updates         | 2151386   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -12819.999999999789\n",
      "No more requests.\n",
      "Total reward for this episode is -21539.99999999989\n",
      "No more requests.\n",
      "Total reward for this episode is -62960.00000000062\n",
      "No more requests.\n",
      "Total reward for this episode is -21959.99999999986\n",
      "------------------------------------\n",
      "| average_route_length | 4.33      |\n",
      "| blocked_contiguous   | 0.1       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.1       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.2e+04   |\n",
      "|    ep_rew_mean       | -3.64e+04 |\n",
      "|    exploration_rate  | 0.446     |\n",
      "| time/                |           |\n",
      "|    episodes          | 440       |\n",
      "|    fps               | 635       |\n",
      "|    time_elapsed      | 13773     |\n",
      "|    total_timesteps   | 8746652   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00207   |\n",
      "|    n_updates         | 2174162   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -37719.99999999993\n",
      "No more requests.\n",
      "Total reward for this episode is -47060.0000000003\n",
      "Num timesteps: 8800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -36719.80\n",
      "No more requests.\n",
      "Total reward for this episode is -37540.0\n",
      "No more requests.\n",
      "Total reward for this episode is -8759.999999999818\n",
      "-----------------------------------\n",
      "| average_route_length | 4.32     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.2e+04  |\n",
      "|    ep_rew_mean       | -3.6e+04 |\n",
      "|    exploration_rate  | 0.44     |\n",
      "| time/                |          |\n",
      "|    episodes          | 444      |\n",
      "|    fps               | 634      |\n",
      "|    time_elapsed      | 13929    |\n",
      "|    total_timesteps   | 8837657  |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 1.16     |\n",
      "|    n_updates         | 2196914  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -19239.99999999992\n",
      "No more requests.\n",
      "Total reward for this episode is 10480.000000000024\n",
      "No more requests.\n",
      "Total reward for this episode is -35919.99999999997\n",
      "No more requests.\n",
      "Total reward for this episode is -21279.999999999873\n",
      "------------------------------------\n",
      "| average_route_length | 4.67      |\n",
      "| blocked_contiguous   | 0.1       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.1       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.19e+04  |\n",
      "|    ep_rew_mean       | -3.54e+04 |\n",
      "|    exploration_rate  | 0.435     |\n",
      "| time/                |           |\n",
      "|    episodes          | 448       |\n",
      "|    fps               | 633       |\n",
      "|    time_elapsed      | 14086     |\n",
      "|    total_timesteps   | 8928291   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00179   |\n",
      "|    n_updates         | 2219572   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -17520.000000000015\n",
      "No more requests.\n",
      "Total reward for this episode is -45380.00000000035\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -4419.999999999996\n",
      "Num timesteps: 9000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -35350.40\n",
      "No more requests.\n",
      "Total reward for this episode is -27899.999999999796\n",
      "------------------------------------\n",
      "| average_route_length | 3.88      |\n",
      "| blocked_contiguous   | 0.15      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.18e+04  |\n",
      "|    ep_rew_mean       | -3.51e+04 |\n",
      "|    exploration_rate  | 0.43      |\n",
      "| time/                |           |\n",
      "|    episodes          | 452       |\n",
      "|    fps               | 633       |\n",
      "|    time_elapsed      | 14216     |\n",
      "|    total_timesteps   | 9002845   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0198    |\n",
      "|    n_updates         | 2238211   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -38360.0\n",
      "No more requests.\n",
      "Total reward for this episode is 5440.000000000077\n",
      "No more requests.\n",
      "Total reward for this episode is 7680.000000000112\n",
      "No more requests.\n",
      "Total reward for this episode is -67300.00000000064\n",
      "-----------------------------------\n",
      "| average_route_length | 6        |\n",
      "| blocked_contiguous   | 0.2      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.2      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.18e+04 |\n",
      "|    ep_rew_mean       | -3.4e+04 |\n",
      "|    exploration_rate  | 0.424    |\n",
      "| time/                |          |\n",
      "|    episodes          | 456      |\n",
      "|    fps               | 632      |\n",
      "|    time_elapsed      | 14372    |\n",
      "|    total_timesteps   | 9093038  |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0387   |\n",
      "|    n_updates         | 2260759  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -86260.00000000122\n",
      "No more requests.\n",
      "Total reward for this episode is -80900.00000000089\n",
      "No more requests.\n",
      "Total reward for this episode is -21599.9999999999\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -48880.000000000306\n",
      "------------------------------------\n",
      "| average_route_length | 4.94      |\n",
      "| blocked_contiguous   | 0.1       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.18e+04  |\n",
      "|    ep_rew_mean       | -3.43e+04 |\n",
      "|    exploration_rate  | 0.418     |\n",
      "| time/                |           |\n",
      "|    episodes          | 460       |\n",
      "|    fps               | 632       |\n",
      "|    time_elapsed      | 14528     |\n",
      "|    total_timesteps   | 9182596   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00469   |\n",
      "|    n_updates         | 2283148   |\n",
      "------------------------------------\n",
      "Num timesteps: 9200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -34349.60\n",
      "No more requests.\n",
      "Total reward for this episode is -31219.999999999847\n",
      "No more requests.\n",
      "Total reward for this episode is -47320.00000000029\n",
      "No more requests.\n",
      "Total reward for this episode is -33639.99999999984\n",
      "No more requests.\n",
      "Total reward for this episode is -41220.00000000011\n",
      "-----------------------------------\n",
      "| average_route_length | 5.65     |\n",
      "| blocked_contiguous   | 0.15     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.15     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.18e+04 |\n",
      "|    ep_rew_mean       | -3.5e+04 |\n",
      "|    exploration_rate  | 0.413    |\n",
      "| time/                |          |\n",
      "|    episodes          | 464      |\n",
      "|    fps               | 631      |\n",
      "|    time_elapsed      | 14684    |\n",
      "|    total_timesteps   | 9271770  |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0125   |\n",
      "|    n_updates         | 2305442  |\n",
      "-----------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -4659.999999999994\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -37259.99999999988\n",
      "No more requests.\n",
      "Total reward for this episode is -27019.999999999913\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -25119.999999999887\n",
      "------------------------------------\n",
      "| average_route_length | 4.78      |\n",
      "| blocked_contiguous   | 0.05      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.1       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.15e+04  |\n",
      "|    ep_rew_mean       | -3.49e+04 |\n",
      "|    exploration_rate  | 0.409     |\n",
      "| time/                |           |\n",
      "|    episodes          | 468       |\n",
      "|    fps               | 630       |\n",
      "|    time_elapsed      | 14797     |\n",
      "|    total_timesteps   | 9335761   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0107    |\n",
      "|    n_updates         | 2321440   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 10160.0\n",
      "No more requests.\n",
      "Total reward for this episode is -24399.99999999976\n",
      "Num timesteps: 9400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -34476.00\n",
      "No more requests.\n",
      "Total reward for this episode is -2099.9999999999163\n",
      "No more requests.\n",
      "Total reward for this episode is -3299.9999999998495\n",
      "------------------------------------\n",
      "| average_route_length | 4.53      |\n",
      "| blocked_contiguous   | 0.05      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.05      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.15e+04  |\n",
      "|    ep_rew_mean       | -3.34e+04 |\n",
      "|    exploration_rate  | 0.403     |\n",
      "| time/                |           |\n",
      "|    episodes          | 472       |\n",
      "|    fps               | 630       |\n",
      "|    time_elapsed      | 14953     |\n",
      "|    total_timesteps   | 9424587   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.014     |\n",
      "|    n_updates         | 2343646   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -43180.00000000003\n",
      "No more requests.\n",
      "Total reward for this episode is -59960.000000000386\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -34639.99999999978\n",
      "No more requests.\n",
      "Total reward for this episode is -17779.999999999884\n",
      "-----------------------------------\n",
      "| average_route_length | 4.11     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.17e+04 |\n",
      "|    ep_rew_mean       | -3.3e+04 |\n",
      "|    exploration_rate  | 0.398    |\n",
      "| time/                |          |\n",
      "|    episodes          | 476      |\n",
      "|    fps               | 629      |\n",
      "|    time_elapsed      | 15110    |\n",
      "|    total_timesteps   | 9513150  |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00418  |\n",
      "|    n_updates         | 2365787  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -60900.000000000364\n",
      "No more requests.\n",
      "Total reward for this episode is 4020.0000000001346\n",
      "No more requests.\n",
      "Total reward for this episode is -39640.00000000013\n",
      "Num timesteps: 9600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -32657.00\n",
      "No more requests.\n",
      "Total reward for this episode is -17579.999999999898\n",
      "------------------------------------\n",
      "| average_route_length | 4.56      |\n",
      "| blocked_contiguous   | 0.1       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.1       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.16e+04  |\n",
      "|    ep_rew_mean       | -3.26e+04 |\n",
      "|    exploration_rate  | 0.392     |\n",
      "| time/                |           |\n",
      "|    episodes          | 480       |\n",
      "|    fps               | 628       |\n",
      "|    time_elapsed      | 15266     |\n",
      "|    total_timesteps   | 9602343   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0048    |\n",
      "|    n_updates         | 2388085   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 2220.0000000001196\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -24099.999999999905\n",
      "No more requests.\n",
      "Total reward for this episode is -67920.00000000099\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -14219.999999999933\n",
      "------------------------------------\n",
      "| average_route_length | 4.78      |\n",
      "| blocked_contiguous   | 0.05      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.1       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.16e+04  |\n",
      "|    ep_rew_mean       | -3.22e+04 |\n",
      "|    exploration_rate  | 0.386     |\n",
      "| time/                |           |\n",
      "|    episodes          | 484       |\n",
      "|    fps               | 628       |\n",
      "|    time_elapsed      | 15418     |\n",
      "|    total_timesteps   | 9689160   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0014    |\n",
      "|    n_updates         | 2409789   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -3939.9999999998904\n",
      "No more requests.\n",
      "Total reward for this episode is -42080.000000000284\n",
      "No more requests.\n",
      "Total reward for this episode is -22099.999999999858\n",
      "No more requests.\n",
      "Total reward for this episode is -9199.999999999942\n",
      "------------------------------------\n",
      "| average_route_length | 4         |\n",
      "| blocked_contiguous   | 0.1       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.1       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.17e+04  |\n",
      "|    ep_rew_mean       | -3.18e+04 |\n",
      "|    exploration_rate  | 0.381     |\n",
      "| time/                |           |\n",
      "|    episodes          | 488       |\n",
      "|    fps               | 627       |\n",
      "|    time_elapsed      | 15575     |\n",
      "|    total_timesteps   | 9777960   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 30.3      |\n",
      "|    n_updates         | 2431989   |\n",
      "------------------------------------\n",
      "Num timesteps: 9800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -31838.80\n",
      "No more requests.\n",
      "Total reward for this episode is -49720.00000000019\n",
      "No more requests.\n",
      "Total reward for this episode is -31119.99999999985\n",
      "No more requests.\n",
      "Total reward for this episode is -13259.999999999924\n",
      "No more requests.\n",
      "Total reward for this episode is -39699.999999999985\n",
      "------------------------------------\n",
      "| average_route_length | 5.18      |\n",
      "| blocked_contiguous   | 0.15      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.17e+04  |\n",
      "|    ep_rew_mean       | -3.15e+04 |\n",
      "|    exploration_rate  | 0.375     |\n",
      "| time/                |           |\n",
      "|    episodes          | 492       |\n",
      "|    fps               | 627       |\n",
      "|    time_elapsed      | 15731     |\n",
      "|    total_timesteps   | 9866733   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00631   |\n",
      "|    n_updates         | 2454183   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -64580.00000000082\n",
      "No more requests.\n",
      "Total reward for this episode is -52240.00000000033\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -9040.0\n",
      "No more requests.\n",
      "Total reward for this episode is -10939.99999999979\n",
      "------------------------------------\n",
      "| average_route_length | 4.32      |\n",
      "| blocked_contiguous   | 0.05      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.05      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.15e+04  |\n",
      "|    ep_rew_mean       | -2.99e+04 |\n",
      "|    exploration_rate  | 0.37      |\n",
      "| time/                |           |\n",
      "|    episodes          | 496       |\n",
      "|    fps               | 626       |\n",
      "|    time_elapsed      | 15859     |\n",
      "|    total_timesteps   | 9939659   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00211   |\n",
      "|    n_updates         | 2472414   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -32199.99999999988\n",
      "No more requests.\n",
      "Total reward for this episode is -40980.000000000065\n",
      "Num timesteps: 10000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -30028.80\n",
      "No more requests.\n",
      "Total reward for this episode is -52200.00000000012\n",
      "No more requests.\n",
      "Total reward for this episode is -38299.99999999992\n",
      "------------------------------------\n",
      "| average_route_length | 5.29      |\n",
      "| blocked_contiguous   | 0.15      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.15e+04  |\n",
      "|    ep_rew_mean       | -3.08e+04 |\n",
      "|    exploration_rate  | 0.365     |\n",
      "| time/                |           |\n",
      "|    episodes          | 500       |\n",
      "|    fps               | 626       |\n",
      "|    time_elapsed      | 16015     |\n",
      "|    total_timesteps   | 10027883  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00642   |\n",
      "|    n_updates         | 2494470   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -35159.999999999854\n",
      "No more requests.\n",
      "Total reward for this episode is -18379.999999999865\n",
      "No more requests.\n",
      "Total reward for this episode is -18059.999999999847\n",
      "No more requests.\n",
      "Total reward for this episode is -34299.99999999996\n",
      "-----------------------------------\n",
      "| average_route_length | 4.47     |\n",
      "| blocked_contiguous   | 0.15     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.15     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.14e+04 |\n",
      "|    ep_rew_mean       | -3e+04   |\n",
      "|    exploration_rate  | 0.359    |\n",
      "| time/                |          |\n",
      "|    episodes          | 504      |\n",
      "|    fps               | 625      |\n",
      "|    time_elapsed      | 16171    |\n",
      "|    total_timesteps   | 10116820 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0292   |\n",
      "|    n_updates         | 2516704  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 5280.000000000083\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -61040.00000000035\n",
      "No more requests.\n",
      "Total reward for this episode is -36679.99999999986\n",
      "Num timesteps: 10200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -30050.80\n",
      "No more requests.\n",
      "Total reward for this episode is -35119.99999999989\n",
      "------------------------------------\n",
      "| average_route_length | 4.94      |\n",
      "| blocked_contiguous   | 0.15      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.14e+04  |\n",
      "|    ep_rew_mean       | -3.02e+04 |\n",
      "|    exploration_rate  | 0.354     |\n",
      "| time/                |           |\n",
      "|    episodes          | 508       |\n",
      "|    fps               | 624       |\n",
      "|    time_elapsed      | 16327     |\n",
      "|    total_timesteps   | 10204468  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00377   |\n",
      "|    n_updates         | 2538616   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -14439.999999999878\n",
      "No more requests.\n",
      "Total reward for this episode is 3160.000000000122\n",
      "No more requests.\n",
      "Total reward for this episode is 13979.999999999862\n",
      "No more requests.\n",
      "Total reward for this episode is -12519.999999999927\n",
      "------------------------------------\n",
      "| average_route_length | 4.33      |\n",
      "| blocked_contiguous   | 0.1       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.1       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.14e+04  |\n",
      "|    ep_rew_mean       | -2.89e+04 |\n",
      "|    exploration_rate  | 0.348     |\n",
      "| time/                |           |\n",
      "|    episodes          | 512       |\n",
      "|    fps               | 624       |\n",
      "|    time_elapsed      | 16483     |\n",
      "|    total_timesteps   | 10293056  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00414   |\n",
      "|    n_updates         | 2560763   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -10899.999999999958\n",
      "No more requests.\n",
      "Total reward for this episode is 8560.000000000015\n",
      "No more requests.\n",
      "Total reward for this episode is -22179.999999999876\n",
      "No more requests.\n",
      "Total reward for this episode is -17019.999999999884\n",
      "------------------------------------\n",
      "| average_route_length | 4.67      |\n",
      "| blocked_contiguous   | 0.1       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.1       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.14e+04  |\n",
      "|    ep_rew_mean       | -2.79e+04 |\n",
      "|    exploration_rate  | 0.343     |\n",
      "| time/                |           |\n",
      "|    episodes          | 516       |\n",
      "|    fps               | 623       |\n",
      "|    time_elapsed      | 16640     |\n",
      "|    total_timesteps   | 10380762  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00662   |\n",
      "|    n_updates         | 2582690   |\n",
      "------------------------------------\n",
      "Num timesteps: 10400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -27903.80\n",
      "No more requests.\n",
      "Total reward for this episode is -6659.999999999948\n",
      "No more requests.\n",
      "Total reward for this episode is -6779.999999999942\n",
      "No more requests.\n",
      "Total reward for this episode is -26639.999999999913\n",
      "No more requests.\n",
      "Total reward for this episode is 12320.000000000078\n",
      "------------------------------------\n",
      "| average_route_length | 3.79      |\n",
      "| blocked_contiguous   | 0.05      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.05      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.16e+04  |\n",
      "|    ep_rew_mean       | -2.68e+04 |\n",
      "|    exploration_rate  | 0.337     |\n",
      "| time/                |           |\n",
      "|    episodes          | 520       |\n",
      "|    fps               | 623       |\n",
      "|    time_elapsed      | 16796     |\n",
      "|    total_timesteps   | 10468288  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 61.6      |\n",
      "|    n_updates         | 2604571   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -12699.999999999904\n",
      "No more requests.\n",
      "Total reward for this episode is -9339.999999999907\n",
      "No more requests.\n",
      "Total reward for this episode is -10619.99999999993\n",
      "No more requests.\n",
      "Total reward for this episode is -14039.999999999924\n",
      "------------------------------------\n",
      "| average_route_length | 4.56      |\n",
      "| blocked_contiguous   | 0.1       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.1       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.17e+04  |\n",
      "|    ep_rew_mean       | -2.61e+04 |\n",
      "|    exploration_rate  | 0.331     |\n",
      "| time/                |           |\n",
      "|    episodes          | 524       |\n",
      "|    fps               | 622       |\n",
      "|    time_elapsed      | 16952     |\n",
      "|    total_timesteps   | 10556021  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0177    |\n",
      "|    n_updates         | 2626505   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -11979.99999999993\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -41819.99999999996\n",
      "Num timesteps: 10600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -26483.20\n",
      "No more requests.\n",
      "Total reward for this episode is -38879.99999999991\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -13319.999999999965\n",
      "------------------------------------\n",
      "| average_route_length | 4.91      |\n",
      "| blocked_contiguous   | 0.05      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.45      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.16e+04  |\n",
      "|    ep_rew_mean       | -2.61e+04 |\n",
      "|    exploration_rate  | 0.326     |\n",
      "| time/                |           |\n",
      "|    episodes          | 528       |\n",
      "|    fps               | 622       |\n",
      "|    time_elapsed      | 17094     |\n",
      "|    total_timesteps   | 10635379  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00739   |\n",
      "|    n_updates         | 2646344   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -35219.99999999985\n",
      "No more requests.\n",
      "Total reward for this episode is -14299.99999999997\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -33820.00000000003\n",
      "No more requests.\n",
      "Total reward for this episode is -38119.999999999956\n",
      "-----------------------------------\n",
      "| average_route_length | 5.29     |\n",
      "| blocked_contiguous   | 0.15     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.15     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.16e+04 |\n",
      "|    ep_rew_mean       | -2.6e+04 |\n",
      "|    exploration_rate  | 0.321    |\n",
      "| time/                |          |\n",
      "|    episodes          | 532      |\n",
      "|    fps               | 621      |\n",
      "|    time_elapsed      | 17250    |\n",
      "|    total_timesteps   | 10722474 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.003    |\n",
      "|    n_updates         | 2668118  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -28219.99999999986\n",
      "No more requests.\n",
      "Total reward for this episode is 11400.000000000004\n",
      "No more requests.\n",
      "Total reward for this episode is 5600.000000000061\n",
      "Num timesteps: 10800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -25570.00\n",
      "No more requests.\n",
      "Total reward for this episode is -33959.99999999987\n",
      "------------------------------------\n",
      "| average_route_length | 5.65      |\n",
      "| blocked_contiguous   | 0.15      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.15e+04  |\n",
      "|    ep_rew_mean       | -2.57e+04 |\n",
      "|    exploration_rate  | 0.315     |\n",
      "| time/                |           |\n",
      "|    episodes          | 536       |\n",
      "|    fps               | 620       |\n",
      "|    time_elapsed      | 17406     |\n",
      "|    total_timesteps   | 10809552  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00114   |\n",
      "|    n_updates         | 2689887   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -11239.999999999887\n",
      "No more requests.\n",
      "Total reward for this episode is -29699.99999999985\n",
      "No more requests.\n",
      "Total reward for this episode is -12319.999999999869\n",
      "No more requests.\n",
      "Total reward for this episode is 9440.000000000104\n",
      "-----------------------------------\n",
      "| average_route_length | 3.89     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.15e+04 |\n",
      "|    ep_rew_mean       | -2.5e+04 |\n",
      "|    exploration_rate  | 0.31     |\n",
      "| time/                |          |\n",
      "|    episodes          | 540      |\n",
      "|    fps               | 620      |\n",
      "|    time_elapsed      | 17562    |\n",
      "|    total_timesteps   | 10896988 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00234  |\n",
      "|    n_updates         | 2711746  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -47100.00000000026\n",
      "No more requests.\n",
      "Total reward for this episode is -44560.00000000016\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -32179.999999999894\n",
      "No more requests.\n",
      "Total reward for this episode is -9219.999999999922\n",
      "-----------------------------------\n",
      "| average_route_length | 4.33     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.15e+04 |\n",
      "|    ep_rew_mean       | -2.5e+04 |\n",
      "|    exploration_rate  | 0.304    |\n",
      "| time/                |          |\n",
      "|    episodes          | 544      |\n",
      "|    fps               | 619      |\n",
      "|    time_elapsed      | 17718    |\n",
      "|    total_timesteps   | 10984228 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00222  |\n",
      "|    n_updates         | 2733556  |\n",
      "-----------------------------------\n",
      "Num timesteps: 11000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -24989.80\n",
      "No more requests.\n",
      "Total reward for this episode is -64340.00000000039\n",
      "No more requests.\n",
      "Total reward for this episode is -29179.999999999894\n",
      "No more requests.\n",
      "Total reward for this episode is 5060.000000000111\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -49080.000000000065\n",
      "------------------------------------\n",
      "| average_route_length | 4.62      |\n",
      "| blocked_contiguous   | 0.15      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.2       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.14e+04  |\n",
      "|    ep_rew_mean       | -2.56e+04 |\n",
      "|    exploration_rate  | 0.299     |\n",
      "| time/                |           |\n",
      "|    episodes          | 548       |\n",
      "|    fps               | 619       |\n",
      "|    time_elapsed      | 17877     |\n",
      "|    total_timesteps   | 11071789  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00445   |\n",
      "|    n_updates         | 2755447   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 36640.0\n",
      "No more requests.\n",
      "Total reward for this episode is 3000.000000000095\n",
      "No more requests.\n",
      "Total reward for this episode is -29219.999999999858\n",
      "No more requests.\n",
      "Total reward for this episode is -3819.9999999999654\n",
      "------------------------------------\n",
      "| average_route_length | 4.22      |\n",
      "| blocked_contiguous   | 0.1       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.1       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.16e+04  |\n",
      "|    ep_rew_mean       | -2.46e+04 |\n",
      "|    exploration_rate  | 0.293     |\n",
      "| time/                |           |\n",
      "|    episodes          | 552       |\n",
      "|    fps               | 618       |\n",
      "|    time_elapsed      | 18033     |\n",
      "|    total_timesteps   | 11158279  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00795   |\n",
      "|    n_updates         | 2777069   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -12599.999999999924\n",
      "Num timesteps: 11200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -24330.40\n",
      "No more requests.\n",
      "Total reward for this episode is 7360.000000000082\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -11679.999999999918\n",
      "No more requests.\n",
      "Total reward for this episode is -2819.9999999999086\n",
      "------------------------------------\n",
      "| average_route_length | 4.11      |\n",
      "| blocked_contiguous   | 0.1       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.1       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.15e+04  |\n",
      "|    ep_rew_mean       | -2.39e+04 |\n",
      "|    exploration_rate  | 0.288     |\n",
      "| time/                |           |\n",
      "|    episodes          | 556       |\n",
      "|    fps               | 618       |\n",
      "|    time_elapsed      | 18189     |\n",
      "|    total_timesteps   | 11244465  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00648   |\n",
      "|    n_updates         | 2798616   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -30279.999999999916\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -6099.999999999863\n",
      "No more requests.\n",
      "Total reward for this episode is -7099.999999999916\n",
      "No more requests.\n",
      "Total reward for this episode is -20659.999999999894\n",
      "-----------------------------------\n",
      "| average_route_length | 4.12     |\n",
      "| blocked_contiguous   | 0.15     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.15     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.15e+04 |\n",
      "|    ep_rew_mean       | -2.2e+04 |\n",
      "|    exploration_rate  | 0.282    |\n",
      "| time/                |          |\n",
      "|    episodes          | 560      |\n",
      "|    fps               | 617      |\n",
      "|    time_elapsed      | 18347    |\n",
      "|    total_timesteps   | 11331705 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000472 |\n",
      "|    n_updates         | 2820426  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -26119.999999999884\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 10880.000000000067\n",
      "No more requests.\n",
      "Total reward for this episode is 14300.000000000065\n",
      "Num timesteps: 11400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -20902.00\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -12219.999999999887\n",
      "------------------------------------\n",
      "| average_route_length | 4.44      |\n",
      "| blocked_contiguous   | 0.05      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.1       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.14e+04  |\n",
      "|    ep_rew_mean       | -2.06e+04 |\n",
      "|    exploration_rate  | 0.277     |\n",
      "| time/                |           |\n",
      "|    episodes          | 564       |\n",
      "|    fps               | 617       |\n",
      "|    time_elapsed      | 18501     |\n",
      "|    total_timesteps   | 11415741  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00484   |\n",
      "|    n_updates         | 2841435   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -22459.999999999887\n",
      "No more requests.\n",
      "Total reward for this episode is -25179.99999999989\n",
      "No more requests.\n",
      "Total reward for this episode is -4959.999999999925\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -32479.999999999836\n",
      "------------------------------------\n",
      "| average_route_length | 5.06      |\n",
      "| blocked_contiguous   | 0.1       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.17e+04  |\n",
      "|    ep_rew_mean       | -2.04e+04 |\n",
      "|    exploration_rate  | 0.272     |\n",
      "| time/                |           |\n",
      "|    episodes          | 568       |\n",
      "|    fps               | 616       |\n",
      "|    time_elapsed      | 18659     |\n",
      "|    total_timesteps   | 11502347  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0013    |\n",
      "|    n_updates         | 2863086   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -26940.000000000062\n",
      "No more requests.\n",
      "Total reward for this episode is 13780.00000000008\n",
      "No more requests.\n",
      "Total reward for this episode is -20939.9999999999\n",
      "No more requests.\n",
      "Total reward for this episode is -6439.999999999918\n",
      "------------------------------------\n",
      "| average_route_length | 4.33      |\n",
      "| blocked_contiguous   | 0.1       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.1       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.16e+04  |\n",
      "|    ep_rew_mean       | -2.06e+04 |\n",
      "|    exploration_rate  | 0.266     |\n",
      "| time/                |           |\n",
      "|    episodes          | 572       |\n",
      "|    fps               | 615       |\n",
      "|    time_elapsed      | 18815     |\n",
      "|    total_timesteps   | 11588286  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00975   |\n",
      "|    n_updates         | 2884571   |\n",
      "------------------------------------\n",
      "Num timesteps: 11600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -20639.40\n",
      "No more requests.\n",
      "Total reward for this episode is -10679.999999999945\n",
      "No more requests.\n",
      "Total reward for this episode is -579.999999999967\n",
      "No more requests.\n",
      "Total reward for this episode is -10059.99999999989\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -5199.9999999999\n",
      "------------------------------------\n",
      "| average_route_length | 4.22      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.1       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.16e+04  |\n",
      "|    ep_rew_mean       | -1.93e+04 |\n",
      "|    exploration_rate  | 0.261     |\n",
      "| time/                |           |\n",
      "|    episodes          | 576       |\n",
      "|    fps               | 615       |\n",
      "|    time_elapsed      | 18971     |\n",
      "|    total_timesteps   | 11673408  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00106   |\n",
      "|    n_updates         | 2905851   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 10300.000000000042\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -25939.999999999964\n",
      "No more requests.\n",
      "Total reward for this episode is -5499.999999999892\n",
      "No more requests.\n",
      "Total reward for this episode is -6199.999999999905\n",
      "------------------------------------\n",
      "| average_route_length | 4.11      |\n",
      "| blocked_contiguous   | 0.1       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.1       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.15e+04  |\n",
      "|    ep_rew_mean       | -1.85e+04 |\n",
      "|    exploration_rate  | 0.256     |\n",
      "| time/                |           |\n",
      "|    episodes          | 580       |\n",
      "|    fps               | 614       |\n",
      "|    time_elapsed      | 19114     |\n",
      "|    total_timesteps   | 11751013  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00507   |\n",
      "|    n_updates         | 2925253   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 17440.00000000004\n",
      "No more requests.\n",
      "Total reward for this episode is -39279.99999999986\n",
      "Num timesteps: 11800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -18481.00\n",
      "No more requests.\n",
      "Total reward for this episode is 10840.000000000076\n",
      "No more requests.\n",
      "Total reward for this episode is 6840.000000000075\n",
      "------------------------------------\n",
      "| average_route_length | 4.21      |\n",
      "| blocked_contiguous   | 0.05      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.05      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.15e+04  |\n",
      "|    ep_rew_mean       | -1.75e+04 |\n",
      "|    exploration_rate  | 0.25      |\n",
      "| time/                |           |\n",
      "|    episodes          | 584       |\n",
      "|    fps               | 614       |\n",
      "|    time_elapsed      | 19270     |\n",
      "|    total_timesteps   | 11836128  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0156    |\n",
      "|    n_updates         | 2946531   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -9159.999999999918\n",
      "No more requests.\n",
      "Total reward for this episode is -39260.000000000095\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -8920.0\n",
      "No more requests.\n",
      "Total reward for this episode is 12620.00000000007\n",
      "------------------------------------\n",
      "| average_route_length | 3.89      |\n",
      "| blocked_contiguous   | 0.05      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.05      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.13e+04  |\n",
      "|    ep_rew_mean       | -1.72e+04 |\n",
      "|    exploration_rate  | 0.246     |\n",
      "| time/                |           |\n",
      "|    episodes          | 588       |\n",
      "|    fps               | 613       |\n",
      "|    time_elapsed      | 19398     |\n",
      "|    total_timesteps   | 11905862  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00273   |\n",
      "|    n_updates         | 2963965   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -39539.99999999995\n",
      "No more requests.\n",
      "Total reward for this episode is -1419.9999999999513\n",
      "No more requests.\n",
      "Total reward for this episode is 8000.0000000001055\n",
      "No more requests.\n",
      "Total reward for this episode is -4279.99999999993\n",
      "------------------------------------\n",
      "| average_route_length | 4.33      |\n",
      "| blocked_contiguous   | 0.1       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.1       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.12e+04  |\n",
      "|    ep_rew_mean       | -1.62e+04 |\n",
      "|    exploration_rate  | 0.241     |\n",
      "| time/                |           |\n",
      "|    episodes          | 592       |\n",
      "|    fps               | 613       |\n",
      "|    time_elapsed      | 19554     |\n",
      "|    total_timesteps   | 11991552  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0014    |\n",
      "|    n_updates         | 2985387   |\n",
      "------------------------------------\n",
      "Num timesteps: 12000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -16185.60\n",
      "No more requests.\n",
      "Total reward for this episode is -40240.00000000003\n",
      "No more requests.\n",
      "Total reward for this episode is -1459.9999999998913\n",
      "No more requests.\n",
      "Total reward for this episode is -20619.99999999989\n",
      "No more requests.\n",
      "Total reward for this episode is -38819.999999999985\n",
      "------------------------------------\n",
      "| average_route_length | 4.88      |\n",
      "| blocked_contiguous   | 0.2       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.2       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.14e+04  |\n",
      "|    ep_rew_mean       | -1.58e+04 |\n",
      "|    exploration_rate  | 0.235     |\n",
      "| time/                |           |\n",
      "|    episodes          | 596       |\n",
      "|    fps               | 612       |\n",
      "|    time_elapsed      | 19710     |\n",
      "|    total_timesteps   | 12076517  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0354    |\n",
      "|    n_updates         | 3006629   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 16280.000000000047\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -4699.999999999872\n",
      "No more requests.\n",
      "Total reward for this episode is -37979.999999999876\n",
      "No more requests.\n",
      "Total reward for this episode is -17859.999999999898\n",
      "------------------------------------\n",
      "| average_route_length | 4.12      |\n",
      "| blocked_contiguous   | 0.15      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.13e+04  |\n",
      "|    ep_rew_mean       | -1.45e+04 |\n",
      "|    exploration_rate  | 0.23      |\n",
      "| time/                |           |\n",
      "|    episodes          | 600       |\n",
      "|    fps               | 612       |\n",
      "|    time_elapsed      | 19868     |\n",
      "|    total_timesteps   | 12162813  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00193   |\n",
      "|    n_updates         | 3028203   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 10440.000000000075\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -12040.0\n",
      "Num timesteps: 12200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -14010.40\n",
      "No more requests.\n",
      "Total reward for this episode is 14980.000000000022\n",
      "No more requests.\n",
      "Total reward for this episode is -1959.9999999999127\n",
      "------------------------------------\n",
      "| average_route_length | 4         |\n",
      "| blocked_contiguous   | 0.1       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.1       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.11e+04  |\n",
      "|    ep_rew_mean       | -1.34e+04 |\n",
      "|    exploration_rate  | 0.225     |\n",
      "| time/                |           |\n",
      "|    episodes          | 604       |\n",
      "|    fps               | 611       |\n",
      "|    time_elapsed      | 19992     |\n",
      "|    total_timesteps   | 12229856  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00263   |\n",
      "|    n_updates         | 3044963   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -19719.99999999988\n",
      "No more requests.\n",
      "Total reward for this episode is -9359.9999999999\n",
      "No more requests.\n",
      "Total reward for this episode is -5099.999999999931\n",
      "No more requests.\n",
      "Total reward for this episode is -21399.999999999898\n",
      "------------------------------------\n",
      "| average_route_length | 4.71      |\n",
      "| blocked_contiguous   | 0.15      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.11e+04  |\n",
      "|    ep_rew_mean       | -1.26e+04 |\n",
      "|    exploration_rate  | 0.22      |\n",
      "| time/                |           |\n",
      "|    episodes          | 608       |\n",
      "|    fps               | 611       |\n",
      "|    time_elapsed      | 20149     |\n",
      "|    total_timesteps   | 12314772  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00124   |\n",
      "|    n_updates         | 3066192   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -19539.999999999927\n",
      "No more requests.\n",
      "Total reward for this episode is 1759.9999999999454\n",
      "No more requests.\n",
      "Total reward for this episode is -4319.999999999923\n",
      "No more requests.\n",
      "Total reward for this episode is 14720.000000000076\n",
      "------------------------------------\n",
      "| average_route_length | 3.58      |\n",
      "| blocked_contiguous   | 0.05      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.05      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.11e+04  |\n",
      "|    ep_rew_mean       | -1.26e+04 |\n",
      "|    exploration_rate  | 0.215     |\n",
      "| time/                |           |\n",
      "|    episodes          | 612       |\n",
      "|    fps               | 610       |\n",
      "|    time_elapsed      | 20305     |\n",
      "|    total_timesteps   | 12399428  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00484   |\n",
      "|    n_updates         | 3087356   |\n",
      "------------------------------------\n",
      "Num timesteps: 12400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -12612.40\n",
      "No more requests.\n",
      "Total reward for this episode is -9759.999999999975\n",
      "No more requests.\n",
      "Total reward for this episode is -2259.9999999999495\n",
      "No more requests.\n",
      "Total reward for this episode is -4619.999999999967\n",
      "No more requests.\n",
      "Total reward for this episode is -25919.999999999884\n",
      "------------------------------------\n",
      "| average_route_length | 5.06      |\n",
      "| blocked_contiguous   | 0.15      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.1e+04   |\n",
      "|    ep_rew_mean       | -1.26e+04 |\n",
      "|    exploration_rate  | 0.209     |\n",
      "| time/                |           |\n",
      "|    episodes          | 616       |\n",
      "|    fps               | 610       |\n",
      "|    time_elapsed      | 20461     |\n",
      "|    total_timesteps   | 12483799  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0149    |\n",
      "|    n_updates         | 3108449   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -17679.99999999991\n",
      "No more requests.\n",
      "Total reward for this episode is -4419.999999999937\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -6979.999999999995\n",
      "No more requests.\n",
      "Total reward for this episode is 760.0000000000374\n",
      "------------------------------------\n",
      "| average_route_length | 4         |\n",
      "| blocked_contiguous   | 0.1       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.1       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.08e+04  |\n",
      "|    ep_rew_mean       | -1.26e+04 |\n",
      "|    exploration_rate  | 0.205     |\n",
      "| time/                |           |\n",
      "|    episodes          | 620       |\n",
      "|    fps               | 609       |\n",
      "|    time_elapsed      | 20589     |\n",
      "|    total_timesteps   | 12552437  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00221   |\n",
      "|    n_updates         | 3125609   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -6039.999999999996\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -7219.999999999996\n",
      "No more requests.\n",
      "Total reward for this episode is -15519.99999999994\n",
      "Num timesteps: 12600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -12572.60\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is 19299.999999999858\n",
      "------------------------------------\n",
      "| average_route_length | 3.58      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.05      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.05e+04  |\n",
      "|    ep_rew_mean       | -1.21e+04 |\n",
      "|    exploration_rate  | 0.202     |\n",
      "| time/                |           |\n",
      "|    episodes          | 624       |\n",
      "|    fps               | 609       |\n",
      "|    time_elapsed      | 20691     |\n",
      "|    total_timesteps   | 12606865  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 54.7      |\n",
      "|    n_updates         | 3139216   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -20379.9999999999\n",
      "No more requests.\n",
      "Total reward for this episode is 16879.999999999884\n",
      "No more requests.\n",
      "Total reward for this episode is 4000.0000000000837\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -38859.999999999935\n",
      "------------------------------------\n",
      "| average_route_length | 5.12      |\n",
      "| blocked_contiguous   | 0.15      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.2       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.06e+04  |\n",
      "|    ep_rew_mean       | -1.15e+04 |\n",
      "|    exploration_rate  | 0.196     |\n",
      "| time/                |           |\n",
      "|    episodes          | 628       |\n",
      "|    fps               | 608       |\n",
      "|    time_elapsed      | 20848     |\n",
      "|    total_timesteps   | 12691288  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0198    |\n",
      "|    n_updates         | 3160321   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -18079.99999999996\n",
      "No more requests.\n",
      "Total reward for this episode is -16919.999999999975\n",
      "No more requests.\n",
      "Total reward for this episode is -10399.99999999992\n",
      "No more requests.\n",
      "Total reward for this episode is 1520.0000000000848\n",
      "------------------------------------\n",
      "| average_route_length | 3.89      |\n",
      "| blocked_contiguous   | 0.1       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.1       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.05e+04  |\n",
      "|    ep_rew_mean       | -1.07e+04 |\n",
      "|    exploration_rate  | 0.191     |\n",
      "| time/                |           |\n",
      "|    episodes          | 632       |\n",
      "|    fps               | 608       |\n",
      "|    time_elapsed      | 21004     |\n",
      "|    total_timesteps   | 12775056  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00377   |\n",
      "|    n_updates         | 3181263   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 1660.000000000001\n",
      "Num timesteps: 12800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -10386.80\n",
      "No more requests.\n",
      "Total reward for this episode is -21799.99999999988\n",
      "No more requests.\n",
      "Total reward for this episode is -6319.999999999926\n",
      "No more requests.\n",
      "Total reward for this episode is -39.99999999992383\n",
      "------------------------------------\n",
      "| average_route_length | 4         |\n",
      "| blocked_contiguous   | 0.1       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.1       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.05e+04  |\n",
      "|    ep_rew_mean       | -1.05e+04 |\n",
      "|    exploration_rate  | 0.186     |\n",
      "| time/                |           |\n",
      "|    episodes          | 636       |\n",
      "|    fps               | 607       |\n",
      "|    time_elapsed      | 21160     |\n",
      "|    total_timesteps   | 12858500  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0117    |\n",
      "|    n_updates         | 3202124   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 180.00000000008106\n",
      "No more requests.\n",
      "Total reward for this episode is 12580.000000000064\n",
      "No more requests.\n",
      "Total reward for this episode is 14180.000000000015\n",
      "No more requests.\n",
      "Total reward for this episode is 14099.999999999985\n",
      "------------------------------------\n",
      "| average_route_length | 4         |\n",
      "| blocked_contiguous   | 0.05      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.05      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.05e+04  |\n",
      "|    ep_rew_mean       | -9.65e+03 |\n",
      "|    exploration_rate  | 0.18      |\n",
      "| time/                |           |\n",
      "|    episodes          | 640       |\n",
      "|    fps               | 607       |\n",
      "|    time_elapsed      | 21316     |\n",
      "|    total_timesteps   | 12942427  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00108   |\n",
      "|    n_updates         | 3223106   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 10540.000000000055\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -18499.999999999927\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -9579.999999999975\n",
      "Num timesteps: 13000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -8576.00\n",
      "No more requests.\n",
      "Total reward for this episode is 11000.00000000006\n",
      "------------------------------------\n",
      "| average_route_length | 4.21      |\n",
      "| blocked_contiguous   | 0.05      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.05      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.03e+04  |\n",
      "|    ep_rew_mean       | -8.37e+03 |\n",
      "|    exploration_rate  | 0.176     |\n",
      "| time/                |           |\n",
      "|    episodes          | 644       |\n",
      "|    fps               | 606       |\n",
      "|    time_elapsed      | 21453     |\n",
      "|    total_timesteps   | 13015060  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00304   |\n",
      "|    n_updates         | 3241264   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -13319.999999999927\n",
      "No more requests.\n",
      "Total reward for this episode is 580.0000000000545\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 7740.000000000012\n",
      "No more requests.\n",
      "Total reward for this episode is -1519.9999999999577\n",
      "------------------------------------\n",
      "| average_route_length | 3.89      |\n",
      "| blocked_contiguous   | 0.1       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.1       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.02e+04  |\n",
      "|    ep_rew_mean       | -7.16e+03 |\n",
      "|    exploration_rate  | 0.171     |\n",
      "| time/                |           |\n",
      "|    episodes          | 648       |\n",
      "|    fps               | 606       |\n",
      "|    time_elapsed      | 21591     |\n",
      "|    total_timesteps   | 13089238  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00696   |\n",
      "|    n_updates         | 3259809   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 1480.0000000000778\n",
      "No more requests.\n",
      "Total reward for this episode is 2620.0000000000477\n",
      "No more requests.\n",
      "Total reward for this episode is -1299.999999999936\n",
      "No more requests.\n",
      "Total reward for this episode is 13080.000000000064\n",
      "------------------------------------\n",
      "| average_route_length | 3.89      |\n",
      "| blocked_contiguous   | 0.05      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.05      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.01e+04  |\n",
      "|    ep_rew_mean       | -7.07e+03 |\n",
      "|    exploration_rate  | 0.166     |\n",
      "| time/                |           |\n",
      "|    episodes          | 652       |\n",
      "|    fps               | 605       |\n",
      "|    time_elapsed      | 21747     |\n",
      "|    total_timesteps   | 13172363  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00379   |\n",
      "|    n_updates         | 3280590   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -19699.999999999913\n",
      "Num timesteps: 13200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -7141.20\n",
      "No more requests.\n",
      "Total reward for this episode is 13600.00000000007\n",
      "No more requests.\n",
      "Total reward for this episode is -2619.999999999926\n",
      "No more requests.\n",
      "Total reward for this episode is 13840.000000000065\n",
      "------------------------------------\n",
      "| average_route_length | 3.79      |\n",
      "| blocked_contiguous   | 0.05      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.05      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.01e+04  |\n",
      "|    ep_rew_mean       | -6.82e+03 |\n",
      "|    exploration_rate  | 0.16      |\n",
      "| time/                |           |\n",
      "|    episodes          | 656       |\n",
      "|    fps               | 605       |\n",
      "|    time_elapsed      | 21904     |\n",
      "|    total_timesteps   | 13255589  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00319   |\n",
      "|    n_updates         | 3301397   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -919.9999999999479\n",
      "No more requests.\n",
      "Total reward for this episode is 17000.00000000004\n",
      "No more requests.\n",
      "Total reward for this episode is -12419.99999999994\n",
      "No more requests.\n",
      "Total reward for this episode is -13159.999999999925\n",
      "------------------------------------\n",
      "| average_route_length | 4.24      |\n",
      "| blocked_contiguous   | 0.15      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.01e+04  |\n",
      "|    ep_rew_mean       | -6.38e+03 |\n",
      "|    exploration_rate  | 0.155     |\n",
      "| time/                |           |\n",
      "|    episodes          | 660       |\n",
      "|    fps               | 604       |\n",
      "|    time_elapsed      | 22060     |\n",
      "|    total_timesteps   | 13338790  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.229     |\n",
      "|    n_updates         | 3322197   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 300.0000000000057\n",
      "No more requests.\n",
      "Total reward for this episode is -36800.00000000003\n",
      "Num timesteps: 13400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -6594.40\n",
      "No more requests.\n",
      "Total reward for this episode is 4239.9999999999545\n",
      "No more requests.\n",
      "Total reward for this episode is -16739.999999999916\n",
      "------------------------------------\n",
      "| average_route_length | 4.35      |\n",
      "| blocked_contiguous   | 0.15      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.01e+04  |\n",
      "|    ep_rew_mean       | -6.74e+03 |\n",
      "|    exploration_rate  | 0.15      |\n",
      "| time/                |           |\n",
      "|    episodes          | 664       |\n",
      "|    fps               | 604       |\n",
      "|    time_elapsed      | 22217     |\n",
      "|    total_timesteps   | 13422225  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0303    |\n",
      "|    n_updates         | 3343056   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -18019.999999999956\n",
      "No more requests.\n",
      "Total reward for this episode is -919.9999999999616\n",
      "No more requests.\n",
      "Total reward for this episode is -15579.999999999922\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -3539.999999999969\n",
      "------------------------------------\n",
      "| average_route_length | 4.2       |\n",
      "| blocked_contiguous   | 0.05      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.5       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2e+04     |\n",
      "|    ep_rew_mean       | -6.26e+03 |\n",
      "|    exploration_rate  | 0.145     |\n",
      "| time/                |           |\n",
      "|    episodes          | 668       |\n",
      "|    fps               | 603       |\n",
      "|    time_elapsed      | 22360     |\n",
      "|    total_timesteps   | 13497697  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 14        |\n",
      "|    n_updates         | 3361924   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 12060.000000000073\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -659.9999999999682\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -4979.999999999997\n",
      "No more requests.\n",
      "Total reward for this episode is 15200.000000000015\n",
      "------------------------------------\n",
      "| average_route_length | 4.11      |\n",
      "| blocked_contiguous   | 0.05      |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.05      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.97e+04  |\n",
      "|    ep_rew_mean       | -5.63e+03 |\n",
      "|    exploration_rate  | 0.141     |\n",
      "| time/                |           |\n",
      "|    episodes          | 672       |\n",
      "|    fps               | 603       |\n",
      "|    time_elapsed      | 22476     |\n",
      "|    total_timesteps   | 13559040  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00879   |\n",
      "|    n_updates         | 3377259   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 15380.000000000044\n",
      "Num timesteps: 13600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -5369.00\n",
      "No more requests.\n",
      "Total reward for this episode is 14680.00000000006\n",
      "No more requests.\n",
      "Total reward for this episode is 3060.0000000000537\n",
      "No more requests.\n",
      "Total reward for this episode is 1100.0000000000816\n",
      "------------------------------------\n",
      "| average_route_length | 4         |\n",
      "| blocked_contiguous   | 0.1       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.1       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.97e+04  |\n",
      "|    ep_rew_mean       | -5.02e+03 |\n",
      "|    exploration_rate  | 0.136     |\n",
      "| time/                |           |\n",
      "|    episodes          | 676       |\n",
      "|    fps               | 602       |\n",
      "|    time_elapsed      | 22633     |\n",
      "|    total_timesteps   | 13641951  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0212    |\n",
      "|    n_updates         | 3397987   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -14419.99999999994\n",
      "No more requests.\n",
      "Total reward for this episode is 30340.000000000022\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -3539.999999999995\n",
      "No more requests.\n",
      "Total reward for this episode is -119.99999999992781\n",
      "------------------------------------\n",
      "| average_route_length | 4.22      |\n",
      "| blocked_contiguous   | 0.1       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.1       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.96e+04  |\n",
      "|    ep_rew_mean       | -4.63e+03 |\n",
      "|    exploration_rate  | 0.132     |\n",
      "| time/                |           |\n",
      "|    episodes          | 680       |\n",
      "|    fps               | 602       |\n",
      "|    time_elapsed      | 22763     |\n",
      "|    total_timesteps   | 13710607  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 27.8      |\n",
      "|    n_updates         | 3415151   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -8119.999999999959\n",
      "No more requests.\n",
      "Total reward for this episode is 15760.000000000011\n",
      "No more requests.\n",
      "Total reward for this episode is 1320.0000000000641\n",
      "No more requests.\n",
      "Total reward for this episode is 37399.99999999999\n",
      "------------------------------------\n",
      "| average_route_length | 3.1       |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0         |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.96e+04  |\n",
      "|    ep_rew_mean       | -4.12e+03 |\n",
      "|    exploration_rate  | 0.126     |\n",
      "| time/                |           |\n",
      "|    episodes          | 684       |\n",
      "|    fps               | 601       |\n",
      "|    time_elapsed      | 22919     |\n",
      "|    total_timesteps   | 13793283  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.003     |\n",
      "|    n_updates         | 3435820   |\n",
      "------------------------------------\n",
      "Num timesteps: 13800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -4121.00\n",
      "No more requests.\n",
      "Total reward for this episode is 19379.999999999898\n",
      "No more requests.\n",
      "Total reward for this episode is 19480.000000000036\n",
      "No more requests.\n",
      "Total reward for this episode is 2640.00000000006\n",
      "No more requests.\n",
      "Total reward for this episode is 33459.99999999999\n",
      "------------------------------------\n",
      "| average_route_length | 3.5       |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0         |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.97e+04  |\n",
      "|    ep_rew_mean       | -2.93e+03 |\n",
      "|    exploration_rate  | 0.121     |\n",
      "| time/                |           |\n",
      "|    episodes          | 688       |\n",
      "|    fps               | 601       |\n",
      "|    time_elapsed      | 23075     |\n",
      "|    total_timesteps   | 13875370  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00083   |\n",
      "|    n_updates         | 3456342   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 3880.0000000000546\n",
      "No more requests.\n",
      "Total reward for this episode is 19499.999999999913\n",
      "No more requests.\n",
      "Total reward for this episode is -10499.999999999965\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -2539.999999999995\n",
      "------------------------------------\n",
      "| average_route_length | 4         |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.7       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.95e+04  |\n",
      "|    ep_rew_mean       | -2.45e+03 |\n",
      "|    exploration_rate  | 0.117     |\n",
      "| time/                |           |\n",
      "|    episodes          | 692       |\n",
      "|    fps               | 600       |\n",
      "|    time_elapsed      | 23205     |\n",
      "|    total_timesteps   | 13944061  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 6.2       |\n",
      "|    n_updates         | 3473515   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 22839.999999999884\n",
      "No more requests.\n",
      "Total reward for this episode is 5560.0000000000255\n",
      "Num timesteps: 14000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -1747.40\n",
      "No more requests.\n",
      "Total reward for this episode is 2380.0000000000437\n",
      "No more requests.\n",
      "Total reward for this episode is 16700.000000000044\n",
      "-----------------------------------\n",
      "| average_route_length | 3.79     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.95e+04 |\n",
      "|    ep_rew_mean       | -962     |\n",
      "|    exploration_rate  | 0.112    |\n",
      "| time/                |          |\n",
      "|    episodes          | 696      |\n",
      "|    fps               | 600      |\n",
      "|    time_elapsed      | 23361    |\n",
      "|    total_timesteps   | 14026338 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00255  |\n",
      "|    n_updates         | 3494084  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 1860.0000000000534\n",
      "No more requests.\n",
      "Total reward for this episode is 600.0000000000505\n",
      "No more requests.\n",
      "Total reward for this episode is -11119.999999999973\n",
      "No more requests.\n",
      "Total reward for this episode is 1900.0000000000525\n",
      "-----------------------------------\n",
      "| average_route_length | 4.11     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.95e+04 |\n",
      "|    ep_rew_mean       | -692     |\n",
      "|    exploration_rate  | 0.106    |\n",
      "| time/                |          |\n",
      "|    episodes          | 700      |\n",
      "|    fps               | 599      |\n",
      "|    time_elapsed      | 23517    |\n",
      "|    total_timesteps   | 14108241 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 6.66     |\n",
      "|    n_updates         | 3514560  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 13840.000000000058\n",
      "No more requests.\n",
      "Total reward for this episode is 14380.00000000005\n",
      "No more requests.\n",
      "Total reward for this episode is -16799.999999999967\n",
      "No more requests.\n",
      "Total reward for this episode is 6619.999999999971\n",
      "-----------------------------------\n",
      "| average_route_length | 3.89     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.96e+04 |\n",
      "|    ep_rew_mean       | -632     |\n",
      "|    exploration_rate  | 0.101    |\n",
      "| time/                |          |\n",
      "|    episodes          | 704      |\n",
      "|    fps               | 599      |\n",
      "|    time_elapsed      | 23674    |\n",
      "|    total_timesteps   | 14190740 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00137  |\n",
      "|    n_updates         | 3535184  |\n",
      "-----------------------------------\n",
      "Num timesteps: 14200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -631.60\n",
      "No more requests.\n",
      "Total reward for this episode is 18560.00000000003\n",
      "No more requests.\n",
      "Total reward for this episode is -11499.999999999964\n",
      "No more requests.\n",
      "Total reward for this episode is -26939.999999999956\n",
      "No more requests.\n",
      "Total reward for this episode is -9519.999999999953\n",
      "-----------------------------------\n",
      "| average_route_length | 4.35     |\n",
      "| blocked_contiguous   | 0.15     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.15     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.96e+04 |\n",
      "|    ep_rew_mean       | -370     |\n",
      "|    exploration_rate  | 0.0961   |\n",
      "| time/                |          |\n",
      "|    episodes          | 708      |\n",
      "|    fps               | 598      |\n",
      "|    time_elapsed      | 23830    |\n",
      "|    total_timesteps   | 14272551 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000731 |\n",
      "|    n_updates         | 3555637  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 12060.000000000045\n",
      "No more requests.\n",
      "Total reward for this episode is 4140.000000000015\n",
      "No more requests.\n",
      "Total reward for this episode is 5320.000000000042\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -7819.999999999977\n",
      "-----------------------------------\n",
      "| average_route_length | 4.67     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.55     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.95e+04 |\n",
      "|    ep_rew_mean       | -153     |\n",
      "|    exploration_rate  | 0.0915   |\n",
      "| time/                |          |\n",
      "|    episodes          | 712      |\n",
      "|    fps               | 598      |\n",
      "|    time_elapsed      | 23968    |\n",
      "|    total_timesteps   | 14345021 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00355  |\n",
      "|    n_updates         | 3573755  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -8259.999999999964\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -6199.999999999979\n",
      "No more requests.\n",
      "Total reward for this episode is 5100.000000000035\n",
      "Num timesteps: 14400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -75.00\n",
      "No more requests.\n",
      "Total reward for this episode is -8259.999999999965\n",
      "-----------------------------------\n",
      "| average_route_length | 4.35     |\n",
      "| blocked_contiguous   | 0.15     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.15     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.93e+04 |\n",
      "|    ep_rew_mean       | 102      |\n",
      "|    exploration_rate  | 0.0869   |\n",
      "| time/                |          |\n",
      "|    episodes          | 716      |\n",
      "|    fps               | 598      |\n",
      "|    time_elapsed      | 24107    |\n",
      "|    total_timesteps   | 14417023 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00172  |\n",
      "|    n_updates         | 3591755  |\n",
      "-----------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is 1320.0000000000316\n",
      "No more requests.\n",
      "Total reward for this episode is 700.0000000000384\n",
      "No more requests.\n",
      "Total reward for this episode is 14900.000000000036\n",
      "No more requests.\n",
      "Total reward for this episode is 4740.000000000019\n",
      "-----------------------------------\n",
      "| average_route_length | 4.11     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.95e+04 |\n",
      "|    ep_rew_mean       | 598      |\n",
      "|    exploration_rate  | 0.0817   |\n",
      "| time/                |          |\n",
      "|    episodes          | 720      |\n",
      "|    fps               | 597      |\n",
      "|    time_elapsed      | 24264    |\n",
      "|    total_timesteps   | 14498763 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0102   |\n",
      "|    n_updates         | 3612190  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 5340.000000000052\n",
      "No more requests.\n",
      "Total reward for this episode is -9739.999999999962\n",
      "No more requests.\n",
      "Total reward for this episode is 5060.000000000044\n",
      "No more requests.\n",
      "Total reward for this episode is 5920.000000000038\n",
      "-----------------------------------\n",
      "| average_route_length | 4        |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.97e+04 |\n",
      "|    ep_rew_mean       | 642      |\n",
      "|    exploration_rate  | 0.0767   |\n",
      "| time/                |          |\n",
      "|    episodes          | 724      |\n",
      "|    fps               | 596      |\n",
      "|    time_elapsed      | 24420    |\n",
      "|    total_timesteps   | 14578787 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00155  |\n",
      "|    n_updates         | 3632196  |\n",
      "-----------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -2279.9999999999955\n",
      "Num timesteps: 14600000\n",
      "Best mean reward: -inf - Last mean reward per episode: 828.80\n",
      "No more requests.\n",
      "Total reward for this episode is -8839.999999999956\n",
      "No more requests.\n",
      "Total reward for this episode is 5520.000000000033\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -19779.999999999985\n",
      "-----------------------------------\n",
      "| average_route_length | 5.25     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.6      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.94e+04 |\n",
      "|    ep_rew_mean       | 781      |\n",
      "|    exploration_rate  | 0.0732   |\n",
      "| time/                |          |\n",
      "|    episodes          | 728      |\n",
      "|    fps               | 596      |\n",
      "|    time_elapsed      | 24534    |\n",
      "|    total_timesteps   | 14633184 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000745 |\n",
      "|    n_updates         | 3645795  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 18839.999999999956\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is 3800.000000000029\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is 5640.000000000032\n",
      "No more requests.\n",
      "Total reward for this episode is 19519.99999999994\n",
      "-----------------------------------\n",
      "| average_route_length | 3.79     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.94e+04 |\n",
      "|    ep_rew_mean       | 1.7e+03  |\n",
      "|    exploration_rate  | 0.0681   |\n",
      "| time/                |          |\n",
      "|    episodes          | 732      |\n",
      "|    fps               | 595      |\n",
      "|    time_elapsed      | 24691    |\n",
      "|    total_timesteps   | 14713817 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00579  |\n",
      "|    n_updates         | 3665954  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 16739.999999999935\n",
      "No more requests.\n",
      "Total reward for this episode is -8039.999999999972\n",
      "No more requests.\n",
      "Total reward for this episode is 4080.0000000000337\n",
      "No more requests.\n",
      "Total reward for this episode is -6559.999999999964\n",
      "-----------------------------------\n",
      "| average_route_length | 4.24     |\n",
      "| blocked_contiguous   | 0.15     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.15     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.94e+04 |\n",
      "|    ep_rew_mean       | 2.03e+03 |\n",
      "|    exploration_rate  | 0.063    |\n",
      "| time/                |          |\n",
      "|    episodes          | 736      |\n",
      "|    fps               | 595      |\n",
      "|    time_elapsed      | 24848    |\n",
      "|    total_timesteps   | 14795323 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00144  |\n",
      "|    n_updates         | 3686330  |\n",
      "-----------------------------------\n",
      "Num timesteps: 14800000\n",
      "Best mean reward: -inf - Last mean reward per episode: 2027.20\n",
      "No more requests.\n",
      "Total reward for this episode is 7399.999999999965\n",
      "No more requests.\n",
      "Total reward for this episode is 12880.000000000038\n",
      "No more requests.\n",
      "Total reward for this episode is 1960.000000000025\n",
      "No more requests.\n",
      "Total reward for this episode is 6299.999999999977\n",
      "-----------------------------------\n",
      "| average_route_length | 3.89     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.93e+04 |\n",
      "|    ep_rew_mean       | 1.9e+03  |\n",
      "|    exploration_rate  | 0.0578   |\n",
      "| time/                |          |\n",
      "|    episodes          | 740      |\n",
      "|    fps               | 594      |\n",
      "|    time_elapsed      | 25004    |\n",
      "|    total_timesteps   | 14876514 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00084  |\n",
      "|    n_updates         | 3706628  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 31240.000000000007\n",
      "No more requests.\n",
      "Total reward for this episode is 21819.999999999967\n",
      "No more requests.\n",
      "Total reward for this episode is 7300.0000000000255\n",
      "No more requests.\n",
      "Total reward for this episode is 9119.999999999942\n",
      "-----------------------------------\n",
      "| average_route_length | 3.78     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.94e+04 |\n",
      "|    ep_rew_mean       | 2.65e+03 |\n",
      "|    exploration_rate  | 0.0527   |\n",
      "| time/                |          |\n",
      "|    episodes          | 744      |\n",
      "|    fps               | 594      |\n",
      "|    time_elapsed      | 25161    |\n",
      "|    total_timesteps   | 14958028 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00366  |\n",
      "|    n_updates         | 3727006  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -7839.999999999986\n",
      "No more requests.\n",
      "Total reward for this episode is 6320.000000000033\n",
      "Num timesteps: 15000000\n",
      "Best mean reward: -inf - Last mean reward per episode: 2763.40\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 8460.000000000005\n",
      "No more requests.\n",
      "Total reward for this episode is 22119.999999999967\n",
      "-----------------------------------\n",
      "| average_route_length | 3.58     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.94e+04 |\n",
      "|    ep_rew_mean       | 3.01e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 748      |\n",
      "|    fps               | 594      |\n",
      "|    time_elapsed      | 25299    |\n",
      "|    total_timesteps   | 15029090 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000429 |\n",
      "|    n_updates         | 3744772  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 11019.999999999982\n",
      "No more requests.\n",
      "Total reward for this episode is -6299.9999999999845\n",
      "No more requests.\n",
      "Total reward for this episode is 20199.999999999964\n",
      "No more requests.\n",
      "Total reward for this episode is 2440.0000000000255\n",
      "-----------------------------------\n",
      "| average_route_length | 4.44     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.94e+04 |\n",
      "|    ep_rew_mean       | 3.12e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 752      |\n",
      "|    fps               | 593      |\n",
      "|    time_elapsed      | 25456    |\n",
      "|    total_timesteps   | 15109938 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000668 |\n",
      "|    n_updates         | 3764984  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 6440.000000000024\n",
      "No more requests.\n",
      "Total reward for this episode is 6739.999999999984\n",
      "No more requests.\n",
      "Total reward for this episode is 18579.999999999975\n",
      "No more requests.\n",
      "Total reward for this episode is 6060.000000000028\n",
      "-----------------------------------\n",
      "| average_route_length | 4.11     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.94e+04 |\n",
      "|    ep_rew_mean       | 3.45e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 756      |\n",
      "|    fps               | 593      |\n",
      "|    time_elapsed      | 25614    |\n",
      "|    total_timesteps   | 15192035 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0124   |\n",
      "|    n_updates         | 3785508  |\n",
      "-----------------------------------\n",
      "Num timesteps: 15200000\n",
      "Best mean reward: -inf - Last mean reward per episode: 3450.80\n",
      "No more requests.\n",
      "Total reward for this episode is 5800.000000000023\n",
      "No more requests.\n",
      "Total reward for this episode is 22519.99999999995\n",
      "No more requests.\n",
      "Total reward for this episode is -6439.999999999972\n",
      "No more requests.\n",
      "Total reward for this episode is -23719.999999999975\n",
      "-----------------------------------\n",
      "| average_route_length | 5.12     |\n",
      "| blocked_contiguous   | 0.2      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.2      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.93e+04 |\n",
      "|    ep_rew_mean       | 3.53e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 760      |\n",
      "|    fps               | 592      |\n",
      "|    time_elapsed      | 25770    |\n",
      "|    total_timesteps   | 15272407 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0175   |\n",
      "|    n_updates         | 3805601  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 7000.0000000000255\n",
      "No more requests.\n",
      "Total reward for this episode is -19379.999999999975\n",
      "No more requests.\n",
      "Total reward for this episode is 33040.0\n",
      "No more requests.\n",
      "Total reward for this episode is 9059.999999999967\n",
      "-----------------------------------\n",
      "| average_route_length | 3.78     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.93e+04 |\n",
      "|    ep_rew_mean       | 4.31e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 764      |\n",
      "|    fps               | 592      |\n",
      "|    time_elapsed      | 25926    |\n",
      "|    total_timesteps   | 15353323 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00552  |\n",
      "|    n_updates         | 3825830  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 19639.999999999953\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 8100.000000000008\n",
      "Num timesteps: 15400000\n",
      "Best mean reward: -inf - Last mean reward per episode: 4678.00\n",
      "No more requests.\n",
      "Total reward for this episode is 18759.999999999975\n",
      "No more requests.\n",
      "Total reward for this episode is 20680.0\n",
      "-----------------------------------\n",
      "| average_route_length | 3.68     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.93e+04 |\n",
      "|    ep_rew_mean       | 5.26e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 768      |\n",
      "|    fps               | 591      |\n",
      "|    time_elapsed      | 26066    |\n",
      "|    total_timesteps   | 15424282 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 4.71     |\n",
      "|    n_updates         | 3843570  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 3500.0000000000277\n",
      "No more requests.\n",
      "Total reward for this episode is 2200.0000000000236\n",
      "No more requests.\n",
      "Total reward for this episode is -8259.999999999967\n",
      "No more requests.\n",
      "Total reward for this episode is 1880.0000000000346\n",
      "-----------------------------------\n",
      "| average_route_length | 4.44     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.95e+04 |\n",
      "|    ep_rew_mean       | 5.03e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 772      |\n",
      "|    fps               | 591      |\n",
      "|    time_elapsed      | 26222    |\n",
      "|    total_timesteps   | 15504783 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.125    |\n",
      "|    n_updates         | 3863695  |\n",
      "-----------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 2020.0000000000045\n",
      "No more requests.\n",
      "Total reward for this episode is -6799.999999999983\n",
      "No more requests.\n",
      "Total reward for this episode is 20079.99999999996\n",
      "No more requests.\n",
      "Total reward for this episode is 21759.999999999956\n",
      "-----------------------------------\n",
      "| average_route_length | 3.58     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.93e+04 |\n",
      "|    ep_rew_mean       | 5.06e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 776      |\n",
      "|    fps               | 590      |\n",
      "|    time_elapsed      | 26354    |\n",
      "|    total_timesteps   | 15572902 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00291  |\n",
      "|    n_updates         | 3880725  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 3860.000000000024\n",
      "Num timesteps: 15600000\n",
      "Best mean reward: -inf - Last mean reward per episode: 5242.60\n",
      "No more requests.\n",
      "Total reward for this episode is 4120.000000000027\n",
      "No more requests.\n",
      "Total reward for this episode is 21779.999999999956\n",
      "No more requests.\n",
      "Total reward for this episode is 19639.999999999996\n",
      "-----------------------------------\n",
      "| average_route_length | 3.58     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.94e+04 |\n",
      "|    ep_rew_mean       | 5.43e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 780      |\n",
      "|    fps               | 590      |\n",
      "|    time_elapsed      | 26511    |\n",
      "|    total_timesteps   | 15653273 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 9.23     |\n",
      "|    n_updates         | 3900818  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 7220.000000000022\n",
      "No more requests.\n",
      "Total reward for this episode is 35460.0\n",
      "No more requests.\n",
      "Total reward for this episode is -8599.99999999997\n",
      "No more requests.\n",
      "Total reward for this episode is 8560.000000000031\n",
      "-----------------------------------\n",
      "| average_route_length | 3.78     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.94e+04 |\n",
      "|    ep_rew_mean       | 5.39e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 784      |\n",
      "|    fps               | 590      |\n",
      "|    time_elapsed      | 26667    |\n",
      "|    total_timesteps   | 15734063 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000718 |\n",
      "|    n_updates         | 3921015  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 4380.000000000026\n",
      "No more requests.\n",
      "Total reward for this episode is 5300.0000000000255\n",
      "No more requests.\n",
      "Total reward for this episode is 8420.000000000022\n",
      "Num timesteps: 15800000\n",
      "Best mean reward: -inf - Last mean reward per episode: 5154.40\n",
      "No more requests.\n",
      "Total reward for this episode is -6659.999999999983\n",
      "-----------------------------------\n",
      "| average_route_length | 4.35     |\n",
      "| blocked_contiguous   | 0.15     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.15     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.94e+04 |\n",
      "|    ep_rew_mean       | 4.75e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 788      |\n",
      "|    fps               | 589      |\n",
      "|    time_elapsed      | 26823    |\n",
      "|    total_timesteps   | 15814479 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00478  |\n",
      "|    n_updates         | 3941119  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 9600.000000000022\n",
      "No more requests.\n",
      "Total reward for this episode is 8740.00000000003\n",
      "No more requests.\n",
      "Total reward for this episode is 8479.999999999967\n",
      "No more requests.\n",
      "Total reward for this episode is 2840.0000000000255\n",
      "-----------------------------------\n",
      "| average_route_length | 4.33     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.95e+04 |\n",
      "|    ep_rew_mean       | 4.94e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 792      |\n",
      "|    fps               | 589      |\n",
      "|    time_elapsed      | 26980    |\n",
      "|    total_timesteps   | 15895794 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 1.14     |\n",
      "|    n_updates         | 3961448  |\n",
      "-----------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -5099.999999999988\n",
      "No more requests.\n",
      "Total reward for this episode is -7499.999999999969\n",
      "No more requests.\n",
      "Total reward for this episode is 18620.00000000003\n",
      "No more requests.\n",
      "Total reward for this episode is 6120.000000000022\n",
      "-----------------------------------\n",
      "| average_route_length | 4.11     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.94e+04 |\n",
      "|    ep_rew_mean       | 4.59e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 796      |\n",
      "|    fps               | 588      |\n",
      "|    time_elapsed      | 27121    |\n",
      "|    total_timesteps   | 15968823 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00917  |\n",
      "|    n_updates         | 3979705  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -8319.99999999997\n",
      "Num timesteps: 16000000\n",
      "Best mean reward: -inf - Last mean reward per episode: 4491.20\n",
      "No more requests.\n",
      "Total reward for this episode is -7979.999999999981\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -9999.99999999997\n",
      "No more requests.\n",
      "Total reward for this episode is 17500.000000000015\n",
      "-----------------------------------\n",
      "| average_route_length | 4        |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.94e+04 |\n",
      "|    ep_rew_mean       | 4.57e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 800      |\n",
      "|    fps               | 588      |\n",
      "|    time_elapsed      | 27278    |\n",
      "|    total_timesteps   | 16049625 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00181  |\n",
      "|    n_updates         | 3999906  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 3580.000000000023\n",
      "No more requests.\n",
      "Total reward for this episode is -3039.9999999999723\n",
      "No more requests.\n",
      "Total reward for this episode is -9379.999999999978\n",
      "No more requests.\n",
      "Total reward for this episode is 8720.000000000022\n",
      "-----------------------------------\n",
      "| average_route_length | 3.89     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.94e+04 |\n",
      "|    ep_rew_mean       | 4.39e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 804      |\n",
      "|    fps               | 587      |\n",
      "|    time_elapsed      | 27434    |\n",
      "|    total_timesteps   | 16130499 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00237  |\n",
      "|    n_updates         | 4020124  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 34339.999999999985\n",
      "No more requests.\n",
      "Total reward for this episode is -5739.999999999982\n",
      "No more requests.\n",
      "Total reward for this episode is 21560.000000000015\n",
      "Num timesteps: 16200000\n",
      "Best mean reward: -inf - Last mean reward per episode: 5091.40\n",
      "No more requests.\n",
      "Total reward for this episode is 10639.999999999969\n",
      "-----------------------------------\n",
      "| average_route_length | 3.56     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.94e+04 |\n",
      "|    ep_rew_mean       | 5.29e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 808      |\n",
      "|    fps               | 587      |\n",
      "|    time_elapsed      | 27591    |\n",
      "|    total_timesteps   | 16211215 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 1.58     |\n",
      "|    n_updates         | 4040303  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 16180.000000000033\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -3639.999999999989\n",
      "No more requests.\n",
      "Total reward for this episode is -10119.999999999978\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -15360.000000000002\n",
      "-----------------------------------\n",
      "| average_route_length | 7        |\n",
      "| blocked_contiguous   | 0        |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.9      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.92e+04 |\n",
      "|    ep_rew_mean       | 5.03e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 812      |\n",
      "|    fps               | 587      |\n",
      "|    time_elapsed      | 27695    |\n",
      "|    total_timesteps   | 16264904 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00134  |\n",
      "|    n_updates         | 4053725  |\n",
      "-----------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 14079.999999999964\n",
      "No more requests.\n",
      "Total reward for this episode is 7500.000000000023\n",
      "No more requests.\n",
      "Total reward for this episode is -4739.9999999999845\n",
      "No more requests.\n",
      "Total reward for this episode is 4140.000000000021\n",
      "-----------------------------------\n",
      "| average_route_length | 4.22     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.93e+04 |\n",
      "|    ep_rew_mean       | 5.42e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 816      |\n",
      "|    fps               | 586      |\n",
      "|    time_elapsed      | 27848    |\n",
      "|    total_timesteps   | 16343876 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00318  |\n",
      "|    n_updates         | 4073468  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -10199.999999999967\n",
      "No more requests.\n",
      "Total reward for this episode is 8900.00000000002\n",
      "Num timesteps: 16400000\n",
      "Best mean reward: -inf - Last mean reward per episode: 5383.00\n",
      "No more requests.\n",
      "Total reward for this episode is 9240.000000000025\n",
      "No more requests.\n",
      "Total reward for this episode is 4920.000000000025\n",
      "-----------------------------------\n",
      "| average_route_length | 4.22     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.93e+04 |\n",
      "|    ep_rew_mean       | 5.33e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 820      |\n",
      "|    fps               | 586      |\n",
      "|    time_elapsed      | 28004    |\n",
      "|    total_timesteps   | 16424354 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00266  |\n",
      "|    n_updates         | 4093588  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 6280.000000000033\n",
      "No more requests.\n",
      "Total reward for this episode is 10179.999999999953\n",
      "No more requests.\n",
      "Total reward for this episode is 2640.000000000051\n",
      "No more requests.\n",
      "Total reward for this episode is 22879.999999999967\n",
      "-----------------------------------\n",
      "| average_route_length | 3.47     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.93e+04 |\n",
      "|    ep_rew_mean       | 5.68e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 824      |\n",
      "|    fps               | 586      |\n",
      "|    time_elapsed      | 28160    |\n",
      "|    total_timesteps   | 16505615 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000261 |\n",
      "|    n_updates         | 4113903  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -4279.999999999982\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 16160.000000000005\n",
      "No more requests.\n",
      "Total reward for this episode is 23259.99999999997\n",
      "No more requests.\n",
      "Total reward for this episode is -8899.999999999978\n",
      "-----------------------------------\n",
      "| average_route_length | 4.59     |\n",
      "| blocked_contiguous   | 0.15     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.15     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.95e+04 |\n",
      "|    ep_rew_mean       | 6.19e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 828      |\n",
      "|    fps               | 585      |\n",
      "|    time_elapsed      | 28307    |\n",
      "|    total_timesteps   | 16581338 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000537 |\n",
      "|    n_updates         | 4132834  |\n",
      "-----------------------------------\n",
      "Num timesteps: 16600000\n",
      "Best mean reward: -inf - Last mean reward per episode: 6192.80\n",
      "No more requests.\n",
      "Total reward for this episode is 3340.000000000028\n",
      "No more requests.\n",
      "Total reward for this episode is 9060.000000000027\n",
      "No more requests.\n",
      "Total reward for this episode is -4939.999999999972\n",
      "No more requests.\n",
      "Total reward for this episode is -9219.999999999976\n",
      "-----------------------------------\n",
      "| average_route_length | 4.71     |\n",
      "| blocked_contiguous   | 0.15     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.15     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.95e+04 |\n",
      "|    ep_rew_mean       | 5.7e+03  |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 832      |\n",
      "|    fps               | 585      |\n",
      "|    time_elapsed      | 28463    |\n",
      "|    total_timesteps   | 16662048 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00556  |\n",
      "|    n_updates         | 4153011  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 19879.999999999956\n",
      "No more requests.\n",
      "Total reward for this episode is 17359.999999999956\n",
      "No more requests.\n",
      "Total reward for this episode is -6519.999999999976\n",
      "No more requests.\n",
      "Total reward for this episode is 9419.999999999993\n",
      "-----------------------------------\n",
      "| average_route_length | 3.78     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.95e+04 |\n",
      "|    ep_rew_mean       | 6.03e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 836      |\n",
      "|    fps               | 585      |\n",
      "|    time_elapsed      | 28619    |\n",
      "|    total_timesteps   | 16743159 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0185   |\n",
      "|    n_updates         | 4173289  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -3799.9999999999745\n",
      "No more requests.\n",
      "Total reward for this episode is -5979.999999999981\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -3359.999999999988\n",
      "Num timesteps: 16800000\n",
      "Best mean reward: -inf - Last mean reward per episode: 5686.20\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -1419.9999999999818\n",
      "-----------------------------------\n",
      "| average_route_length | 4.36     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.45     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.93e+04 |\n",
      "|    ep_rew_mean       | 5.61e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 840      |\n",
      "|    fps               | 584      |\n",
      "|    time_elapsed      | 28748    |\n",
      "|    total_timesteps   | 16809546 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00531  |\n",
      "|    n_updates         | 4189886  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 5400.000000000035\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 8340.000000000005\n",
      "No more requests.\n",
      "Total reward for this episode is 6740.000000000024\n",
      "No more requests.\n",
      "Total reward for this episode is -5979.999999999977\n",
      "-----------------------------------\n",
      "| average_route_length | 4.35     |\n",
      "| blocked_contiguous   | 0.15     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.15     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.92e+04 |\n",
      "|    ep_rew_mean       | 5.07e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 844      |\n",
      "|    fps               | 584      |\n",
      "|    time_elapsed      | 28886    |\n",
      "|    total_timesteps   | 16881017 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00184  |\n",
      "|    n_updates         | 4207754  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 10419.999999999982\n",
      "No more requests.\n",
      "Total reward for this episode is 9420.000000000022\n",
      "No more requests.\n",
      "Total reward for this episode is 7760.000000000028\n",
      "No more requests.\n",
      "Total reward for this episode is -9579.999999999975\n",
      "-----------------------------------\n",
      "| average_route_length | 4.71     |\n",
      "| blocked_contiguous   | 0.15     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.15     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.93e+04 |\n",
      "|    ep_rew_mean       | 4.95e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 848      |\n",
      "|    fps               | 584      |\n",
      "|    time_elapsed      | 29043    |\n",
      "|    total_timesteps   | 16961993 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00783  |\n",
      "|    n_updates         | 4227998  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -6819.999999999979\n",
      "Stop hitting yourself\n",
      "Num timesteps: 17000000\n",
      "Best mean reward: -inf - Last mean reward per episode: 4776.00\n",
      "No more requests.\n",
      "Total reward for this episode is -7339.999999999998\n",
      "No more requests.\n",
      "Total reward for this episode is -9959.999999999973\n",
      "No more requests.\n",
      "Total reward for this episode is -10999.99999999997\n",
      "-----------------------------------\n",
      "| average_route_length | 4.71     |\n",
      "| blocked_contiguous   | 0.15     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.15     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.93e+04 |\n",
      "|    ep_rew_mean       | 4.33e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 852      |\n",
      "|    fps               | 583      |\n",
      "|    time_elapsed      | 29200    |\n",
      "|    total_timesteps   | 17043132 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 10.4     |\n",
      "|    n_updates         | 4248282  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 9960.00000000002\n",
      "No more requests.\n",
      "Total reward for this episode is -4939.999999999977\n",
      "No more requests.\n",
      "Total reward for this episode is 5340.000000000016\n",
      "No more requests.\n",
      "Total reward for this episode is 7219.9999999999845\n",
      "-----------------------------------\n",
      "| average_route_length | 4        |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.93e+04 |\n",
      "|    ep_rew_mean       | 4.12e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 856      |\n",
      "|    fps               | 583      |\n",
      "|    time_elapsed      | 29356    |\n",
      "|    total_timesteps   | 17124096 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00806  |\n",
      "|    n_updates         | 4268523  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 7040.000000000019\n",
      "No more requests.\n",
      "Total reward for this episode is 8160.000000000018\n",
      "No more requests.\n",
      "Total reward for this episode is 4800.0000000000255\n",
      "Num timesteps: 17200000\n",
      "Best mean reward: -inf - Last mean reward per episode: 4104.40\n",
      "No more requests.\n",
      "Total reward for this episode is 9120.000000000024\n",
      "-----------------------------------\n",
      "| average_route_length | 3.78     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.93e+04 |\n",
      "|    ep_rew_mean       | 4.43e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 860      |\n",
      "|    fps               | 582      |\n",
      "|    time_elapsed      | 29513    |\n",
      "|    total_timesteps   | 17205049 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000892 |\n",
      "|    n_updates         | 4288762  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 7120.000000000033\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is 4420.000000000025\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 9400.000000000005\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 10400.000000000005\n",
      "-----------------------------------\n",
      "| average_route_length | 3.4      |\n",
      "| blocked_contiguous   | 0        |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.5      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.91e+04 |\n",
      "|    ep_rew_mean       | 4.46e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 864      |\n",
      "|    fps               | 582      |\n",
      "|    time_elapsed      | 29634    |\n",
      "|    total_timesteps   | 17266636 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00124  |\n",
      "|    n_updates         | 4304158  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 22379.99999999997\n",
      "No more requests.\n",
      "Total reward for this episode is -20159.999999999964\n",
      "No more requests.\n",
      "Total reward for this episode is -10079.999999999973\n",
      "No more requests.\n",
      "Total reward for this episode is 19240.00000000001\n",
      "-----------------------------------\n",
      "| average_route_length | 3.89     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.92e+04 |\n",
      "|    ep_rew_mean       | 3.89e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 868      |\n",
      "|    fps               | 582      |\n",
      "|    time_elapsed      | 29790    |\n",
      "|    total_timesteps   | 17347350 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.224    |\n",
      "|    n_updates         | 4324337  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 19819.99999999996\n",
      "No more requests.\n",
      "Total reward for this episode is -6799.999999999977\n",
      "Num timesteps: 17400000\n",
      "Best mean reward: -inf - Last mean reward per episode: 3967.80\n",
      "No more requests.\n",
      "Total reward for this episode is -21839.99999999996\n",
      "No more requests.\n",
      "Total reward for this episode is -20499.999999999967\n",
      "-----------------------------------\n",
      "| average_route_length | 4.75     |\n",
      "| blocked_contiguous   | 0.2      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.2      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.92e+04 |\n",
      "|    ep_rew_mean       | 3.61e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 872      |\n",
      "|    fps               | 581      |\n",
      "|    time_elapsed      | 29947    |\n",
      "|    total_timesteps   | 17428379 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000767 |\n",
      "|    n_updates         | 4344594  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 15920.000000000027\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -7199.999999999989\n",
      "No more requests.\n",
      "Total reward for this episode is 22120.000000000015\n",
      "No more requests.\n",
      "Total reward for this episode is 21280.00000000002\n",
      "-----------------------------------\n",
      "| average_route_length | 3.58     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.94e+04 |\n",
      "|    ep_rew_mean       | 3.75e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 876      |\n",
      "|    fps               | 581      |\n",
      "|    time_elapsed      | 30103    |\n",
      "|    total_timesteps   | 17509262 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0176   |\n",
      "|    n_updates         | 4364815  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -5759.999999999968\n",
      "No more requests.\n",
      "Total reward for this episode is 6419.999999999999\n",
      "No more requests.\n",
      "Total reward for this episode is 3740.0000000000177\n",
      "No more requests.\n",
      "Total reward for this episode is -9639.999999999976\n",
      "-----------------------------------\n",
      "| average_route_length | 4.71     |\n",
      "| blocked_contiguous   | 0.15     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.15     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.94e+04 |\n",
      "|    ep_rew_mean       | 3.21e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 880      |\n",
      "|    fps               | 581      |\n",
      "|    time_elapsed      | 30260    |\n",
      "|    total_timesteps   | 17590902 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.014    |\n",
      "|    n_updates         | 4385225  |\n",
      "-----------------------------------\n",
      "Num timesteps: 17600000\n",
      "Best mean reward: -inf - Last mean reward per episode: 3206.80\n",
      "No more requests.\n",
      "Total reward for this episode is -5399.999999999976\n",
      "No more requests.\n",
      "Total reward for this episode is 4440.000000000027\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -8459.999999999998\n",
      "No more requests.\n",
      "Total reward for this episode is 6500.000000000028\n",
      "-----------------------------------\n",
      "| average_route_length | 4        |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.94e+04 |\n",
      "|    ep_rew_mean       | 2.76e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 884      |\n",
      "|    fps               | 580      |\n",
      "|    time_elapsed      | 30415    |\n",
      "|    total_timesteps   | 17670931 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0087   |\n",
      "|    n_updates         | 4405232  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 7000.000000000028\n",
      "No more requests.\n",
      "Total reward for this episode is 17720.00000000003\n",
      "No more requests.\n",
      "Total reward for this episode is 5120.000000000005\n",
      "No more requests.\n",
      "Total reward for this episode is 6440.00000000002\n",
      "-----------------------------------\n",
      "| average_route_length | 4        |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.94e+04 |\n",
      "|    ep_rew_mean       | 3.01e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 888      |\n",
      "|    fps               | 580      |\n",
      "|    time_elapsed      | 30572    |\n",
      "|    total_timesteps   | 17752453 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000879 |\n",
      "|    n_updates         | 4425613  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 18960.000000000022\n",
      "No more requests.\n",
      "Total reward for this episode is 7060.000000000028\n",
      "Num timesteps: 17800000\n",
      "Best mean reward: -inf - Last mean reward per episode: 3084.00\n",
      "No more requests.\n",
      "Total reward for this episode is 5740.000000000029\n",
      "No more requests.\n",
      "Total reward for this episode is 5240.000000000026\n",
      "-----------------------------------\n",
      "| average_route_length | 4.22     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.94e+04 |\n",
      "|    ep_rew_mean       | 3.08e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 892      |\n",
      "|    fps               | 580      |\n",
      "|    time_elapsed      | 30730    |\n",
      "|    total_timesteps   | 17833924 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00721  |\n",
      "|    n_updates         | 4445980  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 3980.000000000023\n",
      "No more requests.\n",
      "Total reward for this episode is 17180.00000000001\n",
      "No more requests.\n",
      "Total reward for this episode is 7780.000000000021\n",
      "No more requests.\n",
      "Total reward for this episode is -9779.999999999969\n",
      "-----------------------------------\n",
      "| average_route_length | 4.59     |\n",
      "| blocked_contiguous   | 0.15     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.15     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.95e+04 |\n",
      "|    ep_rew_mean       | 3.15e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 896      |\n",
      "|    fps               | 580      |\n",
      "|    time_elapsed      | 30887    |\n",
      "|    total_timesteps   | 17915552 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 27.8     |\n",
      "|    n_updates         | 4466387  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -20099.999999999967\n",
      "No more requests.\n",
      "Total reward for this episode is -17779.999999999985\n",
      "No more requests.\n",
      "Total reward for this episode is 5180.000000000026\n",
      "No more requests.\n",
      "Total reward for this episode is 21939.999999999967\n",
      "-----------------------------------\n",
      "| average_route_length | 3.58     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.95e+04 |\n",
      "|    ep_rew_mean       | 3.13e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 900      |\n",
      "|    fps               | 579      |\n",
      "|    time_elapsed      | 31043    |\n",
      "|    total_timesteps   | 17996512 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000728 |\n",
      "|    n_updates         | 4486627  |\n",
      "-----------------------------------\n",
      "Num timesteps: 18000000\n",
      "Best mean reward: -inf - Last mean reward per episode: 3125.60\n",
      "No more requests.\n",
      "Total reward for this episode is -5199.99999999998\n",
      "No more requests.\n",
      "Total reward for this episode is -3979.9999999999764\n",
      "No more requests.\n",
      "Total reward for this episode is -7639.999999999979\n",
      "No more requests.\n",
      "Total reward for this episode is 3400.000000000029\n",
      "-----------------------------------\n",
      "| average_route_length | 4.22     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.95e+04 |\n",
      "|    ep_rew_mean       | 2.99e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 904      |\n",
      "|    fps               | 579      |\n",
      "|    time_elapsed      | 31200    |\n",
      "|    total_timesteps   | 18077832 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 4.1      |\n",
      "|    n_updates         | 4506957  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 6120.000000000023\n",
      "No more requests.\n",
      "Total reward for this episode is 8420.000000000022\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 8340.000000000005\n",
      "No more requests.\n",
      "Total reward for this episode is -1479.9999999999193\n",
      "-----------------------------------\n",
      "| average_route_length | 4        |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.94e+04 |\n",
      "|    ep_rew_mean       | 2.6e+03  |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 908      |\n",
      "|    fps               | 579      |\n",
      "|    time_elapsed      | 31339    |\n",
      "|    total_timesteps   | 18150038 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0036   |\n",
      "|    n_updates         | 4525009  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -29779.99999999996\n",
      "No more requests.\n",
      "Total reward for this episode is 19999.99999999997\n",
      "Num timesteps: 18200000\n",
      "Best mean reward: -inf - Last mean reward per episode: 2375.40\n",
      "No more requests.\n",
      "Total reward for this episode is 4380.000000000032\n",
      "No more requests.\n",
      "Total reward for this episode is -7099.999999999982\n",
      "-----------------------------------\n",
      "| average_route_length | 4.35     |\n",
      "| blocked_contiguous   | 0.15     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.15     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.97e+04 |\n",
      "|    ep_rew_mean       | 2.6e+03  |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 912      |\n",
      "|    fps               | 578      |\n",
      "|    time_elapsed      | 31495    |\n",
      "|    total_timesteps   | 18231118 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00745  |\n",
      "|    n_updates         | 4545279  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -7959.999999999964\n",
      "No more requests.\n",
      "Total reward for this episode is 16559.99999999998\n",
      "No more requests.\n",
      "Total reward for this episode is 6120.000000000033\n",
      "No more requests.\n",
      "Total reward for this episode is 23079.999999999956\n",
      "-----------------------------------\n",
      "| average_route_length | 3.37     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.97e+04 |\n",
      "|    ep_rew_mean       | 2.76e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 916      |\n",
      "|    fps               | 578      |\n",
      "|    time_elapsed      | 31653    |\n",
      "|    total_timesteps   | 18313162 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 2.88     |\n",
      "|    n_updates         | 4565790  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -22019.999999999964\n",
      "No more requests.\n",
      "Total reward for this episode is 7240.000000000015\n",
      "No more requests.\n",
      "Total reward for this episode is 6880.000000000029\n",
      "No more requests.\n",
      "Total reward for this episode is -7339.999999999968\n",
      "-----------------------------------\n",
      "| average_route_length | 4.24     |\n",
      "| blocked_contiguous   | 0.15     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.15     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.97e+04 |\n",
      "|    ep_rew_mean       | 2.48e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 920      |\n",
      "|    fps               | 578      |\n",
      "|    time_elapsed      | 31811    |\n",
      "|    total_timesteps   | 18395232 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 3.99     |\n",
      "|    n_updates         | 4586307  |\n",
      "-----------------------------------\n",
      "Num timesteps: 18400000\n",
      "Best mean reward: -inf - Last mean reward per episode: 2483.00\n",
      "No more requests.\n",
      "Total reward for this episode is -4799.999999999994\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is 19700.000000000015\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 14780.000000000007\n",
      "No more requests.\n",
      "Total reward for this episode is 1600.0000000000164\n",
      "-----------------------------------\n",
      "| average_route_length | 4.33     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.96e+04 |\n",
      "|    ep_rew_mean       | 2.38e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 924      |\n",
      "|    fps               | 577      |\n",
      "|    time_elapsed      | 31955    |\n",
      "|    total_timesteps   | 18469107 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0107   |\n",
      "|    n_updates         | 4604776  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -7099.999999999983\n",
      "No more requests.\n",
      "Total reward for this episode is 10939.999999999969\n",
      "No more requests.\n",
      "Total reward for this episode is 8000.000000000028\n",
      "No more requests.\n",
      "Total reward for this episode is 19519.99999999997\n",
      "-----------------------------------\n",
      "| average_route_length | 3.68     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.97e+04 |\n",
      "|    ep_rew_mean       | 2.43e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 928      |\n",
      "|    fps               | 577      |\n",
      "|    time_elapsed      | 32112    |\n",
      "|    total_timesteps   | 18550931 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00143  |\n",
      "|    n_updates         | 4625232  |\n",
      "-----------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -7599.999999999972\n",
      "No more requests.\n",
      "Total reward for this episode is 20579.999999999953\n",
      "Num timesteps: 18600000\n",
      "Best mean reward: -inf - Last mean reward per episode: 2438.60\n",
      "No more requests.\n",
      "Total reward for this episode is 4500.000000000031\n",
      "No more requests.\n",
      "Total reward for this episode is 20380.000000000015\n",
      "-----------------------------------\n",
      "| average_route_length | 3.68     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.96e+04 |\n",
      "|    ep_rew_mean       | 2.83e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 932      |\n",
      "|    fps               | 577      |\n",
      "|    time_elapsed      | 32258    |\n",
      "|    total_timesteps   | 18626601 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.98     |\n",
      "|    n_updates         | 4644150  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 6740.000000000019\n",
      "No more requests.\n",
      "Total reward for this episode is 6000.0000000000255\n",
      "No more requests.\n",
      "Total reward for this episode is 21999.999999999996\n",
      "No more requests.\n",
      "Total reward for this episode is -6979.999999999981\n",
      "-----------------------------------\n",
      "| average_route_length | 4.47     |\n",
      "| blocked_contiguous   | 0.15     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.15     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.96e+04 |\n",
      "|    ep_rew_mean       | 2.71e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 936      |\n",
      "|    fps               | 577      |\n",
      "|    time_elapsed      | 32415    |\n",
      "|    total_timesteps   | 18708142 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00163  |\n",
      "|    n_updates         | 4664535  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -5159.999999999983\n",
      "No more requests.\n",
      "Total reward for this episode is 21559.999999999975\n",
      "No more requests.\n",
      "Total reward for this episode is 9120.000000000027\n",
      "No more requests.\n",
      "Total reward for this episode is -8939.999999999964\n",
      "-----------------------------------\n",
      "| average_route_length | 4.47     |\n",
      "| blocked_contiguous   | 0.15     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.15     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.98e+04 |\n",
      "|    ep_rew_mean       | 3.01e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 940      |\n",
      "|    fps               | 576      |\n",
      "|    time_elapsed      | 32572    |\n",
      "|    total_timesteps   | 18789215 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00145  |\n",
      "|    n_updates         | 4684803  |\n",
      "-----------------------------------\n",
      "Num timesteps: 18800000\n",
      "Best mean reward: -inf - Last mean reward per episode: 3005.40\n",
      "No more requests.\n",
      "Total reward for this episode is 14460.000000000025\n",
      "No more requests.\n",
      "Total reward for this episode is 5320.00000000003\n",
      "No more requests.\n",
      "Total reward for this episode is -9059.999999999967\n",
      "No more requests.\n",
      "Total reward for this episode is 4600.00000000002\n",
      "-----------------------------------\n",
      "| average_route_length | 4.22     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.99e+04 |\n",
      "|    ep_rew_mean       | 3.01e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 944      |\n",
      "|    fps               | 576      |\n",
      "|    time_elapsed      | 32730    |\n",
      "|    total_timesteps   | 18870767 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.683    |\n",
      "|    n_updates         | 4705191  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -6159.999999999988\n",
      "No more requests.\n",
      "Total reward for this episode is 19199.999999999964\n",
      "No more requests.\n",
      "Total reward for this episode is 5000.0000000000255\n",
      "No more requests.\n",
      "Total reward for this episode is 5300.0000000000255\n",
      "-----------------------------------\n",
      "| average_route_length | 4.22     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.99e+04 |\n",
      "|    ep_rew_mean       | 3.06e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 948      |\n",
      "|    fps               | 576      |\n",
      "|    time_elapsed      | 32886    |\n",
      "|    total_timesteps   | 18952414 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0115   |\n",
      "|    n_updates         | 4725603  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 7500.00000000002\n",
      "No more requests.\n",
      "Total reward for this episode is -20239.999999999967\n",
      "Num timesteps: 19000000\n",
      "Best mean reward: -inf - Last mean reward per episode: 3079.40\n",
      "No more requests.\n",
      "Total reward for this episode is 6520.00000000003\n",
      "No more requests.\n",
      "Total reward for this episode is -6959.999999999973\n",
      "-----------------------------------\n",
      "| average_route_length | 4.35     |\n",
      "| blocked_contiguous   | 0.15     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.15     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.99e+04 |\n",
      "|    ep_rew_mean       | 3.28e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 952      |\n",
      "|    fps               | 576      |\n",
      "|    time_elapsed      | 33044    |\n",
      "|    total_timesteps   | 19033564 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.056    |\n",
      "|    n_updates         | 4745890  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 6100.000000000017\n",
      "No more requests.\n",
      "Total reward for this episode is 19339.999999999978\n",
      "No more requests.\n",
      "Total reward for this episode is -11599.99999999997\n",
      "No more requests.\n",
      "Total reward for this episode is -5299.999999999979\n",
      "-----------------------------------\n",
      "| average_route_length | 4.24     |\n",
      "| blocked_contiguous   | 0.15     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.15     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.99e+04 |\n",
      "|    ep_rew_mean       | 3.2e+03  |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 956      |\n",
      "|    fps               | 575      |\n",
      "|    time_elapsed      | 33201    |\n",
      "|    total_timesteps   | 19115283 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00148  |\n",
      "|    n_updates         | 4766320  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -18499.999999999967\n",
      "No more requests.\n",
      "Total reward for this episode is -5399.999999999982\n",
      "No more requests.\n",
      "Total reward for this episode is 7220.000000000033\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is 2220.0000000000387\n",
      "-----------------------------------\n",
      "| average_route_length | 4.33     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.99e+04 |\n",
      "|    ep_rew_mean       | 2.76e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 960      |\n",
      "|    fps               | 575      |\n",
      "|    time_elapsed      | 33358    |\n",
      "|    total_timesteps   | 19196316 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0554   |\n",
      "|    n_updates         | 4786578  |\n",
      "-----------------------------------\n",
      "Num timesteps: 19200000\n",
      "Best mean reward: -inf - Last mean reward per episode: 2760.40\n",
      "No more requests.\n",
      "Total reward for this episode is -10659.999999999938\n",
      "No more requests.\n",
      "Total reward for this episode is 6160.000000000022\n",
      "No more requests.\n",
      "Total reward for this episode is 4080.000000000031\n",
      "No more requests.\n",
      "Total reward for this episode is 6180.000000000024\n",
      "-----------------------------------\n",
      "| average_route_length | 4.11     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.01e+04 |\n",
      "|    ep_rew_mean       | 2.49e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 964      |\n",
      "|    fps               | 575      |\n",
      "|    time_elapsed      | 33514    |\n",
      "|    total_timesteps   | 19276992 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00196  |\n",
      "|    n_updates         | 4806747  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -18159.999999999967\n",
      "No more requests.\n",
      "Total reward for this episode is -8079.999999999967\n",
      "No more requests.\n",
      "Total reward for this episode is -4739.999999999983\n",
      "No more requests.\n",
      "Total reward for this episode is 20939.999999999978\n",
      "-----------------------------------\n",
      "| average_route_length | 3.58     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.01e+04 |\n",
      "|    ep_rew_mean       | 2.28e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 968      |\n",
      "|    fps               | 574      |\n",
      "|    time_elapsed      | 33671    |\n",
      "|    total_timesteps   | 19358610 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000776 |\n",
      "|    n_updates         | 4827152  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 7920.000000000027\n",
      "No more requests.\n",
      "Total reward for this episode is 17920.00000000003\n",
      "Num timesteps: 19400000\n",
      "Best mean reward: -inf - Last mean reward per episode: 2409.40\n",
      "No more requests.\n",
      "Total reward for this episode is 17620.000000000022\n",
      "No more requests.\n",
      "Total reward for this episode is 17939.99999999996\n",
      "-----------------------------------\n",
      "| average_route_length | 4        |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.01e+04 |\n",
      "|    ep_rew_mean       | 3.19e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 972      |\n",
      "|    fps               | 574      |\n",
      "|    time_elapsed      | 33828    |\n",
      "|    total_timesteps   | 19439315 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0191   |\n",
      "|    n_updates         | 4847328  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 6800.0000000000255\n",
      "No more requests.\n",
      "Total reward for this episode is 5020.000000000033\n",
      "No more requests.\n",
      "Total reward for this episode is 6480.000000000018\n",
      "No more requests.\n",
      "Total reward for this episode is -4719.9999999999845\n",
      "-----------------------------------\n",
      "| average_route_length | 4.12     |\n",
      "| blocked_contiguous   | 0.15     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.15     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.01e+04 |\n",
      "|    ep_rew_mean       | 2.8e+03  |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 976      |\n",
      "|    fps               | 574      |\n",
      "|    time_elapsed      | 33985    |\n",
      "|    total_timesteps   | 19520654 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 54.8     |\n",
      "|    n_updates         | 4867663  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 4540.000000000029\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 16920.00000000001\n",
      "No more requests.\n",
      "Total reward for this episode is 4599.999999999963\n",
      "No more requests.\n",
      "Total reward for this episode is 4160.000000000031\n",
      "-----------------------------------\n",
      "| average_route_length | 4.11     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.01e+04 |\n",
      "|    ep_rew_mean       | 3.16e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 980      |\n",
      "|    fps               | 574      |\n",
      "|    time_elapsed      | 34131    |\n",
      "|    total_timesteps   | 19596725 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00205  |\n",
      "|    n_updates         | 4886681  |\n",
      "-----------------------------------\n",
      "Num timesteps: 19600000\n",
      "Best mean reward: -inf - Last mean reward per episode: 3163.20\n",
      "No more requests.\n",
      "Total reward for this episode is 31860.000000000004\n",
      "No more requests.\n",
      "Total reward for this episode is 17819.99999999997\n",
      "No more requests.\n",
      "Total reward for this episode is -6799.999999999985\n",
      "No more requests.\n",
      "Total reward for this episode is 8860.000000000027\n",
      "-----------------------------------\n",
      "| average_route_length | 3.78     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.01e+04 |\n",
      "|    ep_rew_mean       | 3.7e+03  |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 984      |\n",
      "|    fps               | 573      |\n",
      "|    time_elapsed      | 34288    |\n",
      "|    total_timesteps   | 19677518 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00142  |\n",
      "|    n_updates         | 4906879  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 8759.999999999962\n",
      "No more requests.\n",
      "Total reward for this episode is 6939.999999999986\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 3300.0000000000086\n",
      "No more requests.\n",
      "Total reward for this episode is 21519.999999999956\n",
      "-----------------------------------\n",
      "| average_route_length | 3.58     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2e+04    |\n",
      "|    ep_rew_mean       | 3.75e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 988      |\n",
      "|    fps               | 573      |\n",
      "|    time_elapsed      | 34442    |\n",
      "|    total_timesteps   | 19757334 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000675 |\n",
      "|    n_updates         | 4926833  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 9500.000000000131\n",
      "No more requests.\n",
      "Total reward for this episode is 19159.999999999967\n",
      "Num timesteps: 19800000\n",
      "Best mean reward: -inf - Last mean reward per episode: 3776.60\n",
      "No more requests.\n",
      "Total reward for this episode is 5099.9999999999945\n",
      "No more requests.\n",
      "Total reward for this episode is -4599.999999999979\n",
      "-----------------------------------\n",
      "| average_route_length | 4.12     |\n",
      "| blocked_contiguous   | 0.15     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.15     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2e+04    |\n",
      "|    ep_rew_mean       | 3.67e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 992      |\n",
      "|    fps               | 573      |\n",
      "|    time_elapsed      | 34600    |\n",
      "|    total_timesteps   | 19838823 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0017   |\n",
      "|    n_updates         | 4947205  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 35520.0\n",
      "No more requests.\n",
      "Total reward for this episode is 22459.99999999995\n",
      "No more requests.\n",
      "Total reward for this episode is 20760.00000000002\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -999.9999999999972\n",
      "-----------------------------------\n",
      "| average_route_length | 4        |\n",
      "| blocked_contiguous   | 0        |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.65     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.99e+04 |\n",
      "|    ep_rew_mean       | 4.26e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 996      |\n",
      "|    fps               | 573      |\n",
      "|    time_elapsed      | 34733    |\n",
      "|    total_timesteps   | 19907513 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000463 |\n",
      "|    n_updates         | 4964378  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 5940.000000000029\n",
      "No more requests.\n",
      "Total reward for this episode is 22939.99999999996\n",
      "No more requests.\n",
      "Total reward for this episode is 5800.0000000000055\n",
      "No more requests.\n",
      "Total reward for this episode is 17960.00000000001\n",
      "-----------------------------------\n",
      "| average_route_length | 3.89     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.99e+04 |\n",
      "|    ep_rew_mean       | 4.9e+03  |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1000     |\n",
      "|    fps               | 572      |\n",
      "|    time_elapsed      | 34889    |\n",
      "|    total_timesteps   | 19988217 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000342 |\n",
      "|    n_updates         | 4984554  |\n",
      "-----------------------------------\n",
      "Num timesteps: 20000000\n",
      "Best mean reward: -inf - Last mean reward per episode: 4897.20\n",
      "No more requests.\n",
      "Total reward for this episode is 19279.99999999998\n",
      "No more requests.\n",
      "Total reward for this episode is -9399.999999999967\n",
      "No more requests.\n",
      "Total reward for this episode is 32680.000000000007\n",
      "No more requests.\n",
      "Total reward for this episode is 8180.000000000026\n",
      "-----------------------------------\n",
      "| average_route_length | 3.89     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.99e+04 |\n",
      "|    ep_rew_mean       | 5.54e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1004     |\n",
      "|    fps               | 572      |\n",
      "|    time_elapsed      | 35046    |\n",
      "|    total_timesteps   | 20069006 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000399 |\n",
      "|    n_updates         | 5004751  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 8880.00000000003\n",
      "No more requests.\n",
      "Total reward for this episode is 7200.0\n",
      "No more requests.\n",
      "Total reward for this episode is 22000.000000000015\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is 5339.999999999978\n",
      "-----------------------------------\n",
      "| average_route_length | 4.22     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2e+04    |\n",
      "|    ep_rew_mean       | 5.86e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1008     |\n",
      "|    fps               | 572      |\n",
      "|    time_elapsed      | 35205    |\n",
      "|    total_timesteps   | 20150877 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00185  |\n",
      "|    n_updates         | 5025219  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -6119.999999999976\n",
      "No more requests.\n",
      "Total reward for this episode is -8819.999999999975\n",
      "Num timesteps: 20200000\n",
      "Best mean reward: -inf - Last mean reward per episode: 5806.80\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 3280.000000000018\n",
      "No more requests.\n",
      "Total reward for this episode is 14660.000000000025\n",
      "-----------------------------------\n",
      "| average_route_length | 4.21     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.99e+04 |\n",
      "|    ep_rew_mean       | 6.02e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1012     |\n",
      "|    fps               | 572      |\n",
      "|    time_elapsed      | 35352    |\n",
      "|    total_timesteps   | 20225997 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0724   |\n",
      "|    n_updates         | 5043999  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 2980.000000000018\n",
      "No more requests.\n",
      "Total reward for this episode is 20059.999999999964\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is 6420.000000000025\n",
      "No more requests.\n",
      "Total reward for this episode is 3980.000000000033\n",
      "-----------------------------------\n",
      "| average_route_length | 4.11     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.99e+04 |\n",
      "|    ep_rew_mean       | 5.97e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1016     |\n",
      "|    fps               | 571      |\n",
      "|    time_elapsed      | 35509    |\n",
      "|    total_timesteps   | 20306324 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00504  |\n",
      "|    n_updates         | 5064080  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -7179.9999999999645\n",
      "No more requests.\n",
      "Total reward for this episode is 6440.000000000024\n",
      "No more requests.\n",
      "Total reward for this episode is 21879.999999999964\n",
      "No more requests.\n",
      "Total reward for this episode is -6099.999999999977\n",
      "-----------------------------------\n",
      "| average_route_length | 4.35     |\n",
      "| blocked_contiguous   | 0.15     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.15     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.99e+04 |\n",
      "|    ep_rew_mean       | 6.27e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1020     |\n",
      "|    fps               | 571      |\n",
      "|    time_elapsed      | 35665    |\n",
      "|    total_timesteps   | 20386569 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00178  |\n",
      "|    n_updates         | 5084142  |\n",
      "-----------------------------------\n",
      "Stop hitting yourself\n",
      "Num timesteps: 20400000\n",
      "Best mean reward: -inf - Last mean reward per episode: 6274.20\n",
      "No more requests.\n",
      "Total reward for this episode is -19.999999999976836\n",
      "No more requests.\n",
      "Total reward for this episode is 9120.000000000027\n",
      "No more requests.\n",
      "Total reward for this episode is 17380.000000000015\n",
      "No more requests.\n",
      "Total reward for this episode is -11439.999999999973\n",
      "-----------------------------------\n",
      "| average_route_length | 4.82     |\n",
      "| blocked_contiguous   | 0.15     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.15     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2e+04    |\n",
      "|    ep_rew_mean       | 6.11e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1024     |\n",
      "|    fps               | 571      |\n",
      "|    time_elapsed      | 35822    |\n",
      "|    total_timesteps   | 20466691 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00109  |\n",
      "|    n_updates         | 5104172  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 6680.000000000026\n",
      "No more requests.\n",
      "Total reward for this episode is 3460.0000000000264\n",
      "No more requests.\n",
      "Total reward for this episode is 15780.000000000022\n",
      "No more requests.\n",
      "Total reward for this episode is 22339.999999999967\n",
      "-----------------------------------\n",
      "| average_route_length | 3.47     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2e+04    |\n",
      "|    ep_rew_mean       | 6.28e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1028     |\n",
      "|    fps               | 571      |\n",
      "|    time_elapsed      | 35979    |\n",
      "|    total_timesteps   | 20547492 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000471 |\n",
      "|    n_updates         | 5124372  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 19160.00000000002\n",
      "No more requests.\n",
      "Total reward for this episode is 6360.0000000000255\n",
      "Num timesteps: 20600000\n",
      "Best mean reward: -inf - Last mean reward per episode: 6397.00\n",
      "No more requests.\n",
      "Total reward for this episode is 20779.99999999996\n",
      "No more requests.\n",
      "Total reward for this episode is 9219.999999999996\n",
      "-----------------------------------\n",
      "| average_route_length | 3.78     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2e+04    |\n",
      "|    ep_rew_mean       | 6.45e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1032     |\n",
      "|    fps               | 570      |\n",
      "|    time_elapsed      | 36136    |\n",
      "|    total_timesteps   | 20627563 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000473 |\n",
      "|    n_updates         | 5144390  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 19259.999999999975\n",
      "No more requests.\n",
      "Total reward for this episode is 7940.000000000026\n",
      "No more requests.\n",
      "Total reward for this episode is 6140.000000000032\n",
      "No more requests.\n",
      "Total reward for this episode is 6320.000000000022\n",
      "-----------------------------------\n",
      "| average_route_length | 4        |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2e+04    |\n",
      "|    ep_rew_mean       | 6.57e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1036     |\n",
      "|    fps               | 570      |\n",
      "|    time_elapsed      | 36293    |\n",
      "|    total_timesteps   | 20707845 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.036    |\n",
      "|    n_updates         | 5164461  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 17780.000000000025\n",
      "No more requests.\n",
      "Total reward for this episode is 5120.000000000016\n",
      "No more requests.\n",
      "Total reward for this episode is 8060.000000000028\n",
      "No more requests.\n",
      "Total reward for this episode is 8159.999999999991\n",
      "-----------------------------------\n",
      "| average_route_length | 3.89     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2e+04    |\n",
      "|    ep_rew_mean       | 6.79e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1040     |\n",
      "|    fps               | 570      |\n",
      "|    time_elapsed      | 36450    |\n",
      "|    total_timesteps   | 20788176 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0134   |\n",
      "|    n_updates         | 5184543  |\n",
      "-----------------------------------\n",
      "Num timesteps: 20800000\n",
      "Best mean reward: -inf - Last mean reward per episode: 6792.60\n",
      "No more requests.\n",
      "Total reward for this episode is 16780.000000000025\n",
      "No more requests.\n",
      "Total reward for this episode is 6740.000000000019\n",
      "No more requests.\n",
      "Total reward for this episode is -7339.999999999992\n",
      "No more requests.\n",
      "Total reward for this episode is 9940.00000000003\n",
      "-----------------------------------\n",
      "| average_route_length | 3.67     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2e+04    |\n",
      "|    ep_rew_mean       | 6.9e+03  |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1044     |\n",
      "|    fps               | 570      |\n",
      "|    time_elapsed      | 36607    |\n",
      "|    total_timesteps   | 20868271 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00376  |\n",
      "|    n_updates         | 5204567  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 7359.99999999998\n",
      "No more requests.\n",
      "Total reward for this episode is 22799.99999999998\n",
      "No more requests.\n",
      "Total reward for this episode is 18840.000000000022\n",
      "No more requests.\n",
      "Total reward for this episode is 35579.99999999999\n",
      "-----------------------------------\n",
      "| average_route_length | 3.3      |\n",
      "| blocked_contiguous   | 0        |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0        |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2e+04    |\n",
      "|    ep_rew_mean       | 7.51e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1048     |\n",
      "|    fps               | 569      |\n",
      "|    time_elapsed      | 36764    |\n",
      "|    total_timesteps   | 20948606 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000408 |\n",
      "|    n_updates         | 5224651  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 7979.999999999975\n",
      "No more requests.\n",
      "Total reward for this episode is 18059.999999999964\n",
      "Num timesteps: 21000000\n",
      "Best mean reward: -inf - Last mean reward per episode: 7896.80\n",
      "No more requests.\n",
      "Total reward for this episode is 20379.99999999999\n",
      "No more requests.\n",
      "Total reward for this episode is -24299.9999999999\n",
      "-----------------------------------\n",
      "| average_route_length | 4.35     |\n",
      "| blocked_contiguous   | 0.15     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.15     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.99e+04 |\n",
      "|    ep_rew_mean       | 7.86e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1052     |\n",
      "|    fps               | 569      |\n",
      "|    time_elapsed      | 36920    |\n",
      "|    total_timesteps   | 21027625 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00827  |\n",
      "|    n_updates         | 5244406  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 20859.999999999985\n",
      "No more requests.\n",
      "Total reward for this episode is -4279.999999999979\n",
      "No more requests.\n",
      "Total reward for this episode is -3319.9999999999513\n",
      "No more requests.\n",
      "Total reward for this episode is 23259.99999999997\n",
      "-----------------------------------\n",
      "| average_route_length | 3.37     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.99e+04 |\n",
      "|    ep_rew_mean       | 8.14e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1056     |\n",
      "|    fps               | 569      |\n",
      "|    time_elapsed      | 37077    |\n",
      "|    total_timesteps   | 21104946 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0123   |\n",
      "|    n_updates         | 5263736  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -5019.999999999977\n",
      "No more requests.\n",
      "Total reward for this episode is 22119.999999999964\n",
      "No more requests.\n",
      "Total reward for this episode is 7260.000000000022\n",
      "No more requests.\n",
      "Total reward for this episode is 7000.000000000028\n",
      "-----------------------------------\n",
      "| average_route_length | 4        |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.99e+04 |\n",
      "|    ep_rew_mean       | 8.6e+03  |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1060     |\n",
      "|    fps               | 568      |\n",
      "|    time_elapsed      | 37234    |\n",
      "|    total_timesteps   | 21184654 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0103   |\n",
      "|    n_updates         | 5283663  |\n",
      "-----------------------------------\n",
      "Num timesteps: 21200000\n",
      "Best mean reward: -inf - Last mean reward per episode: 8598.00\n",
      "No more requests.\n",
      "Total reward for this episode is 7860.000000000028\n",
      "No more requests.\n",
      "Total reward for this episode is -5679.999999999997\n",
      "No more requests.\n",
      "Total reward for this episode is -7219.999999999974\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -14199.99999999999\n",
      "-----------------------------------\n",
      "| average_route_length | 4.67     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.55     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.98e+04 |\n",
      "|    ep_rew_mean       | 8.35e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1064     |\n",
      "|    fps               | 568      |\n",
      "|    time_elapsed      | 37374    |\n",
      "|    total_timesteps   | 21255779 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00786  |\n",
      "|    n_updates         | 5301444  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 8679.99999999981\n",
      "No more requests.\n",
      "Total reward for this episode is 21259.999999999993\n",
      "No more requests.\n",
      "Total reward for this episode is 8099.9999999999945\n",
      "No more requests.\n",
      "Total reward for this episode is 7720.00000000003\n",
      "-----------------------------------\n",
      "| average_route_length | 3.78     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.98e+04 |\n",
      "|    ep_rew_mean       | 8.91e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1068     |\n",
      "|    fps               | 568      |\n",
      "|    time_elapsed      | 37531    |\n",
      "|    total_timesteps   | 21335866 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0034   |\n",
      "|    n_updates         | 5321466  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 15220.000000000056\n",
      "No more requests.\n",
      "Total reward for this episode is 7300.000000000031\n",
      "No more requests.\n",
      "Total reward for this episode is 17599.99999999998\n",
      "Num timesteps: 21400000\n",
      "Best mean reward: -inf - Last mean reward per episode: 8878.20\n",
      "No more requests.\n",
      "Total reward for this episode is 15920.000000000022\n",
      "-----------------------------------\n",
      "| average_route_length | 4        |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.98e+04 |\n",
      "|    ep_rew_mean       | 8.86e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1072     |\n",
      "|    fps               | 568      |\n",
      "|    time_elapsed      | 37689    |\n",
      "|    total_timesteps   | 21416307 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000306 |\n",
      "|    n_updates         | 5341576  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 4080.0000000000246\n",
      "No more requests.\n",
      "Total reward for this episode is 20139.999999999993\n",
      "No more requests.\n",
      "Total reward for this episode is -3859.999999999972\n",
      "No more requests.\n",
      "Total reward for this episode is 8560.000000000027\n",
      "-----------------------------------\n",
      "| average_route_length | 3.78     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.98e+04 |\n",
      "|    ep_rew_mean       | 9.01e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1076     |\n",
      "|    fps               | 567      |\n",
      "|    time_elapsed      | 37845    |\n",
      "|    total_timesteps   | 21496057 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 32.2     |\n",
      "|    n_updates         | 5361514  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 7320.000000000007\n",
      "No more requests.\n",
      "Total reward for this episode is 21019.999999999967\n",
      "No more requests.\n",
      "Total reward for this episode is 7100.00000000002\n",
      "No more requests.\n",
      "Total reward for this episode is 18779.999999999996\n",
      "-----------------------------------\n",
      "| average_route_length | 3.79     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.98e+04 |\n",
      "|    ep_rew_mean       | 9.25e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1080     |\n",
      "|    fps               | 567      |\n",
      "|    time_elapsed      | 38003    |\n",
      "|    total_timesteps   | 21576453 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 63       |\n",
      "|    n_updates         | 5381613  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 18760.000000000015\n",
      "Num timesteps: 21600000\n",
      "Best mean reward: -inf - Last mean reward per episode: 9114.80\n",
      "No more requests.\n",
      "Total reward for this episode is 2560.0000000000286\n",
      "No more requests.\n",
      "Total reward for this episode is -6779.999999999993\n",
      "No more requests.\n",
      "Total reward for this episode is 5380.000000000029\n",
      "-----------------------------------\n",
      "| average_route_length | 4.11     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.98e+04 |\n",
      "|    ep_rew_mean       | 8.93e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1084     |\n",
      "|    fps               | 567      |\n",
      "|    time_elapsed      | 38160    |\n",
      "|    total_timesteps   | 21655813 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000853 |\n",
      "|    n_updates         | 5401453  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 8420.00000000002\n",
      "No more requests.\n",
      "Total reward for this episode is 4820.000000000027\n",
      "No more requests.\n",
      "Total reward for this episode is 11480.000000000024\n",
      "No more requests.\n",
      "Total reward for this episode is 10019.99999999996\n",
      "-----------------------------------\n",
      "| average_route_length | 3.78     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.98e+04 |\n",
      "|    ep_rew_mean       | 8.86e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1088     |\n",
      "|    fps               | 567      |\n",
      "|    time_elapsed      | 38316    |\n",
      "|    total_timesteps   | 21736227 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.153    |\n",
      "|    n_updates         | 5421556  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 21059.999999999964\n",
      "No more requests.\n",
      "Total reward for this episode is 8120.000000000025\n",
      "No more requests.\n",
      "Total reward for this episode is 3200.000000000025\n",
      "Num timesteps: 21800000\n",
      "Best mean reward: -inf - Last mean reward per episode: 8850.40\n",
      "No more requests.\n",
      "Total reward for this episode is 5380.000000000033\n",
      "-----------------------------------\n",
      "| average_route_length | 4.11     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.98e+04 |\n",
      "|    ep_rew_mean       | 8.95e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1092     |\n",
      "|    fps               | 567      |\n",
      "|    time_elapsed      | 38473    |\n",
      "|    total_timesteps   | 21815571 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00169  |\n",
      "|    n_updates         | 5441392  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 34279.99999999999\n",
      "No more requests.\n",
      "Total reward for this episode is 21539.99999999998\n",
      "No more requests.\n",
      "Total reward for this episode is 19319.999999999978\n",
      "No more requests.\n",
      "Total reward for this episode is 22499.99999999998\n",
      "-----------------------------------\n",
      "| average_route_length | 3.47     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.99e+04 |\n",
      "|    ep_rew_mean       | 9.14e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1096     |\n",
      "|    fps               | 566      |\n",
      "|    time_elapsed      | 38630    |\n",
      "|    total_timesteps   | 21896476 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000833 |\n",
      "|    n_updates         | 5461618  |\n",
      "-----------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is 9520.000000000018\n",
      "No more requests.\n",
      "Total reward for this episode is -5019.9999999999845\n",
      "No more requests.\n",
      "Total reward for this episode is 6060.000000000034\n",
      "No more requests.\n",
      "Total reward for this episode is -8439.999999999973\n",
      "-----------------------------------\n",
      "| average_route_length | 4.47     |\n",
      "| blocked_contiguous   | 0.15     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.15     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.99e+04 |\n",
      "|    ep_rew_mean       | 8.64e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1100     |\n",
      "|    fps               | 566      |\n",
      "|    time_elapsed      | 38788    |\n",
      "|    total_timesteps   | 21976994 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00223  |\n",
      "|    n_updates         | 5481748  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -7319.9999999999845\n",
      "Num timesteps: 22000000\n",
      "Best mean reward: -inf - Last mean reward per episode: 8374.40\n",
      "No more requests.\n",
      "Total reward for this episode is -4639.999999999974\n",
      "No more requests.\n",
      "Total reward for this episode is 4520.000000000126\n",
      "No more requests.\n",
      "Total reward for this episode is 18639.99999999998\n",
      "-----------------------------------\n",
      "| average_route_length | 3.89     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.99e+04 |\n",
      "|    ep_rew_mean       | 8.24e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1104     |\n",
      "|    fps               | 566      |\n",
      "|    time_elapsed      | 38945    |\n",
      "|    total_timesteps   | 22057174 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0859   |\n",
      "|    n_updates         | 5501793  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 37519.99999999999\n",
      "No more requests.\n",
      "Total reward for this episode is 12539.999999999989\n",
      "No more requests.\n",
      "Total reward for this episode is -8640.0\n",
      "No more requests.\n",
      "Total reward for this episode is 20060.000000000015\n",
      "-----------------------------------\n",
      "| average_route_length | 3.79     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.99e+04 |\n",
      "|    ep_rew_mean       | 8.32e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1108     |\n",
      "|    fps               | 566      |\n",
      "|    time_elapsed      | 39101    |\n",
      "|    total_timesteps   | 22136594 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000341 |\n",
      "|    n_updates         | 5521648  |\n",
      "-----------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -16319.999999999987\n",
      "No more requests.\n",
      "Total reward for this episode is 7980.000000000018\n",
      "No more requests.\n",
      "Total reward for this episode is 4920.000000000025\n",
      "Num timesteps: 22200000\n",
      "Best mean reward: -inf - Last mean reward per episode: 8405.00\n",
      "No more requests.\n",
      "Total reward for this episode is 23119.99999999999\n",
      "-----------------------------------\n",
      "| average_route_length | 3.47     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.98e+04 |\n",
      "|    ep_rew_mean       | 8.49e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1112     |\n",
      "|    fps               | 565      |\n",
      "|    time_elapsed      | 39244    |\n",
      "|    total_timesteps   | 22209126 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000554 |\n",
      "|    n_updates         | 5539781  |\n",
      "-----------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -16219.999999999982\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -5959.999999999971\n",
      "No more requests.\n",
      "Total reward for this episode is -4139.999999999971\n",
      "No more requests.\n",
      "Total reward for this episode is -6039.999999999983\n",
      "-----------------------------------\n",
      "| average_route_length | 4.35     |\n",
      "| blocked_contiguous   | 0.15     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.15     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.98e+04 |\n",
      "|    ep_rew_mean       | 7.94e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1116     |\n",
      "|    fps               | 565      |\n",
      "|    time_elapsed      | 39402    |\n",
      "|    total_timesteps   | 22290241 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00067  |\n",
      "|    n_updates         | 5560060  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 20040.0\n",
      "No more requests.\n",
      "Total reward for this episode is 10360.000000000025\n",
      "No more requests.\n",
      "Total reward for this episode is 6100.00000000002\n",
      "No more requests.\n",
      "Total reward for this episode is 19680.000000000015\n",
      "-----------------------------------\n",
      "| average_route_length | 3.79     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.98e+04 |\n",
      "|    ep_rew_mean       | 8.35e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1120     |\n",
      "|    fps               | 565      |\n",
      "|    time_elapsed      | 39561    |\n",
      "|    total_timesteps   | 22371435 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000201 |\n",
      "|    n_updates         | 5580358  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 19899.99999999996\n",
      "Num timesteps: 22400000\n",
      "Best mean reward: -inf - Last mean reward per episode: 8551.20\n",
      "No more requests.\n",
      "Total reward for this episode is -9219.99999999997\n",
      "No more requests.\n",
      "Total reward for this episode is 4700.000000000025\n",
      "No more requests.\n",
      "Total reward for this episode is -23719.99999999996\n",
      "-----------------------------------\n",
      "| average_route_length | 5.12     |\n",
      "| blocked_contiguous   | 0.2      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.2      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.98e+04 |\n",
      "|    ep_rew_mean       | 8.12e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1124     |\n",
      "|    fps               | 565      |\n",
      "|    time_elapsed      | 39717    |\n",
      "|    total_timesteps   | 22450528 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0162   |\n",
      "|    n_updates         | 5600131  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 18259.999999999996\n",
      "No more requests.\n",
      "Total reward for this episode is -5019.9999999999745\n",
      "No more requests.\n",
      "Total reward for this episode is 6740.000000000026\n",
      "No more requests.\n",
      "Total reward for this episode is 7340.000000000022\n",
      "-----------------------------------\n",
      "| average_route_length | 4        |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.98e+04 |\n",
      "|    ep_rew_mean       | 7.91e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1128     |\n",
      "|    fps               | 565      |\n",
      "|    time_elapsed      | 39874    |\n",
      "|    total_timesteps   | 22530326 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0422   |\n",
      "|    n_updates         | 5620081  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 21519.999999999953\n",
      "No more requests.\n",
      "Total reward for this episode is -6559.999999999973\n",
      "No more requests.\n",
      "Total reward for this episode is 10839.999999999978\n",
      "Num timesteps: 22600000\n",
      "Best mean reward: -inf - Last mean reward per episode: 7703.80\n",
      "No more requests.\n",
      "Total reward for this episode is 21379.999999999964\n",
      "-----------------------------------\n",
      "| average_route_length | 3.58     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.98e+04 |\n",
      "|    ep_rew_mean       | 7.83e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1132     |\n",
      "|    fps               | 564      |\n",
      "|    time_elapsed      | 40031    |\n",
      "|    total_timesteps   | 22609883 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000782 |\n",
      "|    n_updates         | 5639970  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 5800.000000000028\n",
      "No more requests.\n",
      "Total reward for this episode is 19699.999999999956\n",
      "No more requests.\n",
      "Total reward for this episode is 20280.00000000002\n",
      "No more requests.\n",
      "Total reward for this episode is 8240.000000000025\n",
      "-----------------------------------\n",
      "| average_route_length | 3.89     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.98e+04 |\n",
      "|    ep_rew_mean       | 7.97e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1136     |\n",
      "|    fps               | 564      |\n",
      "|    time_elapsed      | 40188    |\n",
      "|    total_timesteps   | 22689628 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00979  |\n",
      "|    n_updates         | 5659906  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 18899.99999999998\n",
      "No more requests.\n",
      "Total reward for this episode is 6560.000000000031\n",
      "No more requests.\n",
      "Total reward for this episode is -7779.999999999995\n",
      "No more requests.\n",
      "Total reward for this episode is 22579.999999999956\n",
      "-----------------------------------\n",
      "| average_route_length | 3.47     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.98e+04 |\n",
      "|    ep_rew_mean       | 7.98e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1140     |\n",
      "|    fps               | 564      |\n",
      "|    time_elapsed      | 40345    |\n",
      "|    total_timesteps   | 22768479 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 9.73     |\n",
      "|    n_updates         | 5679619  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 9499.999999999978\n",
      "Num timesteps: 22800000\n",
      "Best mean reward: -inf - Last mean reward per episode: 7907.60\n",
      "No more requests.\n",
      "Total reward for this episode is -5979.999999999984\n",
      "No more requests.\n",
      "Total reward for this episode is -6459.999999999976\n",
      "No more requests.\n",
      "Total reward for this episode is -2399.9999999999845\n",
      "-----------------------------------\n",
      "| average_route_length | 3.88     |\n",
      "| blocked_contiguous   | 0.15     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.15     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.98e+04 |\n",
      "|    ep_rew_mean       | 7.67e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1144     |\n",
      "|    fps               | 564      |\n",
      "|    time_elapsed      | 40503    |\n",
      "|    total_timesteps   | 22847902 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00361  |\n",
      "|    n_updates         | 5699475  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 7040.000000000015\n",
      "No more requests.\n",
      "Total reward for this episode is -8279.999999999978\n",
      "No more requests.\n",
      "Total reward for this episode is 34760.0\n",
      "No more requests.\n",
      "Total reward for this episode is -11499.99999999996\n",
      "-----------------------------------\n",
      "| average_route_length | 4.71     |\n",
      "| blocked_contiguous   | 0.15     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.15     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.98e+04 |\n",
      "|    ep_rew_mean       | 7.04e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1148     |\n",
      "|    fps               | 563      |\n",
      "|    time_elapsed      | 40660    |\n",
      "|    total_timesteps   | 22928148 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 3.94     |\n",
      "|    n_updates         | 5719536  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -2919.999999999972\n",
      "No more requests.\n",
      "Total reward for this episode is 11180.000000000027\n",
      "No more requests.\n",
      "Total reward for this episode is 7379.999999999978\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -3619.9999999999854\n",
      "-----------------------------------\n",
      "| average_route_length | 4.6      |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.5      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.97e+04 |\n",
      "|    ep_rew_mean       | 6.95e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1152     |\n",
      "|    fps               | 563      |\n",
      "|    time_elapsed      | 40801    |\n",
      "|    total_timesteps   | 22999661 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000317 |\n",
      "|    n_updates         | 5737415  |\n",
      "-----------------------------------\n",
      "Num timesteps: 23000000\n",
      "Best mean reward: -inf - Last mean reward per episode: 6946.80\n",
      "No more requests.\n",
      "Total reward for this episode is -19779.99999999997\n",
      "No more requests.\n",
      "Total reward for this episode is 3440.000000000019\n",
      "No more requests.\n",
      "Total reward for this episode is -6499.999999999989\n",
      "No more requests.\n",
      "Total reward for this episode is 4320.000000000029\n",
      "-----------------------------------\n",
      "| average_route_length | 4.11     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.97e+04 |\n",
      "|    ep_rew_mean       | 6.4e+03  |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1156     |\n",
      "|    fps               | 563      |\n",
      "|    time_elapsed      | 40959    |\n",
      "|    total_timesteps   | 23079147 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00441  |\n",
      "|    n_updates         | 5757286  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 7760.000000000034\n",
      "No more requests.\n",
      "Total reward for this episode is 9139.999999999975\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 10280.000000000007\n",
      "No more requests.\n",
      "Total reward for this episode is 10599.99999999996\n",
      "-----------------------------------\n",
      "| average_route_length | 3.44     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.96e+04 |\n",
      "|    ep_rew_mean       | 6.47e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1160     |\n",
      "|    fps               | 563      |\n",
      "|    time_elapsed      | 41097    |\n",
      "|    total_timesteps   | 23149172 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0464   |\n",
      "|    n_updates         | 5774792  |\n",
      "-----------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 16120.000000000016\n",
      "No more requests.\n",
      "Total reward for this episode is 6540.00000000002\n",
      "Num timesteps: 23200000\n",
      "Best mean reward: -inf - Last mean reward per episode: 6678.60\n",
      "No more requests.\n",
      "Total reward for this episode is 22179.99999999997\n",
      "No more requests.\n",
      "Total reward for this episode is 5800.0000000000155\n",
      "-----------------------------------\n",
      "| average_route_length | 4.11     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.97e+04 |\n",
      "|    ep_rew_mean       | 7.17e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1164     |\n",
      "|    fps               | 563      |\n",
      "|    time_elapsed      | 41245    |\n",
      "|    total_timesteps   | 23224261 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00037  |\n",
      "|    n_updates         | 5793565  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 9179.999999999967\n",
      "No more requests.\n",
      "Total reward for this episode is -17679.999999999967\n",
      "No more requests.\n",
      "Total reward for this episode is -17819.99999999998\n",
      "No more requests.\n",
      "Total reward for this episode is 16600.000000000025\n",
      "-----------------------------------\n",
      "| average_route_length | 4        |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.97e+04 |\n",
      "|    ep_rew_mean       | 6.61e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1168     |\n",
      "|    fps               | 562      |\n",
      "|    time_elapsed      | 41401    |\n",
      "|    total_timesteps   | 23303064 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000212 |\n",
      "|    n_updates         | 5813265  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 3660.0000000000377\n",
      "No more requests.\n",
      "Total reward for this episode is 7060.000000000028\n",
      "No more requests.\n",
      "Total reward for this episode is 8600.000000000022\n",
      "No more requests.\n",
      "Total reward for this episode is 22939.999999999956\n",
      "-----------------------------------\n",
      "| average_route_length | 3.47     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.97e+04 |\n",
      "|    ep_rew_mean       | 6.47e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1172     |\n",
      "|    fps               | 562      |\n",
      "|    time_elapsed      | 41558    |\n",
      "|    total_timesteps   | 23382218 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000666 |\n",
      "|    n_updates         | 5833054  |\n",
      "-----------------------------------\n",
      "Num timesteps: 23400000\n",
      "Best mean reward: -inf - Last mean reward per episode: 6474.40\n",
      "No more requests.\n",
      "Total reward for this episode is 18199.99999999997\n",
      "No more requests.\n",
      "Total reward for this episode is 9939.999999999964\n",
      "No more requests.\n",
      "Total reward for this episode is -4859.99999999998\n",
      "No more requests.\n",
      "Total reward for this episode is 10860.000000000027\n",
      "-----------------------------------\n",
      "| average_route_length | 3.56     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.97e+04 |\n",
      "|    ep_rew_mean       | 6.53e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1176     |\n",
      "|    fps               | 562      |\n",
      "|    time_elapsed      | 41714    |\n",
      "|    total_timesteps   | 23461447 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00474  |\n",
      "|    n_updates         | 5852861  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -7099.999999999977\n",
      "No more requests.\n",
      "Total reward for this episode is 20720.0\n",
      "No more requests.\n",
      "Total reward for this episode is 18719.999999999964\n",
      "No more requests.\n",
      "Total reward for this episode is 5160.000000000019\n",
      "-----------------------------------\n",
      "| average_route_length | 4.22     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.97e+04 |\n",
      "|    ep_rew_mean       | 6.36e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1180     |\n",
      "|    fps               | 562      |\n",
      "|    time_elapsed      | 41872    |\n",
      "|    total_timesteps   | 23541595 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00105  |\n",
      "|    n_updates         | 5872898  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 20059.999999999975\n",
      "No more requests.\n",
      "Total reward for this episode is 32079.999999999985\n",
      "Num timesteps: 23600000\n",
      "Best mean reward: -inf - Last mean reward per episode: 6671.60\n",
      "No more requests.\n",
      "Total reward for this episode is 22199.999999999956\n",
      "No more requests.\n",
      "Total reward for this episode is 23319.999999999975\n",
      "-----------------------------------\n",
      "| average_route_length | 3.37     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.97e+04 |\n",
      "|    ep_rew_mean       | 7.14e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1184     |\n",
      "|    fps               | 562      |\n",
      "|    time_elapsed      | 42029    |\n",
      "|    total_timesteps   | 23621659 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000951 |\n",
      "|    n_updates         | 5892914  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 8439.999999999964\n",
      "No more requests.\n",
      "Total reward for this episode is -7879.9999999999745\n",
      "No more requests.\n",
      "Total reward for this episode is 5940.000000000029\n",
      "No more requests.\n",
      "Total reward for this episode is 22439.999999999985\n",
      "-----------------------------------\n",
      "| average_route_length | 3.47     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.97e+04 |\n",
      "|    ep_rew_mean       | 7.08e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1188     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 42186    |\n",
      "|    total_timesteps   | 23701418 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 1.73     |\n",
      "|    n_updates         | 5912854  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -4819.999999999975\n",
      "No more requests.\n",
      "Total reward for this episode is 9060.000000000025\n",
      "No more requests.\n",
      "Total reward for this episode is 32279.999999999996\n",
      "No more requests.\n",
      "Total reward for this episode is 7380.000000000033\n",
      "-----------------------------------\n",
      "| average_route_length | 3.89     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.97e+04 |\n",
      "|    ep_rew_mean       | 7.14e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1192     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 42344    |\n",
      "|    total_timesteps   | 23781566 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000222 |\n",
      "|    n_updates         | 5932891  |\n",
      "-----------------------------------\n",
      "Num timesteps: 23800000\n",
      "Best mean reward: -inf - Last mean reward per episode: 7144.20\n",
      "No more requests.\n",
      "Total reward for this episode is 16380.000000000087\n",
      "No more requests.\n",
      "Total reward for this episode is -6159.99999999998\n",
      "No more requests.\n",
      "Total reward for this episode is -9119.999999999975\n",
      "No more requests.\n",
      "Total reward for this episode is 9359.999999999996\n",
      "-----------------------------------\n",
      "| average_route_length | 3.78     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.97e+04 |\n",
      "|    ep_rew_mean       | 6.27e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1196     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 42501    |\n",
      "|    total_timesteps   | 23861576 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000358 |\n",
      "|    n_updates         | 5952893  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 7320.000000000027\n",
      "No more requests.\n",
      "Total reward for this episode is 7659.999999999963\n",
      "No more requests.\n",
      "Total reward for this episode is 19600.000000000025\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is 5740.000000000026\n",
      "-----------------------------------\n",
      "| average_route_length | 4.11     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.96e+04 |\n",
      "|    ep_rew_mean       | 6.65e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1200     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 42657    |\n",
      "|    total_timesteps   | 23940903 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00565  |\n",
      "|    n_updates         | 5972725  |\n",
      "-----------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -6039.999999999996\n",
      "No more requests.\n",
      "Total reward for this episode is 8600.00000000002\n",
      "No more requests.\n",
      "Total reward for this episode is 8800.000000000025\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 18220.000000000007\n",
      "-----------------------------------\n",
      "| average_route_length | 3.71     |\n",
      "| blocked_contiguous   | 0        |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.3      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.94e+04 |\n",
      "|    ep_rew_mean       | 6.85e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1204     |\n",
      "|    fps               | 560      |\n",
      "|    time_elapsed      | 42776    |\n",
      "|    total_timesteps   | 23995600 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0049   |\n",
      "|    n_updates         | 5986399  |\n",
      "-----------------------------------\n",
      "Num timesteps: 24000000\n",
      "Best mean reward: -inf - Last mean reward per episode: 6847.40\n",
      "No more requests.\n",
      "Total reward for this episode is 35400.0\n",
      "No more requests.\n",
      "Total reward for this episode is 20999.999999999993\n",
      "No more requests.\n",
      "Total reward for this episode is 8480.000000000024\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is 5579.999999999987\n",
      "-----------------------------------\n",
      "| average_route_length | 4.11     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.94e+04 |\n",
      "|    ep_rew_mean       | 6.94e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1208     |\n",
      "|    fps               | 560      |\n",
      "|    time_elapsed      | 42932    |\n",
      "|    total_timesteps   | 24081052 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000198 |\n",
      "|    n_updates         | 6007762  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 6760.000000000028\n",
      "No more requests.\n",
      "Total reward for this episode is 21939.999999999967\n",
      "No more requests.\n",
      "Total reward for this episode is 13239.999999999955\n",
      "No more requests.\n",
      "Total reward for this episode is 8779.999999999976\n",
      "-----------------------------------\n",
      "| average_route_length | 3.89     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.96e+04 |\n",
      "|    ep_rew_mean       | 7.24e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1212     |\n",
      "|    fps               | 560      |\n",
      "|    time_elapsed      | 43089    |\n",
      "|    total_timesteps   | 24169167 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000195 |\n",
      "|    n_updates         | 6029791  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 21759.999999999956\n",
      "Num timesteps: 24200000\n",
      "Best mean reward: -inf - Last mean reward per episode: 7619.60\n",
      "No more requests.\n",
      "Total reward for this episode is 6320.000000000022\n",
      "No more requests.\n",
      "Total reward for this episode is 21819.999999999964\n",
      "No more requests.\n",
      "Total reward for this episode is 22819.99999999996\n",
      "-----------------------------------\n",
      "| average_route_length | 3.47     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.97e+04 |\n",
      "|    ep_rew_mean       | 8.19e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1216     |\n",
      "|    fps               | 560      |\n",
      "|    time_elapsed      | 43245    |\n",
      "|    total_timesteps   | 24257351 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00151  |\n",
      "|    n_updates         | 6051837  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 19300.000000000007\n",
      "No more requests.\n",
      "Total reward for this episode is 5220.000000000032\n",
      "No more requests.\n",
      "Total reward for this episode is -6699.9999999999745\n",
      "No more requests.\n",
      "Total reward for this episode is 20579.99999999996\n",
      "-----------------------------------\n",
      "| average_route_length | 3.68     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.97e+04 |\n",
      "|    ep_rew_mean       | 8e+03    |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1220     |\n",
      "|    fps               | 560      |\n",
      "|    time_elapsed      | 43402    |\n",
      "|    total_timesteps   | 24345288 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0458   |\n",
      "|    n_updates         | 6073821  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 6940.000000000026\n",
      "No more requests.\n",
      "Total reward for this episode is 4200.000000000031\n",
      "Num timesteps: 24400000\n",
      "Best mean reward: -inf - Last mean reward per episode: 8008.40\n",
      "No more requests.\n",
      "Total reward for this episode is 6999.999999999983\n",
      "No more requests.\n",
      "Total reward for this episode is 25259.999999999956\n",
      "-----------------------------------\n",
      "| average_route_length | 3.16     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.98e+04 |\n",
      "|    ep_rew_mean       | 8.52e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1224     |\n",
      "|    fps               | 560      |\n",
      "|    time_elapsed      | 43558    |\n",
      "|    total_timesteps   | 24433877 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0054   |\n",
      "|    n_updates         | 6095969  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 10240.000000000025\n",
      "No more requests.\n",
      "Total reward for this episode is 22559.999999999978\n",
      "No more requests.\n",
      "Total reward for this episode is 25879.99999999995\n",
      "No more requests.\n",
      "Total reward for this episode is 8539.999999999969\n",
      "-----------------------------------\n",
      "| average_route_length | 3.89     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.99e+04 |\n",
      "|    ep_rew_mean       | 8.92e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1228     |\n",
      "|    fps               | 560      |\n",
      "|    time_elapsed      | 43715    |\n",
      "|    total_timesteps   | 24522711 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000323 |\n",
      "|    n_updates         | 6118177  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 25480.000000000007\n",
      "No more requests.\n",
      "Total reward for this episode is 8560.000000000031\n",
      "No more requests.\n",
      "Total reward for this episode is 6200.0000000000255\n",
      "Num timesteps: 24600000\n",
      "Best mean reward: -inf - Last mean reward per episode: 9062.60\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -7139.9999999999745\n",
      "-----------------------------------\n",
      "| average_route_length | 4.35     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.15     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2e+04    |\n",
      "|    ep_rew_mean       | 8.78e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1232     |\n",
      "|    fps               | 560      |\n",
      "|    time_elapsed      | 43871    |\n",
      "|    total_timesteps   | 24610823 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 10.4     |\n",
      "|    n_updates         | 6140205  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -2699.9999999999645\n",
      "No more requests.\n",
      "Total reward for this episode is 19779.99999999997\n",
      "No more requests.\n",
      "Total reward for this episode is 18820.000000000015\n",
      "No more requests.\n",
      "Total reward for this episode is 34399.99999999999\n",
      "-----------------------------------\n",
      "| average_route_length | 3.4      |\n",
      "| blocked_contiguous   | 0        |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0        |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.01e+04 |\n",
      "|    ep_rew_mean       | 8.94e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1236     |\n",
      "|    fps               | 560      |\n",
      "|    time_elapsed      | 44029    |\n",
      "|    total_timesteps   | 24700673 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 5.67     |\n",
      "|    n_updates         | 6162668  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 6000.0000000000255\n",
      "No more requests.\n",
      "Total reward for this episode is 16840.000000000022\n",
      "No more requests.\n",
      "Total reward for this episode is 7000.000000000028\n",
      "No more requests.\n",
      "Total reward for this episode is 7499.999999999992\n",
      "-----------------------------------\n",
      "| average_route_length | 3.89     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.02e+04 |\n",
      "|    ep_rew_mean       | 8.92e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1240     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 44188    |\n",
      "|    total_timesteps   | 24791051 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0139   |\n",
      "|    n_updates         | 6185262  |\n",
      "-----------------------------------\n",
      "Num timesteps: 24800000\n",
      "Best mean reward: -inf - Last mean reward per episode: 8915.00\n",
      "No more requests.\n",
      "Total reward for this episode is 8520.000000000033\n",
      "No more requests.\n",
      "Total reward for this episode is 20959.999999999956\n",
      "No more requests.\n",
      "Total reward for this episode is 21039.99999999998\n",
      "No more requests.\n",
      "Total reward for this episode is 23239.99999999997\n",
      "-----------------------------------\n",
      "| average_route_length | 3.47     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.03e+04 |\n",
      "|    ep_rew_mean       | 9.71e+03 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1244     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 44344    |\n",
      "|    total_timesteps   | 24880010 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0101   |\n",
      "|    n_updates         | 6207502  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 34540.00000000001\n",
      "No more requests.\n",
      "Total reward for this episode is 23639.99999999997\n",
      "No more requests.\n",
      "Total reward for this episode is 9120.000000000024\n",
      "No more requests.\n",
      "Total reward for this episode is -5059.999999999975\n",
      "-----------------------------------\n",
      "| average_route_length | 4.12     |\n",
      "| blocked_contiguous   | 0.15     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.15     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.04e+04 |\n",
      "|    ep_rew_mean       | 1.01e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1248     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 44502    |\n",
      "|    total_timesteps   | 24969552 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0733   |\n",
      "|    n_updates         | 6229887  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 25319.999999999964\n",
      "Num timesteps: 25000000\n",
      "Best mean reward: -inf - Last mean reward per episode: 10390.60\n",
      "No more requests.\n",
      "Total reward for this episode is -4239.999999999957\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is 8820.000000000031\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 22719.99999999998\n",
      "-----------------------------------\n",
      "| average_route_length | 3.47     |\n",
      "| blocked_contiguous   | 0        |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.25     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.05e+04 |\n",
      "|    ep_rew_mean       | 1.05e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1252     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 44650    |\n",
      "|    total_timesteps   | 25053957 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.552    |\n",
      "|    n_updates         | 6250989  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 11519.99999999997\n",
      "No more requests.\n",
      "Total reward for this episode is -5179.999999999963\n",
      "No more requests.\n",
      "Total reward for this episode is 14479.999999999967\n",
      "No more requests.\n",
      "Total reward for this episode is -8419.999999999965\n",
      "-----------------------------------\n",
      "| average_route_length | 4.35     |\n",
      "| blocked_contiguous   | 0.15     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.15     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.06e+04 |\n",
      "|    ep_rew_mean       | 1.08e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1256     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 44807    |\n",
      "|    total_timesteps   | 25143517 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00692  |\n",
      "|    n_updates         | 6273379  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 4620.000000000047\n",
      "No more requests.\n",
      "Total reward for this episode is 22939.999999999956\n",
      "Num timesteps: 25200000\n",
      "Best mean reward: -inf - Last mean reward per episode: 10928.00\n",
      "No more requests.\n",
      "Total reward for this episode is 17539.99999999998\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 16540.000000000007\n",
      "-----------------------------------\n",
      "| average_route_length | 3.86     |\n",
      "| blocked_contiguous   | 0        |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.3      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.08e+04 |\n",
      "|    ep_rew_mean       | 1.11e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1260     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 44953    |\n",
      "|    total_timesteps   | 25227048 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0778   |\n",
      "|    n_updates         | 6294261  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -4619.99999999998\n",
      "No more requests.\n",
      "Total reward for this episode is 4380.000000000032\n",
      "No more requests.\n",
      "Total reward for this episode is 8420.000000000022\n",
      "No more requests.\n",
      "Total reward for this episode is 7639.999999999972\n",
      "-----------------------------------\n",
      "| average_route_length | 3.89     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.09e+04 |\n",
      "|    ep_rew_mean       | 1.07e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1264     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 45110    |\n",
      "|    total_timesteps   | 25316044 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0015   |\n",
      "|    n_updates         | 6316510  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 22459.999999999953\n",
      "No more requests.\n",
      "Total reward for this episode is 10580.000000000033\n",
      "No more requests.\n",
      "Total reward for this episode is 33400.0\n",
      "Num timesteps: 25400000\n",
      "Best mean reward: -inf - Last mean reward per episode: 11631.80\n",
      "No more requests.\n",
      "Total reward for this episode is 21659.999999999945\n",
      "-----------------------------------\n",
      "| average_route_length | 3.47     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.1e+04  |\n",
      "|    ep_rew_mean       | 1.17e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1268     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 45267    |\n",
      "|    total_timesteps   | 25405063 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000139 |\n",
      "|    n_updates         | 6338765  |\n",
      "-----------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -7479.999999999997\n",
      "No more requests.\n",
      "Total reward for this episode is 20499.999999999975\n",
      "No more requests.\n",
      "Total reward for this episode is 23639.99999999994\n",
      "No more requests.\n",
      "Total reward for this episode is 8799.99999999998\n",
      "-----------------------------------\n",
      "| average_route_length | 3.78     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.09e+04 |\n",
      "|    ep_rew_mean       | 1.17e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1272     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 45393    |\n",
      "|    total_timesteps   | 25476697 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000156 |\n",
      "|    n_updates         | 6356674  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 24719.99999999995\n",
      "No more requests.\n",
      "Total reward for this episode is 6540.000000000025\n",
      "No more requests.\n",
      "Total reward for this episode is 9179.999999999976\n",
      "No more requests.\n",
      "Total reward for this episode is 19479.999999999858\n",
      "-----------------------------------\n",
      "| average_route_length | 3.6      |\n",
      "| blocked_contiguous   | 0        |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0        |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.11e+04 |\n",
      "|    ep_rew_mean       | 1.2e+04  |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1276     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 45551    |\n",
      "|    total_timesteps   | 25567212 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000261 |\n",
      "|    n_updates         | 6379302  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 7259.999999999931\n",
      "Num timesteps: 25600000\n",
      "Best mean reward: -inf - Last mean reward per episode: 12123.40\n",
      "No more requests.\n",
      "Total reward for this episode is 20279.999999999985\n",
      "No more requests.\n",
      "Total reward for this episode is 22299.999999999956\n",
      "No more requests.\n",
      "Total reward for this episode is 37399.99999999999\n",
      "-----------------------------------\n",
      "| average_route_length | 3.1      |\n",
      "| blocked_contiguous   | 0        |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0        |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.12e+04 |\n",
      "|    ep_rew_mean       | 1.25e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1280     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 45708    |\n",
      "|    total_timesteps   | 25656698 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.505    |\n",
      "|    n_updates         | 6401674  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 8879.999999999976\n",
      "No more requests.\n",
      "Total reward for this episode is 8899.999999999984\n",
      "No more requests.\n",
      "Total reward for this episode is 23580.000000000015\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 22279.999999999985\n",
      "-----------------------------------\n",
      "| average_route_length | 3.14     |\n",
      "| blocked_contiguous   | 0        |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.3      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.12e+04 |\n",
      "|    ep_rew_mean       | 1.21e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1284     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 45855    |\n",
      "|    total_timesteps   | 25740420 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0134   |\n",
      "|    n_updates         | 6422604  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 10399.999999999975\n",
      "No more requests.\n",
      "Total reward for this episode is -5559.999999999974\n",
      "Num timesteps: 25800000\n",
      "Best mean reward: -inf - Last mean reward per episode: 12185.40\n",
      "No more requests.\n",
      "Total reward for this episode is 37219.999999999985\n",
      "No more requests.\n",
      "Total reward for this episode is 25060.000000000004\n",
      "-----------------------------------\n",
      "| average_route_length | 3.26     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.13e+04 |\n",
      "|    ep_rew_mean       | 1.25e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1288     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 46013    |\n",
      "|    total_timesteps   | 25830456 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000714 |\n",
      "|    n_updates         | 6445113  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 9319.999999999962\n",
      "No more requests.\n",
      "Total reward for this episode is 25819.99999999995\n",
      "No more requests.\n",
      "Total reward for this episode is -4939.9999999999745\n",
      "No more requests.\n",
      "Total reward for this episode is 21719.999999999978\n",
      "-----------------------------------\n",
      "| average_route_length | 3.47     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.14e+04 |\n",
      "|    ep_rew_mean       | 1.26e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1292     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 46169    |\n",
      "|    total_timesteps   | 25919755 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000421 |\n",
      "|    n_updates         | 6467438  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 23580.000000000015\n",
      "No more requests.\n",
      "Total reward for this episode is 11539.999999999949\n",
      "No more requests.\n",
      "Total reward for this episode is 35780.00000000001\n",
      "Num timesteps: 26000000\n",
      "Best mean reward: -inf - Last mean reward per episode: 13302.60\n",
      "No more requests.\n",
      "Total reward for this episode is 11399.999999999949\n",
      "-----------------------------------\n",
      "| average_route_length | 3.44     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.15e+04 |\n",
      "|    ep_rew_mean       | 1.33e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1296     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 46325    |\n",
      "|    total_timesteps   | 26008465 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000643 |\n",
      "|    n_updates         | 6489616  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 26059.999999999967\n",
      "No more requests.\n",
      "Total reward for this episode is 11540.00000000002\n",
      "No more requests.\n",
      "Total reward for this episode is 840.0000000000318\n",
      "No more requests.\n",
      "Total reward for this episode is 25259.999999999956\n",
      "-----------------------------------\n",
      "| average_route_length | 3.16     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.16e+04 |\n",
      "|    ep_rew_mean       | 1.36e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1300     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 46482    |\n",
      "|    total_timesteps   | 26097669 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000313 |\n",
      "|    n_updates         | 6511917  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -6559.999999999968\n",
      "No more requests.\n",
      "Total reward for this episode is 24999.99999999996\n",
      "No more requests.\n",
      "Total reward for this episode is 25699.99999999994\n",
      "No more requests.\n",
      "Total reward for this episode is 9800.00000000003\n",
      "-----------------------------------\n",
      "| average_route_length | 3.67     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.19e+04 |\n",
      "|    ep_rew_mean       | 1.38e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1304     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 46638    |\n",
      "|    total_timesteps   | 26186454 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000325 |\n",
      "|    n_updates         | 6534113  |\n",
      "-----------------------------------\n",
      "Num timesteps: 26200000\n",
      "Best mean reward: -inf - Last mean reward per episode: 13789.20\n",
      "No more requests.\n",
      "Total reward for this episode is 9760.000000000027\n",
      "No more requests.\n",
      "Total reward for this episode is 20580.00000000002\n",
      "No more requests.\n",
      "Total reward for this episode is 24699.999999999956\n",
      "No more requests.\n",
      "Total reward for this episode is 20839.999999999964\n",
      "-----------------------------------\n",
      "| average_route_length | 3.58     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.19e+04 |\n",
      "|    ep_rew_mean       | 1.38e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1308     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 46796    |\n",
      "|    total_timesteps   | 26275865 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00525  |\n",
      "|    n_updates         | 6556466  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -1499.9999999999745\n",
      "No more requests.\n",
      "Total reward for this episode is -3079.999999999915\n",
      "No more requests.\n",
      "Total reward for this episode is 20659.999999999996\n",
      "No more requests.\n",
      "Total reward for this episode is 19839.99999999998\n",
      "-----------------------------------\n",
      "| average_route_length | 3.68     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.2e+04  |\n",
      "|    ep_rew_mean       | 1.37e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1312     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 46952    |\n",
      "|    total_timesteps   | 26365564 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000209 |\n",
      "|    n_updates         | 6578890  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 5300.00000000004\n",
      "Num timesteps: 26400000\n",
      "Best mean reward: -inf - Last mean reward per episode: 13530.80\n",
      "No more requests.\n",
      "Total reward for this episode is -5699.999999999977\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -13420.000000000002\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 5600.000000000008\n",
      "-----------------------------------\n",
      "| average_route_length | 3.78     |\n",
      "| blocked_contiguous   | 0        |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.55     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.17e+04 |\n",
      "|    ep_rew_mean       | 1.29e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1316     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 47055    |\n",
      "|    total_timesteps   | 26423672 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 6.16     |\n",
      "|    n_updates         | 6593417  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 22199.99999999997\n",
      "No more requests.\n",
      "Total reward for this episode is 9060.000000000027\n",
      "No more requests.\n",
      "Total reward for this episode is 24639.999999999964\n",
      "No more requests.\n",
      "Total reward for this episode is 21220.000000000022\n",
      "-----------------------------------\n",
      "| average_route_length | 3.58     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.17e+04 |\n",
      "|    ep_rew_mean       | 1.33e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1320     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 47212    |\n",
      "|    total_timesteps   | 26512876 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000191 |\n",
      "|    n_updates         | 6615718  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 34579.99999999999\n",
      "No more requests.\n",
      "Total reward for this episode is 8300.000000000025\n",
      "No more requests.\n",
      "Total reward for this episode is 11660.00000000002\n",
      "Num timesteps: 26600000\n",
      "Best mean reward: -inf - Last mean reward per episode: 13648.40\n",
      "No more requests.\n",
      "Total reward for this episode is 23519.999999999953\n",
      "-----------------------------------\n",
      "| average_route_length | 3.26     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.17e+04 |\n",
      "|    ep_rew_mean       | 1.36e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1324     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 47368    |\n",
      "|    total_timesteps   | 26602139 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0312   |\n",
      "|    n_updates         | 6638034  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 6320.00000000003\n",
      "No more requests.\n",
      "Total reward for this episode is 9079.999999999962\n",
      "No more requests.\n",
      "Total reward for this episode is 8760.000000000031\n",
      "No more requests.\n",
      "Total reward for this episode is 12299.999999999975\n",
      "-----------------------------------\n",
      "| average_route_length | 3.44     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.17e+04 |\n",
      "|    ep_rew_mean       | 1.33e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1328     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 47525    |\n",
      "|    total_timesteps   | 26690979 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00236  |\n",
      "|    n_updates         | 6660244  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -2879.9999999999623\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 17160.000000000015\n",
      "No more requests.\n",
      "Total reward for this episode is 6220.000000000042\n",
      "No more requests.\n",
      "Total reward for this episode is 9139.999999999964\n",
      "-----------------------------------\n",
      "| average_route_length | 3.67     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.16e+04 |\n",
      "|    ep_rew_mean       | 1.33e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1332     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 47671    |\n",
      "|    total_timesteps   | 26774284 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000139 |\n",
      "|    n_updates         | 6681070  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 9179.999999999962\n",
      "Num timesteps: 26800000\n",
      "Best mean reward: -inf - Last mean reward per episode: 13413.20\n",
      "No more requests.\n",
      "Total reward for this episode is 11639.999999999949\n",
      "No more requests.\n",
      "Total reward for this episode is -4839.99999999997\n",
      "No more requests.\n",
      "Total reward for this episode is 11179.999999999964\n",
      "-----------------------------------\n",
      "| average_route_length | 3.56     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.16e+04 |\n",
      "|    ep_rew_mean       | 1.29e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1336     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 47827    |\n",
      "|    total_timesteps   | 26863318 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000818 |\n",
      "|    n_updates         | 6703329  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 22239.99999999997\n",
      "No more requests.\n",
      "Total reward for this episode is 6220.000000000038\n",
      "No more requests.\n",
      "Total reward for this episode is 18339.99999999997\n",
      "No more requests.\n",
      "Total reward for this episode is 37519.99999999999\n",
      "-----------------------------------\n",
      "| average_route_length | 3.1      |\n",
      "| blocked_contiguous   | 0        |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0        |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.16e+04 |\n",
      "|    ep_rew_mean       | 1.33e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1340     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 47986    |\n",
      "|    total_timesteps   | 26954640 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00209  |\n",
      "|    n_updates         | 6726159  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 10759.999999999975\n",
      "No more requests.\n",
      "Total reward for this episode is 10299.999999999955\n",
      "Num timesteps: 27000000\n",
      "Best mean reward: -inf - Last mean reward per episode: 13246.60\n",
      "No more requests.\n",
      "Total reward for this episode is 6979.999999999993\n",
      "No more requests.\n",
      "Total reward for this episode is -8739.999999999973\n",
      "-----------------------------------\n",
      "| average_route_length | 4.47     |\n",
      "| blocked_contiguous   | 0.15     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.15     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.16e+04 |\n",
      "|    ep_rew_mean       | 1.28e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1344     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 48144    |\n",
      "|    total_timesteps   | 27044133 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 22.8     |\n",
      "|    n_updates         | 6748533  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 5200.000000000011\n",
      "No more requests.\n",
      "Total reward for this episode is 23319.999999999964\n",
      "No more requests.\n",
      "Total reward for this episode is -4439.999999999986\n",
      "No more requests.\n",
      "Total reward for this episode is 10060.000000000025\n",
      "-----------------------------------\n",
      "| average_route_length | 3.67     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.16e+04 |\n",
      "|    ep_rew_mean       | 1.25e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1348     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 48301    |\n",
      "|    total_timesteps   | 27133939 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00135  |\n",
      "|    n_updates         | 6770984  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 23000.000000000007\n",
      "No more requests.\n",
      "Total reward for this episode is 36700.0\n",
      "Num timesteps: 27200000\n",
      "Best mean reward: -inf - Last mean reward per episode: 12893.40\n",
      "No more requests.\n",
      "Total reward for this episode is 8199.999999999989\n",
      "No more requests.\n",
      "Total reward for this episode is 3960.0000000000236\n",
      "-----------------------------------\n",
      "| average_route_length | 4.22     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.17e+04 |\n",
      "|    ep_rew_mean       | 1.27e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1352     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 48459    |\n",
      "|    total_timesteps   | 27223786 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00123  |\n",
      "|    n_updates         | 6793446  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 23939.999999999964\n",
      "No more requests.\n",
      "Total reward for this episode is 21959.99999999999\n",
      "No more requests.\n",
      "Total reward for this episode is 10299.999999999945\n",
      "No more requests.\n",
      "Total reward for this episode is 25299.999999999967\n",
      "-----------------------------------\n",
      "| average_route_length | 3.26     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.17e+04 |\n",
      "|    ep_rew_mean       | 1.34e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1356     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 48615    |\n",
      "|    total_timesteps   | 27312446 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.963    |\n",
      "|    n_updates         | 6815611  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 8600.000000000022\n",
      "No more requests.\n",
      "Total reward for this episode is 8299.999999999996\n",
      "No more requests.\n",
      "Total reward for this episode is 35460.0\n",
      "Num timesteps: 27400000\n",
      "Best mean reward: -inf - Last mean reward per episode: 13457.60\n",
      "No more requests.\n",
      "Total reward for this episode is 8759.999999999962\n",
      "-----------------------------------\n",
      "| average_route_length | 3.78     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.17e+04 |\n",
      "|    ep_rew_mean       | 1.34e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1360     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 48771    |\n",
      "|    total_timesteps   | 27401411 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.017    |\n",
      "|    n_updates         | 6837852  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 11879.999999999949\n",
      "No more requests.\n",
      "Total reward for this episode is 21799.99999999998\n",
      "No more requests.\n",
      "Total reward for this episode is 25999.999999999956\n",
      "No more requests.\n",
      "Total reward for this episode is 12839.999999999953\n",
      "-----------------------------------\n",
      "| average_route_length | 3.44     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.17e+04 |\n",
      "|    ep_rew_mean       | 1.39e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1364     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 48928    |\n",
      "|    total_timesteps   | 27490932 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000616 |\n",
      "|    n_updates         | 6860232  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -1480.0000000000673\n",
      "No more requests.\n",
      "Total reward for this episode is 11000.00000000003\n",
      "No more requests.\n",
      "Total reward for this episode is 8520.000000000033\n",
      "No more requests.\n",
      "Total reward for this episode is 21239.99999999995\n",
      "-----------------------------------\n",
      "| average_route_length | 3.47     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.17e+04 |\n",
      "|    ep_rew_mean       | 1.35e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1368     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 49084    |\n",
      "|    total_timesteps   | 27579691 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 8.83     |\n",
      "|    n_updates         | 6882422  |\n",
      "-----------------------------------\n",
      "Num timesteps: 27600000\n",
      "Best mean reward: -inf - Last mean reward per episode: 13453.00\n",
      "No more requests.\n",
      "Total reward for this episode is 22640.000000000015\n",
      "No more requests.\n",
      "Total reward for this episode is 16579.999999999975\n",
      "No more requests.\n",
      "Total reward for this episode is 20559.99999999999\n",
      "No more requests.\n",
      "Total reward for this episode is 36660.0\n",
      "-----------------------------------\n",
      "| average_route_length | 3.1      |\n",
      "| blocked_contiguous   | 0        |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0        |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.19e+04 |\n",
      "|    ep_rew_mean       | 1.4e+04  |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1372     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 49242    |\n",
      "|    total_timesteps   | 27669343 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00153  |\n",
      "|    n_updates         | 6904835  |\n",
      "-----------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -19859.99999999997\n",
      "No more requests.\n",
      "Total reward for this episode is 24879.99999999996\n",
      "No more requests.\n",
      "Total reward for this episode is 10599.999999999969\n",
      "No more requests.\n",
      "Total reward for this episode is 11940.000000000013\n",
      "-----------------------------------\n",
      "| average_route_length | 3.44     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.19e+04 |\n",
      "|    ep_rew_mean       | 1.36e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1376     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 49399    |\n",
      "|    total_timesteps   | 27757967 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000368 |\n",
      "|    n_updates         | 6926991  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 23379.99999999996\n",
      "Num timesteps: 27800000\n",
      "Best mean reward: -inf - Last mean reward per episode: 13794.80\n",
      "No more requests.\n",
      "Total reward for this episode is 22760.00000000002\n",
      "No more requests.\n",
      "Total reward for this episode is 6800.000000000028\n",
      "No more requests.\n",
      "Total reward for this episode is 10160.000000000025\n",
      "-----------------------------------\n",
      "| average_route_length | 3.67     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.19e+04 |\n",
      "|    ep_rew_mean       | 1.34e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1380     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 49555    |\n",
      "|    total_timesteps   | 27845976 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0003   |\n",
      "|    n_updates         | 6948993  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 10459.999999999976\n",
      "No more requests.\n",
      "Total reward for this episode is 21079.99999999996\n",
      "No more requests.\n",
      "Total reward for this episode is 22959.999999999956\n",
      "No more requests.\n",
      "Total reward for this episode is 21399.999999999956\n",
      "-----------------------------------\n",
      "| average_route_length | 3.47     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.19e+04 |\n",
      "|    ep_rew_mean       | 1.35e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1384     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 49712    |\n",
      "|    total_timesteps   | 27934016 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000172 |\n",
      "|    n_updates         | 6971003  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 8720.000000000022\n",
      "No more requests.\n",
      "Total reward for this episode is 7939.999999999984\n",
      "Num timesteps: 28000000\n",
      "Best mean reward: -inf - Last mean reward per episode: 13623.40\n",
      "No more requests.\n",
      "Total reward for this episode is -4299.999999999969\n",
      "No more requests.\n",
      "Total reward for this episode is 10380.00000000003\n",
      "-----------------------------------\n",
      "| average_route_length | 3.56     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.19e+04 |\n",
      "|    ep_rew_mean       | 1.31e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1388     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 49868    |\n",
      "|    total_timesteps   | 28022650 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000419 |\n",
      "|    n_updates         | 6993162  |\n",
      "-----------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 23419.99999999997\n",
      "No more requests.\n",
      "Total reward for this episode is 9539.999999999967\n",
      "No more requests.\n",
      "Total reward for this episode is 37459.99999999999\n",
      "No more requests.\n",
      "Total reward for this episode is 9600.00000000002\n",
      "-----------------------------------\n",
      "| average_route_length | 3.78     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.19e+04 |\n",
      "|    ep_rew_mean       | 1.33e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1392     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 50017    |\n",
      "|    total_timesteps   | 28105804 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 5.47e-05 |\n",
      "|    n_updates         | 7013950  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 10420.000000000024\n",
      "No more requests.\n",
      "Total reward for this episode is 23839.999999999924\n",
      "No more requests.\n",
      "Total reward for this episode is 10319.999999999927\n",
      "No more requests.\n",
      "Total reward for this episode is 9460.000000000033\n",
      "-----------------------------------\n",
      "| average_route_length | 3.67     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.18e+04 |\n",
      "|    ep_rew_mean       | 1.31e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1396     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 50173    |\n",
      "|    total_timesteps   | 28193159 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000129 |\n",
      "|    n_updates         | 7035789  |\n",
      "-----------------------------------\n",
      "Num timesteps: 28200000\n",
      "Best mean reward: -inf - Last mean reward per episode: 13065.40\n",
      "No more requests.\n",
      "Total reward for this episode is 6280.0000000000155\n",
      "No more requests.\n",
      "Total reward for this episode is 20780.0\n",
      "No more requests.\n",
      "Total reward for this episode is 19640.00000000002\n",
      "No more requests.\n",
      "Total reward for this episode is 24819.99999999995\n",
      "-----------------------------------\n",
      "| average_route_length | 3.26     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.18e+04 |\n",
      "|    ep_rew_mean       | 1.31e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1400     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 50330    |\n",
      "|    total_timesteps   | 28280059 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00523  |\n",
      "|    n_updates         | 7057514  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 7059.999999999963\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -7999.999999999926\n",
      "No more requests.\n",
      "Total reward for this episode is 11699.999999999935\n",
      "No more requests.\n",
      "Total reward for this episode is 22059.99999999996\n",
      "-----------------------------------\n",
      "| average_route_length | 3.58     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.17e+04 |\n",
      "|    ep_rew_mean       | 1.29e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1404     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 50470    |\n",
      "|    total_timesteps   | 28357901 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 4.55e-05 |\n",
      "|    n_updates         | 7076975  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 11540.000000000024\n",
      "Num timesteps: 28400000\n",
      "Best mean reward: -inf - Last mean reward per episode: 12955.80\n",
      "No more requests.\n",
      "Total reward for this episode is 13299.999999999965\n",
      "No more requests.\n",
      "Total reward for this episode is 35760.0\n",
      "No more requests.\n",
      "Total reward for this episode is 9860.000000000027\n",
      "-----------------------------------\n",
      "| average_route_length | 3.67     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.17e+04 |\n",
      "|    ep_rew_mean       | 1.29e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1408     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 50627    |\n",
      "|    total_timesteps   | 28444816 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 4.31     |\n",
      "|    n_updates         | 7098703  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 38279.999999999985\n",
      "No more requests.\n",
      "Total reward for this episode is 6180.000000000026\n",
      "No more requests.\n",
      "Total reward for this episode is 25579.999999999938\n",
      "No more requests.\n",
      "Total reward for this episode is 25299.999999999967\n",
      "-----------------------------------\n",
      "| average_route_length | 3.26     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.17e+04 |\n",
      "|    ep_rew_mean       | 1.35e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1412     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 50784    |\n",
      "|    total_timesteps   | 28531848 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 30       |\n",
      "|    n_updates         | 7120461  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 9980.000000000027\n",
      "No more requests.\n",
      "Total reward for this episode is 24139.99999999996\n",
      "No more requests.\n",
      "Total reward for this episode is -13819.999999999962\n",
      "Num timesteps: 28600000\n",
      "Best mean reward: -inf - Last mean reward per episode: 13815.60\n",
      "No more requests.\n",
      "Total reward for this episode is 6720.000000000022\n",
      "-----------------------------------\n",
      "| average_route_length | 4.11     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.19e+04 |\n",
      "|    ep_rew_mean       | 1.38e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1416     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 50940    |\n",
      "|    total_timesteps   | 28618496 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0817   |\n",
      "|    n_updates         | 7142123  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 10839.999999999973\n",
      "No more requests.\n",
      "Total reward for this episode is 9580.000000000033\n",
      "No more requests.\n",
      "Total reward for this episode is -1979.9999999999777\n",
      "No more requests.\n",
      "Total reward for this episode is 21839.99999999993\n",
      "-----------------------------------\n",
      "| average_route_length | 3.47     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.19e+04 |\n",
      "|    ep_rew_mean       | 1.35e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1420     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 51096    |\n",
      "|    total_timesteps   | 28705011 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000317 |\n",
      "|    n_updates         | 7163752  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 10059.99999999997\n",
      "No more requests.\n",
      "Total reward for this episode is 11419.999999999965\n",
      "No more requests.\n",
      "Total reward for this episode is 21059.999999999964\n",
      "No more requests.\n",
      "Total reward for this episode is 19139.99999999997\n",
      "-----------------------------------\n",
      "| average_route_length | 3.79     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.19e+04 |\n",
      "|    ep_rew_mean       | 1.33e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1424     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 51253    |\n",
      "|    total_timesteps   | 28791671 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000268 |\n",
      "|    n_updates         | 7185417  |\n",
      "-----------------------------------\n",
      "Num timesteps: 28800000\n",
      "Best mean reward: -inf - Last mean reward per episode: 13289.00\n",
      "No more requests.\n",
      "Total reward for this episode is 21459.999999999975\n",
      "No more requests.\n",
      "Total reward for this episode is 38579.99999999999\n",
      "No more requests.\n",
      "Total reward for this episode is 8959.99999999999\n",
      "No more requests.\n",
      "Total reward for this episode is 25819.99999999995\n",
      "-----------------------------------\n",
      "| average_route_length | 3.16     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.19e+04 |\n",
      "|    ep_rew_mean       | 1.39e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1428     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 51410    |\n",
      "|    total_timesteps   | 28878116 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0192   |\n",
      "|    n_updates         | 7207028  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 7040.000000000026\n",
      "No more requests.\n",
      "Total reward for this episode is 36460.0\n",
      "No more requests.\n",
      "Total reward for this episode is 4180.000000000057\n",
      "No more requests.\n",
      "Total reward for this episode is 24639.999999999964\n",
      "-----------------------------------\n",
      "| average_route_length | 3.26     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.19e+04 |\n",
      "|    ep_rew_mean       | 1.43e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1432     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 51568    |\n",
      "|    total_timesteps   | 28964762 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000283 |\n",
      "|    n_updates         | 7228690  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 24479.99999999997\n",
      "Num timesteps: 29000000\n",
      "Best mean reward: -inf - Last mean reward per episode: 14448.80\n",
      "No more requests.\n",
      "Total reward for this episode is 22699.999999999945\n",
      "No more requests.\n",
      "Total reward for this episode is 10320.000000000033\n",
      "No more requests.\n",
      "Total reward for this episode is 10179.999999999956\n",
      "-----------------------------------\n",
      "| average_route_length | 3.67     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.19e+04 |\n",
      "|    ep_rew_mean       | 1.47e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1436     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 51724    |\n",
      "|    total_timesteps   | 29050703 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0475   |\n",
      "|    n_updates         | 7250175  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 10800.000000000116\n",
      "No more requests.\n",
      "Total reward for this episode is 23339.999999999945\n",
      "No more requests.\n",
      "Total reward for this episode is 9719.999999999978\n",
      "No more requests.\n",
      "Total reward for this episode is 36820.0\n",
      "-----------------------------------\n",
      "| average_route_length | 3.2      |\n",
      "| blocked_contiguous   | 0        |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0        |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.18e+04 |\n",
      "|    ep_rew_mean       | 1.47e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1440     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 51881    |\n",
      "|    total_timesteps   | 29136676 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 1.63     |\n",
      "|    n_updates         | 7271668  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 23639.99999999994\n",
      "No more requests.\n",
      "Total reward for this episode is 24519.999999999935\n",
      "Num timesteps: 29200000\n",
      "Best mean reward: -inf - Last mean reward per episode: 14933.60\n",
      "No more requests.\n",
      "Total reward for this episode is 21320.00000000002\n",
      "No more requests.\n",
      "Total reward for this episode is 21019.999999999953\n",
      "-----------------------------------\n",
      "| average_route_length | 3.58     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.18e+04 |\n",
      "|    ep_rew_mean       | 1.54e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1444     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 52037    |\n",
      "|    total_timesteps   | 29222007 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000901 |\n",
      "|    n_updates         | 7293001  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 22699.999999999956\n",
      "No more requests.\n",
      "Total reward for this episode is 11099.999999999969\n",
      "No more requests.\n",
      "Total reward for this episode is 40519.99999999999\n",
      "No more requests.\n",
      "Total reward for this episode is 22759.999999999967\n",
      "-----------------------------------\n",
      "| average_route_length | 3.47     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.17e+04 |\n",
      "|    ep_rew_mean       | 1.6e+04  |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1448     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 52194    |\n",
      "|    total_timesteps   | 29307553 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 9.49e-05 |\n",
      "|    n_updates         | 7314388  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 21199.999999999964\n",
      "No more requests.\n",
      "Total reward for this episode is 25139.999999999964\n",
      "No more requests.\n",
      "Total reward for this episode is 24459.99999999994\n",
      "No more requests.\n",
      "Total reward for this episode is 9660.00000000002\n",
      "-----------------------------------\n",
      "| average_route_length | 3.78     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.17e+04 |\n",
      "|    ep_rew_mean       | 1.61e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1452     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 52351    |\n",
      "|    total_timesteps   | 29393552 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000348 |\n",
      "|    n_updates         | 7335887  |\n",
      "-----------------------------------\n",
      "Num timesteps: 29400000\n",
      "Best mean reward: -inf - Last mean reward per episode: 16086.00\n",
      "No more requests.\n",
      "Total reward for this episode is 21119.999999999967\n",
      "No more requests.\n",
      "Total reward for this episode is 10059.999999999985\n",
      "No more requests.\n",
      "Total reward for this episode is 21959.999999999945\n",
      "No more requests.\n",
      "Total reward for this episode is 33400.0\n",
      "-----------------------------------\n",
      "| average_route_length | 3.5      |\n",
      "| blocked_contiguous   | 0        |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0        |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.17e+04 |\n",
      "|    ep_rew_mean       | 1.61e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1456     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 52507    |\n",
      "|    total_timesteps   | 29478834 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 28.9     |\n",
      "|    n_updates         | 7357208  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -6299.999999999982\n",
      "No more requests.\n",
      "Total reward for this episode is 21400.00000000002\n",
      "No more requests.\n",
      "Total reward for this episode is 38399.99999999999\n",
      "No more requests.\n",
      "Total reward for this episode is 25279.999999999935\n",
      "-----------------------------------\n",
      "| average_route_length | 3.16     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.16e+04 |\n",
      "|    ep_rew_mean       | 1.63e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1460     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 52665    |\n",
      "|    total_timesteps   | 29564886 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000247 |\n",
      "|    n_updates         | 7378721  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -4259.999999999983\n",
      "Num timesteps: 29600000\n",
      "Best mean reward: -inf - Last mean reward per episode: 16151.60\n",
      "No more requests.\n",
      "Total reward for this episode is 12120.000000000024\n",
      "No more requests.\n",
      "Total reward for this episode is 32780.00000000001\n",
      "No more requests.\n",
      "Total reward for this episode is 35640.0\n",
      "-----------------------------------\n",
      "| average_route_length | 3.3      |\n",
      "| blocked_contiguous   | 0        |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0        |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.16e+04 |\n",
      "|    ep_rew_mean       | 1.64e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1464     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 52822    |\n",
      "|    total_timesteps   | 29650372 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 10.3     |\n",
      "|    n_updates         | 7400092  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 23759.99999999996\n",
      "No more requests.\n",
      "Total reward for this episode is 23019.999999999953\n",
      "No more requests.\n",
      "Total reward for this episode is -7379.9999999999345\n",
      "No more requests.\n",
      "Total reward for this episode is 7240.000000000024\n",
      "-----------------------------------\n",
      "| average_route_length | 3.89     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.16e+04 |\n",
      "|    ep_rew_mean       | 1.64e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1468     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 52979    |\n",
      "|    total_timesteps   | 29736191 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000533 |\n",
      "|    n_updates         | 7421547  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 24759.999999999956\n",
      "No more requests.\n",
      "Total reward for this episode is 23639.999999999956\n",
      "Num timesteps: 29800000\n",
      "Best mean reward: -inf - Last mean reward per episode: 16514.00\n",
      "No more requests.\n",
      "Total reward for this episode is -5119.9999999999945\n",
      "No more requests.\n",
      "Total reward for this episode is 10919.999999999978\n",
      "-----------------------------------\n",
      "| average_route_length | 3.56     |\n",
      "| blocked_contiguous   | 0.1      |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.1      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.15e+04 |\n",
      "|    ep_rew_mean       | 1.6e+04  |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1472     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 53136    |\n",
      "|    total_timesteps   | 29821576 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 8.44e-05 |\n",
      "|    n_updates         | 7442893  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 10000.000000000025\n",
      "No more requests.\n",
      "Total reward for this episode is -2340.0000000000177\n",
      "No more requests.\n",
      "Total reward for this episode is 24119.999999999964\n",
      "No more requests.\n",
      "Total reward for this episode is -1519.9999999999625\n",
      "-----------------------------------\n",
      "| average_route_length | 3.76     |\n",
      "| blocked_contiguous   | 0.15     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.15     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.15e+04 |\n",
      "|    ep_rew_mean       | 1.6e+04  |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1476     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 53293    |\n",
      "|    total_timesteps   | 29907314 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0765   |\n",
      "|    n_updates         | 7464328  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is 27339.99999999993\n",
      "No more requests.\n",
      "Total reward for this episode is 37760.0\n",
      "No more requests.\n",
      "Total reward for this episode is 22699.999999999945\n",
      "No more requests.\n",
      "Total reward for this episode is 23520.00000000002\n",
      "-----------------------------------\n",
      "| average_route_length | 3.37     |\n",
      "| blocked_contiguous   | 0.05     |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.05     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.15e+04 |\n",
      "|    ep_rew_mean       | 1.65e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1480     |\n",
      "|    fps               | 561      |\n",
      "|    time_elapsed      | 53451    |\n",
      "|    total_timesteps   | 29993052 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000951 |\n",
      "|    n_updates         | 7485762  |\n",
      "-----------------------------------\n",
      "Num timesteps: 30000000\n",
      "Best mean reward: -inf - Last mean reward per episode: 16509.00\n"
     ]
    }
   ],
   "source": [
    "# training agent on 30 000 000 timesteps\n",
    "model_name = \"DQN_VSNL_TL_5\"\n",
    "model.learn(total_timesteps=30000000, callback=[callback, tensor_callback])\n",
    "model.save(model_name)\n",
    "\n",
    "# tensorboard --logdir ./5_traffic_load_model_log/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traffic Load 15 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create log dir\n",
    "log_dir = os.path.join(os.getcwd(), \"15_Traffic_Load_Model_Log/\")\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VSNL Topology Selected\n",
      "Traffic load is: 15.0\n",
      "Using cuda device\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "# Create the callback: check every 1000 steps\n",
    "callback = SaveOnBestTrainingRewardCallback(check_freq=200000, log_dir=log_dir)\n",
    "tensor_callback = TensorboardCallback()\n",
    "\n",
    "#hold time for traffic load of 15\n",
    "Holdtime_TL_15 = 30\n",
    "\n",
    "# create model\n",
    "\n",
    "nodeList, linkList = createPresetTopology(\"VSNL\", num_slots=link_BW)\n",
    "requestList = generateRequests(nodeList, numberOfRequests=num_req, req_interval=req_int, hold_time=Holdtime_TL_15, time_limit=time_limit)\n",
    "\n",
    "user = User()\n",
    "env = game_gym(nodeList, linkList, requestList, user, dynamic=True)\n",
    "eveon = Monitor(env, log_dir)\n",
    "\n",
    "# check_env(eveon, warn=True)\n",
    "model = DQN('MlpPolicy', eveon, verbose=1, buffer_size=100000, device='cuda', \n",
    "learning_starts=50000, exploration_fraction=0.5, learning_rate=0.0001,\n",
    "gamma=0.8, tensorboard_log='./15_traffic_load_model_log/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./15_traffic_load_model_log/DQN_2\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -86800.00000000128\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -32179.999999999876\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -23739.999999999905\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -8239.999999999976\n",
      "------------------------------------\n",
      "| average_route_length | 5.14      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.42e+04  |\n",
      "|    ep_rew_mean       | -3.77e+04 |\n",
      "|    exploration_rate  | 0.994     |\n",
      "| time/                |           |\n",
      "|    episodes          | 4         |\n",
      "|    fps               | 1136      |\n",
      "|    time_elapsed      | 85        |\n",
      "|    total_timesteps   | 96769     |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0112    |\n",
      "|    n_updates         | 11692     |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -55240.0000000006\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -5499.999999999977\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -12599.999999999964\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -19279.999999999993\n",
      "------------------------------------\n",
      "| average_route_length | 5.14      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.87e+04  |\n",
      "|    ep_rew_mean       | -3.04e+04 |\n",
      "|    exploration_rate  | 0.991     |\n",
      "| time/                |           |\n",
      "|    episodes          | 8         |\n",
      "|    fps               | 945       |\n",
      "|    time_elapsed      | 158       |\n",
      "|    total_timesteps   | 149974    |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0659    |\n",
      "|    n_updates         | 24993     |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -10999.99999999999\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -4679.9999999999445\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -9879.999999999973\n",
      "Num timesteps: 200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -24407.27\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -43660.00000000002\n",
      "-----------------------------------\n",
      "| average_route_length | 4.57     |\n",
      "| blocked_contiguous   | 0.125    |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.65     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.68e+04 |\n",
      "|    ep_rew_mean       | -2.6e+04 |\n",
      "|    exploration_rate  | 0.987    |\n",
      "| time/                |          |\n",
      "|    episodes          | 12       |\n",
      "|    fps               | 892      |\n",
      "|    time_elapsed      | 226      |\n",
      "|    total_timesteps   | 201817   |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00413  |\n",
      "|    n_updates         | 37954    |\n",
      "-----------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -5419.999999999982\n",
      "Stop hitting yourself\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -77120.00000000109\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -8499.99999999997\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -58840.000000000386\n",
      "------------------------------------\n",
      "| average_route_length | 6.29      |\n",
      "| blocked_contiguous   | 0.111     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.63e+04  |\n",
      "|    ep_rew_mean       | -2.89e+04 |\n",
      "|    exploration_rate  | 0.983     |\n",
      "| time/                |           |\n",
      "|    episodes          | 16        |\n",
      "|    fps               | 863       |\n",
      "|    time_elapsed      | 301       |\n",
      "|    total_timesteps   | 260598    |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00217   |\n",
      "|    n_updates         | 52649     |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -25399.999999999938\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -7379.999999999987\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -6959.999999999984\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -5299.9999999999345\n",
      "------------------------------------\n",
      "| average_route_length | 4.67      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.55      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.56e+04  |\n",
      "|    ep_rew_mean       | -2.53e+04 |\n",
      "|    exploration_rate  | 0.98      |\n",
      "| time/                |           |\n",
      "|    episodes          | 20        |\n",
      "|    fps               | 848       |\n",
      "|    time_elapsed      | 367       |\n",
      "|    total_timesteps   | 311805    |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000765  |\n",
      "|    n_updates         | 65451     |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -15239.999999999969\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -22339.999999999993\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -20700.000000000007\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -23219.99999999997\n",
      "------------------------------------\n",
      "| average_route_length | 5.43      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.5e+04   |\n",
      "|    ep_rew_mean       | -2.45e+04 |\n",
      "|    exploration_rate  | 0.977     |\n",
      "| time/                |           |\n",
      "|    episodes          | 24        |\n",
      "|    fps               | 836       |\n",
      "|    time_elapsed      | 429       |\n",
      "|    total_timesteps   | 359778    |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00116   |\n",
      "|    n_updates         | 77444     |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 3740.0000000000687\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -23259.999999999945\n",
      "Num timesteps: 400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -23353.08\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -44020.00000000003\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -10619.999999999962\n",
      "------------------------------------\n",
      "| average_route_length | 4.75      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.6       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.51e+04  |\n",
      "|    ep_rew_mean       | -2.36e+04 |\n",
      "|    exploration_rate  | 0.973     |\n",
      "| time/                |           |\n",
      "|    episodes          | 28        |\n",
      "|    fps               | 826       |\n",
      "|    time_elapsed      | 510       |\n",
      "|    total_timesteps   | 421853    |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00269   |\n",
      "|    n_updates         | 92963     |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -11339.999999999964\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -22199.99999999993\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -24259.99999999995\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -12919.999999999953\n",
      "------------------------------------\n",
      "| average_route_length | 4.29      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.48e+04  |\n",
      "|    ep_rew_mean       | -2.29e+04 |\n",
      "|    exploration_rate  | 0.97      |\n",
      "| time/                |           |\n",
      "|    episodes          | 32        |\n",
      "|    fps               | 819       |\n",
      "|    time_elapsed      | 576       |\n",
      "|    total_timesteps   | 472884    |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000719  |\n",
      "|    n_updates         | 105720    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -18840.000000000004\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -21219.999999999953\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -3179.99999999997\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -1039.999999999995\n",
      "------------------------------------\n",
      "| average_route_length | 4.29      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.46e+04  |\n",
      "|    ep_rew_mean       | -2.16e+04 |\n",
      "|    exploration_rate  | 0.967     |\n",
      "| time/                |           |\n",
      "|    episodes          | 36        |\n",
      "|    fps               | 814       |\n",
      "|    time_elapsed      | 643       |\n",
      "|    total_timesteps   | 524321    |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0012    |\n",
      "|    n_updates         | 118580    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -8159.999999999955\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -55360.000000000386\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -6459.9999999999\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -12699.999999999978\n",
      "------------------------------------\n",
      "| average_route_length | 4.86      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.44e+04  |\n",
      "|    ep_rew_mean       | -2.15e+04 |\n",
      "|    exploration_rate  | 0.963     |\n",
      "| time/                |           |\n",
      "|    episodes          | 40        |\n",
      "|    fps               | 809       |\n",
      "|    time_elapsed      | 712       |\n",
      "|    total_timesteps   | 576897    |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0157    |\n",
      "|    n_updates         | 131724    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -47840.00000000051\n",
      "Num timesteps: 600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -22108.29\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -33679.99999999986\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -169899.9999999949\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -54140.000000000444\n",
      "------------------------------------\n",
      "| average_route_length | 5.75      |\n",
      "| blocked_contiguous   | 0.1       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.6       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.49e+04  |\n",
      "|    ep_rew_mean       | -2.65e+04 |\n",
      "|    exploration_rate  | 0.958     |\n",
      "| time/                |           |\n",
      "|    episodes          | 44        |\n",
      "|    fps               | 804       |\n",
      "|    time_elapsed      | 815       |\n",
      "|    total_timesteps   | 656680    |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.216     |\n",
      "|    n_updates         | 151669    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -18239.99999999997\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -19180.000000000015\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -7379.999999999983\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -7319.999999999954\n",
      "------------------------------------\n",
      "| average_route_length | 4.57      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.47e+04  |\n",
      "|    ep_rew_mean       | -2.53e+04 |\n",
      "|    exploration_rate  | 0.955     |\n",
      "| time/                |           |\n",
      "|    episodes          | 48        |\n",
      "|    fps               | 801       |\n",
      "|    time_elapsed      | 882       |\n",
      "|    total_timesteps   | 707716    |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000513  |\n",
      "|    n_updates         | 164428    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -7319.999999999982\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -6759.999999999965\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -8359.999999999982\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -15939.999999999978\n",
      "------------------------------------\n",
      "| average_route_length | 4.86      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.46e+04  |\n",
      "|    ep_rew_mean       | -2.41e+04 |\n",
      "|    exploration_rate  | 0.952     |\n",
      "| time/                |           |\n",
      "|    episodes          | 52        |\n",
      "|    fps               | 799       |\n",
      "|    time_elapsed      | 946       |\n",
      "|    total_timesteps   | 756625    |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0107    |\n",
      "|    n_updates         | 176656    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -25919.99999999994\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -45220.00000000005\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -9659.999999999996\n",
      "Num timesteps: 800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -24265.82\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -58300.00000000061\n",
      "------------------------------------\n",
      "| average_route_length | 5.5       |\n",
      "| blocked_contiguous   | 0.1       |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.6       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.45e+04  |\n",
      "|    ep_rew_mean       | -2.49e+04 |\n",
      "|    exploration_rate  | 0.949     |\n",
      "| time/                |           |\n",
      "|    episodes          | 56        |\n",
      "|    fps               | 796       |\n",
      "|    time_elapsed      | 1015      |\n",
      "|    total_timesteps   | 809553    |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00613   |\n",
      "|    n_updates         | 189888    |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -60360.000000000466\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -30979.999999999858\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -1939.999999999961\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -16819.99999999992\n",
      "-----------------------------------\n",
      "| average_route_length | 4.2      |\n",
      "| blocked_contiguous   | 0        |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.5      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.46e+04 |\n",
      "|    ep_rew_mean       | -2.5e+04 |\n",
      "|    exploration_rate  | 0.945    |\n",
      "| time/                |          |\n",
      "|    episodes          | 60       |\n",
      "|    fps               | 794      |\n",
      "|    time_elapsed      | 1099     |\n",
      "|    total_timesteps   | 873355   |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.122    |\n",
      "|    n_updates         | 205838   |\n",
      "-----------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -4999.999999999944\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -18339.99999999998\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -11900.000000000002\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -10480.000000000002\n",
      "------------------------------------\n",
      "| average_route_length | 5.71      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.43e+04  |\n",
      "|    ep_rew_mean       | -2.42e+04 |\n",
      "|    exploration_rate  | 0.942     |\n",
      "| time/                |           |\n",
      "|    episodes          | 64        |\n",
      "|    fps               | 793       |\n",
      "|    time_elapsed      | 1157      |\n",
      "|    total_timesteps   | 917683    |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00149   |\n",
      "|    n_updates         | 216920    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -2699.999999999976\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -18499.99999999996\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -195559.99999999237\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -6099.999999999972\n",
      "-----------------------------------\n",
      "| average_route_length | 4.86     |\n",
      "| blocked_contiguous   | 0        |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.65     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.44e+04 |\n",
      "|    ep_rew_mean       | -2.6e+04 |\n",
      "|    exploration_rate  | 0.938    |\n",
      "| time/                |          |\n",
      "|    episodes          | 68       |\n",
      "|    fps               | 791      |\n",
      "|    time_elapsed      | 1237     |\n",
      "|    total_timesteps   | 978644   |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0064   |\n",
      "|    n_updates         | 232160   |\n",
      "-----------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -40720.00000000007\n",
      "Num timesteps: 1000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -26253.33\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -13619.99999999999\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -43520.00000000009\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -12479.999999999996\n",
      "------------------------------------\n",
      "| average_route_length | 5.43      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.44e+04  |\n",
      "|    ep_rew_mean       | -2.61e+04 |\n",
      "|    exploration_rate  | 0.934     |\n",
      "| time/                |           |\n",
      "|    episodes          | 72        |\n",
      "|    fps               | 788       |\n",
      "|    time_elapsed      | 1311      |\n",
      "|    total_timesteps   | 1034657   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0456    |\n",
      "|    n_updates         | 246164    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -24799.99999999994\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -89780.00000000162\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -40540.000000000065\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -28859.999999999887\n",
      "------------------------------------\n",
      "| average_route_length | 4.73      |\n",
      "| blocked_contiguous   | 0.0769    |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.45      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.46e+04  |\n",
      "|    ep_rew_mean       | -2.72e+04 |\n",
      "|    exploration_rate  | 0.93      |\n",
      "| time/                |           |\n",
      "|    episodes          | 76        |\n",
      "|    fps               | 786       |\n",
      "|    time_elapsed      | 1407      |\n",
      "|    total_timesteps   | 1107425   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0091    |\n",
      "|    n_updates         | 264356    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -4219.999999999967\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -20079.999999999978\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -19920.0\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -14459.999999999995\n",
      "------------------------------------\n",
      "| average_route_length | 5.43      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.44e+04  |\n",
      "|    ep_rew_mean       | -2.65e+04 |\n",
      "|    exploration_rate  | 0.927     |\n",
      "| time/                |           |\n",
      "|    episodes          | 80        |\n",
      "|    fps               | 785       |\n",
      "|    time_elapsed      | 1470      |\n",
      "|    total_timesteps   | 1154944   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0246    |\n",
      "|    n_updates         | 276235    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -28159.999999999935\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -13899.999999999978\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -4359.99999999997\n",
      "Num timesteps: 1200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -26136.63\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -65640.00000000058\n",
      "------------------------------------\n",
      "| average_route_length | 6.86      |\n",
      "| blocked_contiguous   | 0.0769    |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.43e+04  |\n",
      "|    ep_rew_mean       | -2.66e+04 |\n",
      "|    exploration_rate  | 0.924     |\n",
      "| time/                |           |\n",
      "|    episodes          | 84        |\n",
      "|    fps               | 783       |\n",
      "|    time_elapsed      | 1538      |\n",
      "|    total_timesteps   | 1205019   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0114    |\n",
      "|    n_updates         | 288754    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -24479.999999999927\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -17080.00000000002\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -18439.99999999996\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -10379.999999999993\n",
      "------------------------------------\n",
      "| average_route_length | 5.43      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.42e+04  |\n",
      "|    ep_rew_mean       | -2.62e+04 |\n",
      "|    exploration_rate  | 0.921     |\n",
      "| time/                |           |\n",
      "|    episodes          | 88        |\n",
      "|    fps               | 781       |\n",
      "|    time_elapsed      | 1602      |\n",
      "|    total_timesteps   | 1253210   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0975    |\n",
      "|    n_updates         | 300802    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -22239.999999999964\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -20839.99999999998\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -2059.999999999971\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -40239.99999999995\n",
      "-----------------------------------\n",
      "| average_route_length | 5.43     |\n",
      "| blocked_contiguous   | 0.0769   |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.65     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.42e+04 |\n",
      "|    ep_rew_mean       | -2.6e+04 |\n",
      "|    exploration_rate  | 0.917    |\n",
      "| time/                |          |\n",
      "|    episodes          | 92       |\n",
      "|    fps               | 780      |\n",
      "|    time_elapsed      | 1669     |\n",
      "|    total_timesteps   | 1303450  |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00255  |\n",
      "|    n_updates         | 313362   |\n",
      "-----------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -9579.999999999996\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -29639.99999999986\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -11599.999999999955\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -81460.00000000134\n",
      "------------------------------------\n",
      "| average_route_length | 5.33      |\n",
      "| blocked_contiguous   | 0.154     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.4       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.43e+04  |\n",
      "|    ep_rew_mean       | -2.63e+04 |\n",
      "|    exploration_rate  | 0.913     |\n",
      "| time/                |           |\n",
      "|    episodes          | 96        |\n",
      "|    fps               | 778       |\n",
      "|    time_elapsed      | 1759      |\n",
      "|    total_timesteps   | 1370629   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0468    |\n",
      "|    n_updates         | 330157    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -9779.999999999993\n",
      "Num timesteps: 1400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -26102.68\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 2460.0000000000905\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -12779.999999999989\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -47140.00000000002\n",
      "------------------------------------\n",
      "| average_route_length | 6.29      |\n",
      "| blocked_contiguous   | 0.0769    |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.43e+04  |\n",
      "|    ep_rew_mean       | -2.59e+04 |\n",
      "|    exploration_rate  | 0.91      |\n",
      "| time/                |           |\n",
      "|    episodes          | 100       |\n",
      "|    fps               | 777       |\n",
      "|    time_elapsed      | 1835      |\n",
      "|    total_timesteps   | 1427340   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00424   |\n",
      "|    n_updates         | 344334    |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -68520.00000000074\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -35919.999999999854\n",
      "Stop hitting yourself\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -32499.999999999876\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -18239.999999999978\n",
      "------------------------------------\n",
      "| average_route_length | 4.57      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.39e+04  |\n",
      "|    ep_rew_mean       | -2.59e+04 |\n",
      "|    exploration_rate  | 0.906     |\n",
      "| time/                |           |\n",
      "|    episodes          | 104       |\n",
      "|    fps               | 776       |\n",
      "|    time_elapsed      | 1921      |\n",
      "|    total_timesteps   | 1491517   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0021    |\n",
      "|    n_updates         | 360379    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -24199.999999999935\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -3719.9999999999504\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -115940.00000000143\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -125340.00000000038\n",
      "------------------------------------\n",
      "| average_route_length | 6         |\n",
      "| blocked_contiguous   | 0.214     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.55      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.41e+04  |\n",
      "|    ep_rew_mean       | -2.77e+04 |\n",
      "|    exploration_rate  | 0.901     |\n",
      "| time/                |           |\n",
      "|    episodes          | 108       |\n",
      "|    fps               | 775       |\n",
      "|    time_elapsed      | 2016      |\n",
      "|    total_timesteps   | 1562549   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.088     |\n",
      "|    n_updates         | 378137    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -66760.00000000093\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -33539.9999999999\n",
      "Num timesteps: 1600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -28546.60\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -20559.999999999975\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -19299.99999999998\n",
      "------------------------------------\n",
      "| average_route_length | 4.86      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.42e+04  |\n",
      "|    ep_rew_mean       | -2.84e+04 |\n",
      "|    exploration_rate  | 0.897     |\n",
      "| time/                |           |\n",
      "|    episodes          | 112       |\n",
      "|    fps               | 773       |\n",
      "|    time_elapsed      | 2099      |\n",
      "|    total_timesteps   | 1623325   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00326   |\n",
      "|    n_updates         | 393331    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -75480.00000000095\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -16699.99999999996\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -53900.000000000386\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -32859.99999999988\n",
      "------------------------------------\n",
      "| average_route_length | 4.89      |\n",
      "| blocked_contiguous   | 0.0714    |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.55      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.42e+04  |\n",
      "|    ep_rew_mean       | -2.87e+04 |\n",
      "|    exploration_rate  | 0.894     |\n",
      "| time/                |           |\n",
      "|    episodes          | 116       |\n",
      "|    fps               | 772       |\n",
      "|    time_elapsed      | 2175      |\n",
      "|    total_timesteps   | 1679740   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00151   |\n",
      "|    n_updates         | 407434    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -61860.000000000786\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -12440.000000000011\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -16119.999999999947\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -41680.000000000044\n",
      "------------------------------------\n",
      "| average_route_length | 4.89      |\n",
      "| blocked_contiguous   | 0.0714    |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.55      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.42e+04  |\n",
      "|    ep_rew_mean       | -2.96e+04 |\n",
      "|    exploration_rate  | 0.89      |\n",
      "| time/                |           |\n",
      "|    episodes          | 120       |\n",
      "|    fps               | 771       |\n",
      "|    time_elapsed      | 2249      |\n",
      "|    total_timesteps   | 1734893   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00228   |\n",
      "|    n_updates         | 421223    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -20820.000000000015\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -83060.00000000112\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -55640.00000000055\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -4439.999999999981\n",
      "------------------------------------\n",
      "| average_route_length | 4.57      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.43e+04  |\n",
      "|    ep_rew_mean       | -3.04e+04 |\n",
      "|    exploration_rate  | 0.887     |\n",
      "| time/                |           |\n",
      "|    episodes          | 124       |\n",
      "|    fps               | 770       |\n",
      "|    time_elapsed      | 2318      |\n",
      "|    total_timesteps   | 1785925   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00181   |\n",
      "|    n_updates         | 433981    |\n",
      "------------------------------------\n",
      "Num timesteps: 1800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -30395.60\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -35779.99999999988\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -21019.999999999935\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -79560.00000000105\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -27679.999999999945\n",
      "------------------------------------\n",
      "| average_route_length | 6         |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.42e+04  |\n",
      "|    ep_rew_mean       | -3.13e+04 |\n",
      "|    exploration_rate  | 0.883     |\n",
      "| time/                |           |\n",
      "|    episodes          | 128       |\n",
      "|    fps               | 769       |\n",
      "|    time_elapsed      | 2392      |\n",
      "|    total_timesteps   | 1840496   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00234   |\n",
      "|    n_updates         | 447623    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -3939.999999999975\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 3240.000000000009\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -34219.99999999979\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -137599.99999999907\n",
      "------------------------------------\n",
      "| average_route_length | 5.67      |\n",
      "| blocked_contiguous   | 0.286     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.4       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.44e+04  |\n",
      "|    ep_rew_mean       | -3.23e+04 |\n",
      "|    exploration_rate  | 0.879     |\n",
      "| time/                |           |\n",
      "|    episodes          | 132       |\n",
      "|    fps               | 768       |\n",
      "|    time_elapsed      | 2492      |\n",
      "|    total_timesteps   | 1914417   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00432   |\n",
      "|    n_updates         | 466104    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -3799.9999999999545\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -10239.999999999973\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -44460.00000000013\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -30279.999999999836\n",
      "------------------------------------\n",
      "| average_route_length | 4.5       |\n",
      "| blocked_contiguous   | 0.0714    |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.4       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.45e+04  |\n",
      "|    ep_rew_mean       | -3.28e+04 |\n",
      "|    exploration_rate  | 0.875     |\n",
      "| time/                |           |\n",
      "|    episodes          | 136       |\n",
      "|    fps               | 766       |\n",
      "|    time_elapsed      | 2572      |\n",
      "|    total_timesteps   | 1973141   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0102    |\n",
      "|    n_updates         | 480785    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -123460.00000000019\n",
      "Num timesteps: 2000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -33910.40\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -28939.99999999989\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -36899.99999999992\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -21879.9999999999\n",
      "-----------------------------------\n",
      "| average_route_length | 4.44     |\n",
      "| blocked_contiguous   | 0        |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.55     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.46e+04 |\n",
      "|    ep_rew_mean       | -3.4e+04 |\n",
      "|    exploration_rate  | 0.871    |\n",
      "| time/                |          |\n",
      "|    episodes          | 140      |\n",
      "|    fps               | 765      |\n",
      "|    time_elapsed      | 2660     |\n",
      "|    total_timesteps   | 2037315  |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00292  |\n",
      "|    n_updates         | 496828   |\n",
      "-----------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -14859.999999999975\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -7899.9999999999945\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -46400.000000000146\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -11939.999999999993\n",
      "------------------------------------\n",
      "| average_route_length | 5.43      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.43e+04  |\n",
      "|    ep_rew_mean       | -3.18e+04 |\n",
      "|    exploration_rate  | 0.868     |\n",
      "| time/                |           |\n",
      "|    episodes          | 144       |\n",
      "|    fps               | 764       |\n",
      "|    time_elapsed      | 2725      |\n",
      "|    total_timesteps   | 2084030   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00727   |\n",
      "|    n_updates         | 508507    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -21020.000000000004\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -20479.999999999956\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -69880.00000000081\n",
      "Stop hitting yourself\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -62960.00000000057\n",
      "-----------------------------------\n",
      "| average_route_length | 5.5      |\n",
      "| blocked_contiguous   | 0.0714   |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.4      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.44e+04 |\n",
      "|    ep_rew_mean       | -3.3e+04 |\n",
      "|    exploration_rate  | 0.864    |\n",
      "| time/                |          |\n",
      "|    episodes          | 148      |\n",
      "|    fps               | 763      |\n",
      "|    time_elapsed      | 2816     |\n",
      "|    total_timesteps   | 2150656  |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00872  |\n",
      "|    n_updates         | 525163   |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -186559.99999999377\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -4599.999999999983\n",
      "Num timesteps: 2200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -34796.40\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -84860.00000000138\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -12079.999999999976\n",
      "------------------------------------\n",
      "| average_route_length | 5.25      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.6       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.47e+04  |\n",
      "|    ep_rew_mean       | -3.55e+04 |\n",
      "|    exploration_rate  | 0.859     |\n",
      "| time/                |           |\n",
      "|    episodes          | 152       |\n",
      "|    fps               | 762       |\n",
      "|    time_elapsed      | 2917      |\n",
      "|    total_timesteps   | 2223516   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00743   |\n",
      "|    n_updates         | 543378    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -21359.999999999956\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -81480.00000000153\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -5279.99999999995\n",
      "Stop hitting yourself\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -56300.00000000043\n",
      "------------------------------------\n",
      "| average_route_length | 6.67      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.7       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.47e+04  |\n",
      "|    ep_rew_mean       | -3.58e+04 |\n",
      "|    exploration_rate  | 0.856     |\n",
      "| time/                |           |\n",
      "|    episodes          | 156       |\n",
      "|    fps               | 760       |\n",
      "|    time_elapsed      | 2996      |\n",
      "|    total_timesteps   | 2280213   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 28.9      |\n",
      "|    n_updates         | 557553    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -45320.000000000146\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -3239.9999999999864\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -188099.9999999934\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -93200.00000000178\n",
      "-----------------------------------\n",
      "| average_route_length | 6.22     |\n",
      "| blocked_contiguous   | 0.125    |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.55     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.48e+04 |\n",
      "|    ep_rew_mean       | -3.8e+04 |\n",
      "|    exploration_rate  | 0.851    |\n",
      "| time/                |          |\n",
      "|    episodes          | 160      |\n",
      "|    fps               | 759      |\n",
      "|    time_elapsed      | 3093     |\n",
      "|    total_timesteps   | 2349726  |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00195  |\n",
      "|    n_updates         | 574931   |\n",
      "-----------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -16780.000000000004\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -15719.999999999965\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -35839.9999999999\n",
      "Num timesteps: 2400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -38310.20\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -294619.9999999831\n",
      "------------------------------------\n",
      "| average_route_length | 8         |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.45      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.5e+04   |\n",
      "|    ep_rew_mean       | -4.11e+04 |\n",
      "|    exploration_rate  | 0.847     |\n",
      "| time/                |           |\n",
      "|    episodes          | 164       |\n",
      "|    fps               | 758       |\n",
      "|    time_elapsed      | 3194      |\n",
      "|    total_timesteps   | 2422417   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0614    |\n",
      "|    n_updates         | 593104    |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -335239.99999997835\n",
      "Stop hitting yourself\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -40820.00000000006\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -40900.000000000065\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -72040.0000000008\n",
      "------------------------------------\n",
      "| average_route_length | 6.86      |\n",
      "| blocked_contiguous   | 0.105     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.52e+04  |\n",
      "|    ep_rew_mean       | -4.37e+04 |\n",
      "|    exploration_rate  | 0.841     |\n",
      "| time/                |           |\n",
      "|    episodes          | 168       |\n",
      "|    fps               | 756       |\n",
      "|    time_elapsed      | 3307      |\n",
      "|    total_timesteps   | 2503248   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00431   |\n",
      "|    n_updates         | 613311    |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -195399.9999999925\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -12919.999999999982\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -8199.999999999965\n",
      "Stop hitting yourself\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -99580.00000000154\n",
      "------------------------------------\n",
      "| average_route_length | 5.17      |\n",
      "| blocked_contiguous   | 0.105     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.4       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.54e+04  |\n",
      "|    ep_rew_mean       | -4.58e+04 |\n",
      "|    exploration_rate  | 0.837     |\n",
      "| time/                |           |\n",
      "|    episodes          | 172       |\n",
      "|    fps               | 755       |\n",
      "|    time_elapsed      | 3414      |\n",
      "|    total_timesteps   | 2579646   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00488   |\n",
      "|    n_updates         | 632411    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -21839.99999999993\n",
      "Num timesteps: 2600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -45747.20\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -69200.00000000092\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -24739.99999999993\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -142539.99999999854\n",
      "------------------------------------\n",
      "| average_route_length | 7.09      |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.45      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.54e+04  |\n",
      "|    ep_rew_mean       | -4.65e+04 |\n",
      "|    exploration_rate  | 0.832     |\n",
      "| time/                |           |\n",
      "|    episodes          | 176       |\n",
      "|    fps               | 754       |\n",
      "|    time_elapsed      | 3511      |\n",
      "|    total_timesteps   | 2649017   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00878   |\n",
      "|    n_updates         | 649754    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -9139.999999999896\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -168299.99999999598\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -9079.999999999945\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -36499.999999999905\n",
      "------------------------------------\n",
      "| average_route_length | 5.75      |\n",
      "| blocked_contiguous   | 0.0526    |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.6       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.56e+04  |\n",
      "|    ep_rew_mean       | -4.82e+04 |\n",
      "|    exploration_rate  | 0.828     |\n",
      "| time/                |           |\n",
      "|    episodes          | 180       |\n",
      "|    fps               | 753       |\n",
      "|    time_elapsed      | 3609      |\n",
      "|    total_timesteps   | 2717969   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0213    |\n",
      "|    n_updates         | 666992    |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -145879.99999999837\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -11419.999999999945\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -41779.99999999999\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -9220.000000000002\n",
      "------------------------------------\n",
      "| average_route_length | 5.6       |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.75      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.57e+04  |\n",
      "|    ep_rew_mean       | -4.91e+04 |\n",
      "|    exploration_rate  | 0.824     |\n",
      "| time/                |           |\n",
      "|    episodes          | 184       |\n",
      "|    fps               | 752       |\n",
      "|    time_elapsed      | 3689      |\n",
      "|    total_timesteps   | 2774585   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00916   |\n",
      "|    n_updates         | 681146    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -9359.999999999964\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -58020.00000000064\n",
      "Num timesteps: 2800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -49384.20\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -249899.99999998749\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -63300.000000000466\n",
      "------------------------------------\n",
      "| average_route_length | 5.75      |\n",
      "| blocked_contiguous   | 0.105     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.6       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.59e+04  |\n",
      "|    ep_rew_mean       | -5.22e+04 |\n",
      "|    exploration_rate  | 0.82      |\n",
      "| time/                |           |\n",
      "|    episodes          | 188       |\n",
      "|    fps               | 751       |\n",
      "|    time_elapsed      | 3783      |\n",
      "|    total_timesteps   | 2841688   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 7.55      |\n",
      "|    n_updates         | 697921    |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -254319.9999999872\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -4799.999999999956\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -8499.99999999993\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -5179.999999999955\n",
      "------------------------------------\n",
      "| average_route_length | 4.29      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.6e+04   |\n",
      "|    ep_rew_mean       | -5.41e+04 |\n",
      "|    exploration_rate  | 0.816     |\n",
      "| time/                |           |\n",
      "|    episodes          | 192       |\n",
      "|    fps               | 749       |\n",
      "|    time_elapsed      | 3875      |\n",
      "|    total_timesteps   | 2906276   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00156   |\n",
      "|    n_updates         | 714068    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -93500.00000000154\n",
      "No more requests.\n",
      "Total reward for this episode is -217619.99999999072\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -91580.00000000157\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -89740.0000000015\n",
      "------------------------------------\n",
      "| average_route_length | 6.75      |\n",
      "| blocked_contiguous   | 0.105     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.6       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.62e+04  |\n",
      "|    ep_rew_mean       | -5.77e+04 |\n",
      "|    exploration_rate  | 0.81      |\n",
      "| time/                |           |\n",
      "|    episodes          | 196       |\n",
      "|    fps               | 748       |\n",
      "|    time_elapsed      | 4000      |\n",
      "|    total_timesteps   | 2994798   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00895   |\n",
      "|    n_updates         | 736199    |\n",
      "------------------------------------\n",
      "Num timesteps: 3000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -57720.60\n",
      "No more requests.\n",
      "Total reward for this episode is -107060.0000000019\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -31319.999999999935\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -48260.000000000306\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -14219.999999999993\n",
      "------------------------------------\n",
      "| average_route_length | 5.43      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.64e+04  |\n",
      "|    ep_rew_mean       | -5.91e+04 |\n",
      "|    exploration_rate  | 0.806     |\n",
      "| time/                |           |\n",
      "|    episodes          | 200       |\n",
      "|    fps               | 747       |\n",
      "|    time_elapsed      | 4099      |\n",
      "|    total_timesteps   | 3064706   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0167    |\n",
      "|    n_updates         | 753676    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -50460.00000000036\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -15359.999999999962\n",
      "Stop hitting yourself\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -164879.99999999584\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -18859.999999999964\n",
      "-----------------------------------\n",
      "| average_route_length | 4.29     |\n",
      "| blocked_contiguous   | 0        |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.65     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.64e+04 |\n",
      "|    ep_rew_mean       | -6e+04   |\n",
      "|    exploration_rate  | 0.802    |\n",
      "| time/                |          |\n",
      "|    episodes          | 204      |\n",
      "|    fps               | 746      |\n",
      "|    time_elapsed      | 4191     |\n",
      "|    total_timesteps   | 3129441  |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0186   |\n",
      "|    n_updates         | 769860   |\n",
      "-----------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -100640.00000000176\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -31459.999999999753\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -10819.999999999985\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -5139.999999999988\n",
      "------------------------------------\n",
      "| average_route_length | 4.86      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.63e+04  |\n",
      "|    ep_rew_mean       | -5.88e+04 |\n",
      "|    exploration_rate  | 0.798     |\n",
      "| time/                |           |\n",
      "|    episodes          | 208       |\n",
      "|    fps               | 745       |\n",
      "|    time_elapsed      | 4278      |\n",
      "|    total_timesteps   | 3190486   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0105    |\n",
      "|    n_updates         | 785121    |\n",
      "------------------------------------\n",
      "Num timesteps: 3200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -58794.80\n",
      "Stop hitting yourself\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -151339.99999999852\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -15839.999999999967\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -46600.00000000033\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -14919.999999999945\n",
      "------------------------------------\n",
      "| average_route_length | 4         |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.63e+04  |\n",
      "|    ep_rew_mean       | -5.97e+04 |\n",
      "|    exploration_rate  | 0.794     |\n",
      "| time/                |           |\n",
      "|    episodes          | 212       |\n",
      "|    fps               | 744       |\n",
      "|    time_elapsed      | 4366      |\n",
      "|    total_timesteps   | 3252057   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00718   |\n",
      "|    n_updates         | 800514    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -18100.000000000007\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -36499.999999999876\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -101160.00000000176\n",
      "Stop hitting yourself\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -39180.0\n",
      "------------------------------------\n",
      "| average_route_length | 4.33      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.4       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.64e+04  |\n",
      "|    ep_rew_mean       | -5.98e+04 |\n",
      "|    exploration_rate  | 0.79      |\n",
      "| time/                |           |\n",
      "|    episodes          | 216       |\n",
      "|    fps               | 743       |\n",
      "|    time_elapsed      | 4458      |\n",
      "|    total_timesteps   | 3316528   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0156    |\n",
      "|    n_updates         | 816631    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -14019.999999999973\n",
      "No more requests.\n",
      "Total reward for this episode is -210599.99999999205\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -30919.999999999847\n",
      "Stop hitting yourself\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -227559.99999999057\n",
      "------------------------------------\n",
      "| average_route_length | 8.67      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.55      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.66e+04  |\n",
      "|    ep_rew_mean       | -6.34e+04 |\n",
      "|    exploration_rate  | 0.785     |\n",
      "| time/                |           |\n",
      "|    episodes          | 220       |\n",
      "|    fps               | 742       |\n",
      "|    time_elapsed      | 4575      |\n",
      "|    total_timesteps   | 3398344   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00658   |\n",
      "|    n_updates         | 837085    |\n",
      "------------------------------------\n",
      "Num timesteps: 3400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -63355.80\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -42400.00000000014\n",
      "Stop hitting yourself\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -119980.00000000147\n",
      "Stop hitting yourself\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -60660.00000000046\n",
      "Stop hitting yourself\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -91660.00000000124\n",
      "------------------------------------\n",
      "| average_route_length | 6.8       |\n",
      "| blocked_contiguous   | 0.105     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.5       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.69e+04  |\n",
      "|    ep_rew_mean       | -6.49e+04 |\n",
      "|    exploration_rate  | 0.78      |\n",
      "| time/                |           |\n",
      "|    episodes          | 224       |\n",
      "|    fps               | 741       |\n",
      "|    time_elapsed      | 4691      |\n",
      "|    total_timesteps   | 3479736   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00818   |\n",
      "|    n_updates         | 857433    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -114900.00000000201\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -102680.00000000179\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -41540.000000000015\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 13180.000000000055\n",
      "------------------------------------\n",
      "| average_route_length | 3.71      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.71e+04  |\n",
      "|    ep_rew_mean       | -6.57e+04 |\n",
      "|    exploration_rate  | 0.775     |\n",
      "| time/                |           |\n",
      "|    episodes          | 228       |\n",
      "|    fps               | 740       |\n",
      "|    time_elapsed      | 4798      |\n",
      "|    total_timesteps   | 3554347   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.007     |\n",
      "|    n_updates         | 876086    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -35719.99999999987\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -15699.999999999945\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -38539.99999999993\n",
      "Num timesteps: 3600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -66232.60\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -25939.999999999804\n",
      "------------------------------------\n",
      "| average_route_length | 4.17      |\n",
      "| blocked_contiguous   | 0.0526    |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.4       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.7e+04   |\n",
      "|    ep_rew_mean       | -6.51e+04 |\n",
      "|    exploration_rate  | 0.771     |\n",
      "| time/                |           |\n",
      "|    episodes          | 232       |\n",
      "|    fps               | 739       |\n",
      "|    time_elapsed      | 4883      |\n",
      "|    total_timesteps   | 3613508   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00235   |\n",
      "|    n_updates         | 890876    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -6239.999999999949\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -45280.000000000175\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -106480.00000000188\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -240839.99999998778\n",
      "------------------------------------\n",
      "| average_route_length | 7.33      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.4       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.71e+04  |\n",
      "|    ep_rew_mean       | -6.81e+04 |\n",
      "|    exploration_rate  | 0.767     |\n",
      "| time/                |           |\n",
      "|    episodes          | 236       |\n",
      "|    fps               | 738       |\n",
      "|    time_elapsed      | 4988      |\n",
      "|    total_timesteps   | 3686553   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00589   |\n",
      "|    n_updates         | 909138    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 1600.0000000000057\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -49180.00000000029\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -12259.999999999945\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -25959.999999999887\n",
      "------------------------------------\n",
      "| average_route_length | 4.36      |\n",
      "| blocked_contiguous   | 0.0526    |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.45      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.71e+04  |\n",
      "|    ep_rew_mean       | -6.69e+04 |\n",
      "|    exploration_rate  | 0.763     |\n",
      "| time/                |           |\n",
      "|    episodes          | 240       |\n",
      "|    fps               | 738       |\n",
      "|    time_elapsed      | 5071      |\n",
      "|    total_timesteps   | 3743950   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0025    |\n",
      "|    n_updates         | 923487    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -15939.999999999975\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -6859.999999999971\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -4179.999999999988\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -33139.99999999982\n",
      "------------------------------------\n",
      "| average_route_length | 4.67      |\n",
      "| blocked_contiguous   | 0.0526    |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.4       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.71e+04  |\n",
      "|    ep_rew_mean       | -6.67e+04 |\n",
      "|    exploration_rate  | 0.76      |\n",
      "| time/                |           |\n",
      "|    episodes          | 244       |\n",
      "|    fps               | 737       |\n",
      "|    time_elapsed      | 5146      |\n",
      "|    total_timesteps   | 3795775   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00436   |\n",
      "|    n_updates         | 936443    |\n",
      "------------------------------------\n",
      "Num timesteps: 3800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -66653.60\n",
      "No more requests.\n",
      "Total reward for this episode is -149739.99999999706\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -91720.00000000137\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -12019.999999999955\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -20239.99999999983\n",
      "------------------------------------\n",
      "| average_route_length | 4.31      |\n",
      "| blocked_contiguous   | 0.0526    |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.72e+04  |\n",
      "|    ep_rew_mean       | -6.77e+04 |\n",
      "|    exploration_rate  | 0.755     |\n",
      "| time/                |           |\n",
      "|    episodes          | 248       |\n",
      "|    fps               | 736       |\n",
      "|    time_elapsed      | 5258      |\n",
      "|    total_timesteps   | 3873515   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 7.08      |\n",
      "|    n_updates         | 955878    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -14199.99999999997\n",
      "Stop hitting yourself\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -189279.9999999944\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -61340.000000000546\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -39859.99999999997\n",
      "------------------------------------\n",
      "| average_route_length | 5.75      |\n",
      "| blocked_contiguous   | 0.0526    |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.6       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.72e+04  |\n",
      "|    ep_rew_mean       | -6.78e+04 |\n",
      "|    exploration_rate  | 0.75      |\n",
      "| time/                |           |\n",
      "|    episodes          | 252       |\n",
      "|    fps               | 735       |\n",
      "|    time_elapsed      | 5358      |\n",
      "|    total_timesteps   | 3942831   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 8.41      |\n",
      "|    n_updates         | 973207    |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -2559.999999999971\n",
      "No more requests.\n",
      "Total reward for this episode is -198439.9999999923\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -13139.999999999958\n",
      "Num timesteps: 4000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -68879.00\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -17359.999999999964\n",
      "------------------------------------\n",
      "| average_route_length | 4.29      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.72e+04  |\n",
      "|    ep_rew_mean       | -6.85e+04 |\n",
      "|    exploration_rate  | 0.746     |\n",
      "| time/                |           |\n",
      "|    episodes          | 256       |\n",
      "|    fps               | 734       |\n",
      "|    time_elapsed      | 5448      |\n",
      "|    total_timesteps   | 4004311   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00578   |\n",
      "|    n_updates         | 988577    |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -217599.99999999005\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -14219.999999999993\n",
      "No more requests.\n",
      "Total reward for this episode is -165659.99999999552\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -178079.9999999952\n",
      "------------------------------------\n",
      "| average_route_length | 7.2       |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.5       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.74e+04  |\n",
      "|    ep_rew_mean       | -7.09e+04 |\n",
      "|    exploration_rate  | 0.741     |\n",
      "| time/                |           |\n",
      "|    episodes          | 260       |\n",
      "|    fps               | 733       |\n",
      "|    time_elapsed      | 5572      |\n",
      "|    total_timesteps   | 4090269   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00286   |\n",
      "|    n_updates         | 1010067   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -179259.99999999415\n",
      "No more requests.\n",
      "Total reward for this episode is -195579.99999999328\n",
      "Stop hitting yourself\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -66180.00000000083\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -7619.999999999987\n",
      "------------------------------------\n",
      "| average_route_length | 5.33      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.7       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.75e+04  |\n",
      "|    ep_rew_mean       | -7.19e+04 |\n",
      "|    exploration_rate  | 0.736     |\n",
      "| time/                |           |\n",
      "|    episodes          | 264       |\n",
      "|    fps               | 733       |\n",
      "|    time_elapsed      | 5689      |\n",
      "|    total_timesteps   | 4170681   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00514   |\n",
      "|    n_updates         | 1030170   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -83580.00000000105\n",
      "Num timesteps: 4200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -69386.20\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -77920.00000000116\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -54800.00000000025\n",
      "Stop hitting yourself\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -145699.99999999802\n",
      "------------------------------------\n",
      "| average_route_length | 6         |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.4       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.75e+04  |\n",
      "|    ep_rew_mean       | -7.06e+04 |\n",
      "|    exploration_rate  | 0.731     |\n",
      "| time/                |           |\n",
      "|    episodes          | 268       |\n",
      "|    fps               | 732       |\n",
      "|    time_elapsed      | 5806      |\n",
      "|    total_timesteps   | 4251637   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0032    |\n",
      "|    n_updates         | 1050409   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -227899.99999999008\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -228819.99999998984\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -151519.9999999968\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -11799.99999999995\n",
      "------------------------------------\n",
      "| average_route_length | 4         |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.76e+04  |\n",
      "|    ep_rew_mean       | -7.37e+04 |\n",
      "|    exploration_rate  | 0.725     |\n",
      "| time/                |           |\n",
      "|    episodes          | 272       |\n",
      "|    fps               | 731       |\n",
      "|    time_elapsed      | 5937      |\n",
      "|    total_timesteps   | 4341708   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 29.4      |\n",
      "|    n_updates         | 1072926   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -233159.9999999883\n",
      "No more requests.\n",
      "Total reward for this episode is -125140.00000000012\n",
      "Num timesteps: 4400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -76367.20\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -198759.99999999214\n",
      "No more requests.\n",
      "Total reward for this episode is -225699.99999999\n",
      "-----------------------------------\n",
      "| average_route_length | 7.83     |\n",
      "| blocked_contiguous   | 0.421    |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.4      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.8e+04  |\n",
      "|    ep_rew_mean       | -7.9e+04 |\n",
      "|    exploration_rate  | 0.718    |\n",
      "| time/                |          |\n",
      "|    episodes          | 276      |\n",
      "|    fps               | 730      |\n",
      "|    time_elapsed      | 6093     |\n",
      "|    total_timesteps   | 4448517  |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00265  |\n",
      "|    n_updates         | 1099629  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -139199.99999999834\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -52140.00000000042\n",
      "No more requests.\n",
      "Total reward for this episode is -168159.99999999543\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -115900.00000000122\n",
      "------------------------------------\n",
      "| average_route_length | 6.55      |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.45      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.82e+04  |\n",
      "|    ep_rew_mean       | -8.15e+04 |\n",
      "|    exploration_rate  | 0.713     |\n",
      "| time/                |           |\n",
      "|    episodes          | 280       |\n",
      "|    fps               | 729       |\n",
      "|    time_elapsed      | 6222      |\n",
      "|    total_timesteps   | 4536673   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00538   |\n",
      "|    n_updates         | 1121668   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -33099.9999999999\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -153079.9999999975\n",
      "Num timesteps: 4600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -81777.60\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -56800.0000000003\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -257979.99999998696\n",
      "------------------------------------\n",
      "| average_route_length | 8.36      |\n",
      "| blocked_contiguous   | 0.421     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.45      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.85e+04  |\n",
      "|    ep_rew_mean       | -8.44e+04 |\n",
      "|    exploration_rate  | 0.707     |\n",
      "| time/                |           |\n",
      "|    episodes          | 284       |\n",
      "|    fps               | 728       |\n",
      "|    time_elapsed      | 6354      |\n",
      "|    total_timesteps   | 4627384   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 16.3      |\n",
      "|    n_updates         | 1144345   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -86520.00000000127\n",
      "Stop hitting yourself\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -118700.00000000114\n",
      "Stop hitting yourself\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -126639.99999999985\n",
      "No more requests.\n",
      "Total reward for this episode is -158679.9999999968\n",
      "------------------------------------\n",
      "| average_route_length | 5.71      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.87e+04  |\n",
      "|    ep_rew_mean       | -8.55e+04 |\n",
      "|    exploration_rate  | 0.701     |\n",
      "| time/                |           |\n",
      "|    episodes          | 288       |\n",
      "|    fps               | 727       |\n",
      "|    time_elapsed      | 6481      |\n",
      "|    total_timesteps   | 4713971   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0206    |\n",
      "|    n_updates         | 1165992   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -29939.99999999988\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -87020.00000000122\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -89740.00000000131\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -128119.9999999995\n",
      "------------------------------------\n",
      "| average_route_length | 6.2       |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.5       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.88e+04  |\n",
      "|    ep_rew_mean       | -8.61e+04 |\n",
      "|    exploration_rate  | 0.697     |\n",
      "| time/                |           |\n",
      "|    episodes          | 292       |\n",
      "|    fps               | 726       |\n",
      "|    time_elapsed      | 6587      |\n",
      "|    total_timesteps   | 4786033   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00533   |\n",
      "|    n_updates         | 1184008   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -63900.0000000005\n",
      "Num timesteps: 4800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -85839.20\n",
      "Stop hitting yourself\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -173359.99999999537\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -164679.99999999485\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -170579.99999999566\n",
      "------------------------------------\n",
      "| average_route_length | 8         |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.55      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.88e+04  |\n",
      "|    ep_rew_mean       | -8.69e+04 |\n",
      "|    exploration_rate  | 0.692     |\n",
      "| time/                |           |\n",
      "|    episodes          | 296       |\n",
      "|    fps               | 725       |\n",
      "|    time_elapsed      | 6712      |\n",
      "|    total_timesteps   | 4870899   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00238   |\n",
      "|    n_updates         | 1205224   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -7299.999999999963\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -55420.000000000575\n",
      "Stop hitting yourself\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -129360.00000000009\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -14339.999999999964\n",
      "-----------------------------------\n",
      "| average_route_length | 4.57     |\n",
      "| blocked_contiguous   | 0        |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.65     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 1.87e+04 |\n",
      "|    ep_rew_mean       | -8.7e+04 |\n",
      "|    exploration_rate  | 0.688    |\n",
      "| time/                |          |\n",
      "|    episodes          | 300      |\n",
      "|    fps               | 725      |\n",
      "|    time_elapsed      | 6802     |\n",
      "|    total_timesteps   | 4932061  |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00145  |\n",
      "|    n_updates         | 1220515  |\n",
      "-----------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -74920.00000000093\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -67940.00000000083\n",
      "No more requests.\n",
      "Total reward for this episode is -153439.99999999756\n",
      "Num timesteps: 5000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -87642.00\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -19320.000000000004\n",
      "------------------------------------\n",
      "| average_route_length | 5.43      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.87e+04  |\n",
      "|    ep_rew_mean       | -8.76e+04 |\n",
      "|    exploration_rate  | 0.683     |\n",
      "| time/                |           |\n",
      "|    episodes          | 304       |\n",
      "|    fps               | 724       |\n",
      "|    time_elapsed      | 6904      |\n",
      "|    total_timesteps   | 5001267   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00291   |\n",
      "|    n_updates         | 1237816   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -139479.99999999846\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -25039.999999999767\n",
      "No more requests.\n",
      "Total reward for this episode is -179339.9999999951\n",
      "Stop hitting yourself\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -110280.00000000192\n",
      "------------------------------------\n",
      "| average_route_length | 5.83      |\n",
      "| blocked_contiguous   | 0.158     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.4       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.91e+04  |\n",
      "|    ep_rew_mean       | -9.07e+04 |\n",
      "|    exploration_rate  | 0.677     |\n",
      "| time/                |           |\n",
      "|    episodes          | 308       |\n",
      "|    fps               | 723       |\n",
      "|    time_elapsed      | 7048      |\n",
      "|    total_timesteps   | 5098618   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00494   |\n",
      "|    n_updates         | 1262154   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -14679.999999999969\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -27119.999999999858\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -137879.99999999892\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -210699.99999999168\n",
      "------------------------------------\n",
      "| average_route_length | 6.83      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.4       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.92e+04  |\n",
      "|    ep_rew_mean       | -9.23e+04 |\n",
      "|    exploration_rate  | 0.672     |\n",
      "| time/                |           |\n",
      "|    episodes          | 312       |\n",
      "|    fps               | 722       |\n",
      "|    time_elapsed      | 7160      |\n",
      "|    total_timesteps   | 5174211   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00558   |\n",
      "|    n_updates         | 1281052   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -36619.99999999995\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -17899.999999999993\n",
      "Num timesteps: 5200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -92334.60\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -6059.999999999988\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -132619.99999999997\n",
      "------------------------------------\n",
      "| average_route_length | 6         |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.4       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.91e+04  |\n",
      "|    ep_rew_mean       | -9.23e+04 |\n",
      "|    exploration_rate  | 0.669     |\n",
      "| time/                |           |\n",
      "|    episodes          | 316       |\n",
      "|    fps               | 721       |\n",
      "|    time_elapsed      | 7245      |\n",
      "|    total_timesteps   | 5231177   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 15.2      |\n",
      "|    n_updates         | 1295294   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -10279.999999999955\n",
      "No more requests.\n",
      "Total reward for this episode is -141599.9999999977\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -2659.9999999999795\n",
      "No more requests.\n",
      "Total reward for this episode is -128099.99999999983\n",
      "------------------------------------\n",
      "| average_route_length | 5.87      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.91e+04  |\n",
      "|    ep_rew_mean       | -9.03e+04 |\n",
      "|    exploration_rate  | 0.664     |\n",
      "| time/                |           |\n",
      "|    episodes          | 320       |\n",
      "|    fps               | 721       |\n",
      "|    time_elapsed      | 7356      |\n",
      "|    total_timesteps   | 5304723   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0131    |\n",
      "|    n_updates         | 1313680   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -93300.00000000144\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -166739.99999999636\n",
      "Stop hitting yourself\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -107880.00000000178\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -143359.9999999988\n",
      "------------------------------------\n",
      "| average_route_length | 5.43      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.91e+04  |\n",
      "|    ep_rew_mean       | -9.23e+04 |\n",
      "|    exploration_rate  | 0.658     |\n",
      "| time/                |           |\n",
      "|    episodes          | 324       |\n",
      "|    fps               | 720       |\n",
      "|    time_elapsed      | 7491      |\n",
      "|    total_timesteps   | 5394420   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00659   |\n",
      "|    n_updates         | 1336104   |\n",
      "------------------------------------\n",
      "Num timesteps: 5400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -92288.40\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -204959.99999999313\n",
      "Stop hitting yourself\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -57640.000000000466\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 1280.0000000000637\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -8519.999999999993\n",
      "------------------------------------\n",
      "| average_route_length | 5.2       |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.75      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.9e+04   |\n",
      "|    ep_rew_mean       | -9.25e+04 |\n",
      "|    exploration_rate  | 0.654     |\n",
      "| time/                |           |\n",
      "|    episodes          | 328       |\n",
      "|    fps               | 719       |\n",
      "|    time_elapsed      | 7588      |\n",
      "|    total_timesteps   | 5459152   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00249   |\n",
      "|    n_updates         | 1352287   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -165699.9999999963\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -242419.99999998897\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -178059.99999999584\n",
      "No more requests.\n",
      "Total reward for this episode is -149239.9999999982\n",
      "------------------------------------\n",
      "| average_route_length | 6.29      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.95e+04  |\n",
      "|    ep_rew_mean       | -9.86e+04 |\n",
      "|    exploration_rate  | 0.648     |\n",
      "| time/                |           |\n",
      "|    episodes          | 332       |\n",
      "|    fps               | 718       |\n",
      "|    time_elapsed      | 7747      |\n",
      "|    total_timesteps   | 5565052   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00858   |\n",
      "|    n_updates         | 1378762   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -143519.99999999924\n",
      "Num timesteps: 5600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -100022.00\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 8140.000000000057\n",
      "Stop hitting yourself\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -59040.000000000284\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -134719.99999999974\n",
      "------------------------------------\n",
      "| average_route_length | 6.55      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.45      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.96e+04  |\n",
      "|    ep_rew_mean       | -9.81e+04 |\n",
      "|    exploration_rate  | 0.643     |\n",
      "| time/                |           |\n",
      "|    episodes          | 336       |\n",
      "|    fps               | 717       |\n",
      "|    time_elapsed      | 7865      |\n",
      "|    total_timesteps   | 5643767   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00821   |\n",
      "|    n_updates         | 1398441   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -4319.999999999976\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -201119.99999999368\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -97100.00000000148\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -30319.99999999987\n",
      "------------------------------------\n",
      "| average_route_length | 4.55      |\n",
      "| blocked_contiguous   | 0.0526    |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.45      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 1.97e+04  |\n",
      "|    ep_rew_mean       | -1.01e+05 |\n",
      "|    exploration_rate  | 0.638     |\n",
      "| time/                |           |\n",
      "|    episodes          | 340       |\n",
      "|    fps               | 716       |\n",
      "|    time_elapsed      | 7977      |\n",
      "|    total_timesteps   | 5718134   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00128   |\n",
      "|    n_updates         | 1417033   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -78060.00000000109\n",
      "No more requests.\n",
      "Total reward for this episode is -110160.00000000163\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -15139.999999999984\n",
      "Stop hitting yourself\n",
      "Num timesteps: 5800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -102302.80\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -151459.9999999985\n",
      "------------------------------------\n",
      "| average_route_length | 5.82      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.45      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.01e+04  |\n",
      "|    ep_rew_mean       | -1.03e+05 |\n",
      "|    exploration_rate  | 0.632     |\n",
      "| time/                |           |\n",
      "|    episodes          | 344       |\n",
      "|    fps               | 715       |\n",
      "|    time_elapsed      | 8107      |\n",
      "|    total_timesteps   | 5804406   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00991   |\n",
      "|    n_updates         | 1438601   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -114080.00000000202\n",
      "No more requests.\n",
      "Total reward for this episode is -141859.99999999904\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -161779.99999999686\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -45939.99999999998\n",
      "------------------------------------\n",
      "| average_route_length | 4.73      |\n",
      "| blocked_contiguous   | 0.105     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.45      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.03e+04  |\n",
      "|    ep_rew_mean       | -1.05e+05 |\n",
      "|    exploration_rate  | 0.626     |\n",
      "| time/                |           |\n",
      "|    episodes          | 348       |\n",
      "|    fps               | 715       |\n",
      "|    time_elapsed      | 8252      |\n",
      "|    total_timesteps   | 5900318   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00274   |\n",
      "|    n_updates         | 1462579   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -8159.999999999996\n",
      "No more requests.\n",
      "Total reward for this episode is -140979.99999999948\n",
      "No more requests.\n",
      "Total reward for this episode is -75520.00000000096\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -182239.99999999482\n",
      "------------------------------------\n",
      "| average_route_length | 6.77      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.04e+04  |\n",
      "|    ep_rew_mean       | -1.06e+05 |\n",
      "|    exploration_rate  | 0.621     |\n",
      "| time/                |           |\n",
      "|    episodes          | 352       |\n",
      "|    fps               | 714       |\n",
      "|    time_elapsed      | 8380      |\n",
      "|    total_timesteps   | 5985187   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 312       |\n",
      "|    n_updates         | 1483796   |\n",
      "------------------------------------\n",
      "Num timesteps: 6000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -106435.60\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -44419.999999999935\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -160559.9999999966\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -71480.00000000089\n",
      "No more requests.\n",
      "Total reward for this episode is -135179.9999999994\n",
      "------------------------------------\n",
      "| average_route_length | 5.57      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.07e+04  |\n",
      "|    ep_rew_mean       | -1.08e+05 |\n",
      "|    exploration_rate  | 0.615     |\n",
      "| time/                |           |\n",
      "|    episodes          | 356       |\n",
      "|    fps               | 713       |\n",
      "|    time_elapsed      | 8517      |\n",
      "|    total_timesteps   | 6075733   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00992   |\n",
      "|    n_updates         | 1506433   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -78840.00000000109\n",
      "No more requests.\n",
      "Total reward for this episode is -138179.99999999913\n",
      "No more requests.\n",
      "Total reward for this episode is -140979.999999999\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -71920.00000000087\n",
      "------------------------------------\n",
      "| average_route_length | 5.17      |\n",
      "| blocked_contiguous   | 0.158     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.4       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.08e+04  |\n",
      "|    ep_rew_mean       | -1.07e+05 |\n",
      "|    exploration_rate  | 0.609     |\n",
      "| time/                |           |\n",
      "|    episodes          | 360       |\n",
      "|    fps               | 712       |\n",
      "|    time_elapsed      | 8659      |\n",
      "|    total_timesteps   | 6169737   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00592   |\n",
      "|    n_updates         | 1529934   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -166979.9999999969\n",
      "Num timesteps: 6200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -106674.60\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -85160.00000000121\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -133719.99999999904\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -170819.99999999566\n",
      "------------------------------------\n",
      "| average_route_length | 6.46      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.1e+04   |\n",
      "|    ep_rew_mean       | -1.08e+05 |\n",
      "|    exploration_rate  | 0.603     |\n",
      "| time/                |           |\n",
      "|    episodes          | 364       |\n",
      "|    fps               | 711       |\n",
      "|    time_elapsed      | 8807      |\n",
      "|    total_timesteps   | 6267153   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00702   |\n",
      "|    n_updates         | 1554288   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -182399.99999999494\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -161319.99999999697\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -20279.999999999996\n",
      "Stop hitting yourself\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -70960.0000000006\n",
      "------------------------------------\n",
      "| average_route_length | 5.09      |\n",
      "| blocked_contiguous   | 0.105     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.45      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.1e+04   |\n",
      "|    ep_rew_mean       | -1.09e+05 |\n",
      "|    exploration_rate  | 0.598     |\n",
      "| time/                |           |\n",
      "|    episodes          | 368       |\n",
      "|    fps               | 710       |\n",
      "|    time_elapsed      | 8929      |\n",
      "|    total_timesteps   | 6346766   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.159     |\n",
      "|    n_updates         | 1574191   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -156459.99999999657\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -167979.99999999523\n",
      "Num timesteps: 6400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -107185.20\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -51080.00000000005\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -6979.999999999997\n",
      "------------------------------------\n",
      "| average_route_length | 4.8       |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.75      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.08e+04  |\n",
      "|    ep_rew_mean       | -1.06e+05 |\n",
      "|    exploration_rate  | 0.593     |\n",
      "| time/                |           |\n",
      "|    episodes          | 372       |\n",
      "|    fps               | 710       |\n",
      "|    time_elapsed      | 9041      |\n",
      "|    total_timesteps   | 6419695   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00646   |\n",
      "|    n_updates         | 1592423   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -16779.999999999978\n",
      "No more requests.\n",
      "Total reward for this episode is -133979.9999999994\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -8720.000000000004\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -134299.99999999933\n",
      "------------------------------------\n",
      "| average_route_length | 5.86      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.04e+04  |\n",
      "|    ep_rew_mean       | -1.01e+05 |\n",
      "|    exploration_rate  | 0.589     |\n",
      "| time/                |           |\n",
      "|    episodes          | 376       |\n",
      "|    fps               | 709       |\n",
      "|    time_elapsed      | 9151      |\n",
      "|    total_timesteps   | 6491169   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0154    |\n",
      "|    n_updates         | 1610292   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -152259.99999999697\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -222679.99999999092\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -195619.99999999374\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -176659.99999999543\n",
      "------------------------------------\n",
      "| average_route_length | 7.5       |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.4       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.05e+04  |\n",
      "|    ep_rew_mean       | -1.04e+05 |\n",
      "|    exploration_rate  | 0.583     |\n",
      "| time/                |           |\n",
      "|    episodes          | 380       |\n",
      "|    fps               | 707       |\n",
      "|    time_elapsed      | 9310      |\n",
      "|    total_timesteps   | 6590642   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00303   |\n",
      "|    n_updates         | 1635160   |\n",
      "------------------------------------\n",
      "Num timesteps: 6600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -103856.00\n",
      "No more requests.\n",
      "Total reward for this episode is -154419.99999999738\n",
      "No more requests.\n",
      "Total reward for this episode is -188599.99999999462\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -179499.99999999537\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -30839.999999999985\n",
      "------------------------------------\n",
      "| average_route_length | 6         |\n",
      "| blocked_contiguous   | 0.0526    |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.05e+04  |\n",
      "|    ep_rew_mean       | -1.04e+05 |\n",
      "|    exploration_rate  | 0.577     |\n",
      "| time/                |           |\n",
      "|    episodes          | 384       |\n",
      "|    fps               | 707       |\n",
      "|    time_elapsed      | 9447      |\n",
      "|    total_timesteps   | 6679480   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0118    |\n",
      "|    n_updates         | 1657369   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -154679.99999999691\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -102400.00000000162\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -186139.9999999947\n",
      "Stop hitting yourself\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -72900.00000000074\n",
      "------------------------------------\n",
      "| average_route_length | 6.6       |\n",
      "| blocked_contiguous   | 0.105     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.5       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.06e+04  |\n",
      "|    ep_rew_mean       | -1.05e+05 |\n",
      "|    exploration_rate  | 0.571     |\n",
      "| time/                |           |\n",
      "|    episodes          | 388       |\n",
      "|    fps               | 706       |\n",
      "|    time_elapsed      | 9591      |\n",
      "|    total_timesteps   | 6773012   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0101    |\n",
      "|    n_updates         | 1680752   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -180619.99999999537\n",
      "Num timesteps: 6800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -106060.00\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -155619.99999999726\n",
      "No more requests.\n",
      "Total reward for this episode is -96600.00000000151\n",
      "Stop hitting yourself\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -38400.00000000002\n",
      "------------------------------------\n",
      "| average_route_length | 4         |\n",
      "| blocked_contiguous   | 0.0526    |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.08e+04  |\n",
      "|    ep_rew_mean       | -1.06e+05 |\n",
      "|    exploration_rate  | 0.565     |\n",
      "| time/                |           |\n",
      "|    episodes          | 392       |\n",
      "|    fps               | 705       |\n",
      "|    time_elapsed      | 9742      |\n",
      "|    total_timesteps   | 6870927   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00498   |\n",
      "|    n_updates         | 1705231   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -147199.99999999764\n",
      "No more requests.\n",
      "Total reward for this episode is -144559.99999999866\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -9359.999999999996\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -148719.99999999825\n",
      "------------------------------------\n",
      "| average_route_length | 6         |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.08e+04  |\n",
      "|    ep_rew_mean       | -1.05e+05 |\n",
      "|    exploration_rate  | 0.56      |\n",
      "| time/                |           |\n",
      "|    episodes          | 396       |\n",
      "|    fps               | 704       |\n",
      "|    time_elapsed      | 9866      |\n",
      "|    total_timesteps   | 6951153   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0234    |\n",
      "|    n_updates         | 1725288   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -97000.00000000146\n",
      "No more requests.\n",
      "Total reward for this episode is -99620.00000000146\n",
      "Num timesteps: 7000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -106057.60\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -127559.99999999965\n",
      "No more requests.\n",
      "Total reward for this episode is -152299.99999999753\n",
      "------------------------------------\n",
      "| average_route_length | 6.15      |\n",
      "| blocked_contiguous   | 0.368     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.12e+04  |\n",
      "|    ep_rew_mean       | -1.07e+05 |\n",
      "|    exploration_rate  | 0.554     |\n",
      "| time/                |           |\n",
      "|    episodes          | 400       |\n",
      "|    fps               | 703       |\n",
      "|    time_elapsed      | 10015     |\n",
      "|    total_timesteps   | 7048443   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0122    |\n",
      "|    n_updates         | 1749610   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -159519.9999999972\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -123980.00000000026\n",
      "No more requests.\n",
      "Total reward for this episode is -147199.99999999793\n",
      "No more requests.\n",
      "Total reward for this episode is -151219.99999999776\n",
      "-----------------------------------\n",
      "| average_route_length | 6.62     |\n",
      "| blocked_contiguous   | 0.368    |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.35     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.14e+04 |\n",
      "|    ep_rew_mean       | -1.1e+05 |\n",
      "|    exploration_rate  | 0.548    |\n",
      "| time/                |          |\n",
      "|    episodes          | 404      |\n",
      "|    fps               | 702      |\n",
      "|    time_elapsed      | 10162    |\n",
      "|    total_timesteps   | 7143579  |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0153   |\n",
      "|    n_updates         | 1773394  |\n",
      "-----------------------------------\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -183719.99999999435\n",
      "No more requests.\n",
      "Total reward for this episode is -135259.99999999866\n",
      "Num timesteps: 7200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -111555.40\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -157719.99999999642\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 2240.000000000009\n",
      "-----------------------------------\n",
      "| average_route_length | 4.44     |\n",
      "| blocked_contiguous   | 0        |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.55     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.13e+04 |\n",
      "|    ep_rew_mean       | -1.1e+05 |\n",
      "|    exploration_rate  | 0.542    |\n",
      "| time/                |          |\n",
      "|    episodes          | 408      |\n",
      "|    fps               | 702      |\n",
      "|    time_elapsed      | 10301    |\n",
      "|    total_timesteps   | 7232434  |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00986  |\n",
      "|    n_updates         | 1795608  |\n",
      "-----------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -149479.99999999825\n",
      "Stop hitting yourself\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -51060.00000000006\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -153299.99999999747\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -98760.00000000138\n",
      "------------------------------------\n",
      "| average_route_length | 5.33      |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.15e+04  |\n",
      "|    ep_rew_mean       | -1.11e+05 |\n",
      "|    exploration_rate  | 0.536     |\n",
      "| time/                |           |\n",
      "|    episodes          | 412       |\n",
      "|    fps               | 701       |\n",
      "|    time_elapsed      | 10443     |\n",
      "|    total_timesteps   | 7324558   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00801   |\n",
      "|    n_updates         | 1818639   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -146219.9999999984\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -150719.99999999802\n",
      "Stop hitting yourself\n",
      "Num timesteps: 7400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -113282.80\n",
      "No more requests.\n",
      "Total reward for this episode is -125440.0000000001\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -183839.99999999494\n",
      "------------------------------------\n",
      "| average_route_length | 7.33      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.4       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.19e+04  |\n",
      "|    ep_rew_mean       | -1.15e+05 |\n",
      "|    exploration_rate  | 0.53      |\n",
      "| time/                |           |\n",
      "|    episodes          | 416       |\n",
      "|    fps               | 700       |\n",
      "|    time_elapsed      | 10601     |\n",
      "|    total_timesteps   | 7426099   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0127    |\n",
      "|    n_updates         | 1844024   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -34599.99999999983\n",
      "No more requests.\n",
      "Total reward for this episode is -133159.99999999886\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -7159.999999999941\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -207599.99999999226\n",
      "------------------------------------\n",
      "| average_route_length | 8.36      |\n",
      "| blocked_contiguous   | 0.368     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.45      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.2e+04   |\n",
      "|    ep_rew_mean       | -1.16e+05 |\n",
      "|    exploration_rate  | 0.525     |\n",
      "| time/                |           |\n",
      "|    episodes          | 420       |\n",
      "|    fps               | 699       |\n",
      "|    time_elapsed      | 10721     |\n",
      "|    total_timesteps   | 7503633   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.537     |\n",
      "|    n_updates         | 1863408   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -89140.00000000131\n",
      "No more requests.\n",
      "Total reward for this episode is -152359.99999999724\n",
      "No more requests.\n",
      "Total reward for this episode is -120400.00000000109\n",
      "Num timesteps: 7600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -115844.80\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -173959.99999999627\n",
      "------------------------------------\n",
      "| average_route_length | 7.5       |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.4       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.21e+04  |\n",
      "|    ep_rew_mean       | -1.16e+05 |\n",
      "|    exploration_rate  | 0.518     |\n",
      "| time/                |           |\n",
      "|    episodes          | 424       |\n",
      "|    fps               | 699       |\n",
      "|    time_elapsed      | 10876     |\n",
      "|    total_timesteps   | 7602745   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 8.07      |\n",
      "|    n_updates         | 1888186   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -123880.00000000013\n",
      "No more requests.\n",
      "Total reward for this episode is -145699.99999999854\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -152459.99999999793\n",
      "Stop hitting yourself\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -41880.00000000013\n",
      "------------------------------------\n",
      "| average_route_length | 4.67      |\n",
      "| blocked_contiguous   | 0.0526    |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.4       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.24e+04  |\n",
      "|    ep_rew_mean       | -1.18e+05 |\n",
      "|    exploration_rate  | 0.513     |\n",
      "| time/                |           |\n",
      "|    episodes          | 428       |\n",
      "|    fps               | 698       |\n",
      "|    time_elapsed      | 11024     |\n",
      "|    total_timesteps   | 7696482   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00182   |\n",
      "|    n_updates         | 1911620   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -170859.99999999645\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -5919.999999999996\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -149299.99999999764\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -143999.9999999991\n",
      "------------------------------------\n",
      "| average_route_length | 6.31      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.21e+04  |\n",
      "|    ep_rew_mean       | -1.15e+05 |\n",
      "|    exploration_rate  | 0.507     |\n",
      "| time/                |           |\n",
      "|    episodes          | 432       |\n",
      "|    fps               | 697       |\n",
      "|    time_elapsed      | 11154     |\n",
      "|    total_timesteps   | 7779631   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0309    |\n",
      "|    n_updates         | 1932407   |\n",
      "------------------------------------\n",
      "Num timesteps: 7800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -115340.80\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -163439.99999999793\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -29079.999999999916\n",
      "No more requests.\n",
      "Total reward for this episode is -70560.0000000007\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -142559.9999999994\n",
      "------------------------------------\n",
      "| average_route_length | 6.31      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.22e+04  |\n",
      "|    ep_rew_mean       | -1.16e+05 |\n",
      "|    exploration_rate  | 0.502     |\n",
      "| time/                |           |\n",
      "|    episodes          | 436       |\n",
      "|    fps               | 696       |\n",
      "|    time_elapsed      | 11293     |\n",
      "|    total_timesteps   | 7868353   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.039     |\n",
      "|    n_updates         | 1954588   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -166539.99999999697\n",
      "No more requests.\n",
      "Total reward for this episode is -88980.00000000135\n",
      "No more requests.\n",
      "Total reward for this episode is -168599.99999999627\n",
      "Stop hitting yourself\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -87520.00000000106\n",
      "------------------------------------\n",
      "| average_route_length | 6         |\n",
      "| blocked_contiguous   | 0.158     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.45      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.25e+04  |\n",
      "|    ep_rew_mean       | -1.18e+05 |\n",
      "|    exploration_rate  | 0.496     |\n",
      "| time/                |           |\n",
      "|    episodes          | 440       |\n",
      "|    fps               | 695       |\n",
      "|    time_elapsed      | 11442     |\n",
      "|    total_timesteps   | 7963149   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0499    |\n",
      "|    n_updates         | 1978287   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -117540.00000000153\n",
      "Num timesteps: 8000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -118205.80\n",
      "No more requests.\n",
      "Total reward for this episode is -111560.00000000144\n",
      "No more requests.\n",
      "Total reward for this episode is -121240.00000000058\n",
      "No more requests.\n",
      "Total reward for this episode is -163879.99999999726\n",
      "------------------------------------\n",
      "| average_route_length | 7         |\n",
      "| blocked_contiguous   | 0.421     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.4       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.26e+04  |\n",
      "|    ep_rew_mean       | -1.19e+05 |\n",
      "|    exploration_rate  | 0.489     |\n",
      "| time/                |           |\n",
      "|    episodes          | 444       |\n",
      "|    fps               | 695       |\n",
      "|    time_elapsed      | 11598     |\n",
      "|    total_timesteps   | 8062636   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00674   |\n",
      "|    n_updates         | 2003158   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -117200.00000000122\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -96240.00000000146\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -166739.99999999703\n",
      "No more requests.\n",
      "Total reward for this episode is -135899.99999999997\n",
      "-----------------------------------\n",
      "| average_route_length | 5.85     |\n",
      "| blocked_contiguous   | 0.368    |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.35     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.26e+04 |\n",
      "|    ep_rew_mean       | -1.2e+05 |\n",
      "|    exploration_rate  | 0.483    |\n",
      "| time/                |          |\n",
      "|    episodes          | 448      |\n",
      "|    fps               | 694      |\n",
      "|    time_elapsed      | 11754    |\n",
      "|    total_timesteps   | 8161664  |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0237   |\n",
      "|    n_updates         | 2027915  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -137639.99999999948\n",
      "Num timesteps: 8200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -121246.40\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -113800.0000000017\n",
      "No more requests.\n",
      "Total reward for this episode is -67080.00000000067\n",
      "No more requests.\n",
      "Total reward for this episode is -162519.99999999724\n",
      "------------------------------------\n",
      "| average_route_length | 6.83      |\n",
      "| blocked_contiguous   | 0.421     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.4       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.28e+04  |\n",
      "|    ep_rew_mean       | -1.21e+05 |\n",
      "|    exploration_rate  | 0.477     |\n",
      "| time/                |           |\n",
      "|    episodes          | 452       |\n",
      "|    fps               | 693       |\n",
      "|    time_elapsed      | 11911     |\n",
      "|    total_timesteps   | 8260357   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00425   |\n",
      "|    n_updates         | 2052589   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -162999.99999999756\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -141419.9999999996\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -188159.99999999543\n",
      "No more requests.\n",
      "Total reward for this episode is -131999.99999999983\n",
      "------------------------------------\n",
      "| average_route_length | 5.85      |\n",
      "| blocked_contiguous   | 0.368     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.28e+04  |\n",
      "|    ep_rew_mean       | -1.23e+05 |\n",
      "|    exploration_rate  | 0.471     |\n",
      "| time/                |           |\n",
      "|    episodes          | 456       |\n",
      "|    fps               | 692       |\n",
      "|    time_elapsed      | 12069     |\n",
      "|    total_timesteps   | 8360040   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0274    |\n",
      "|    n_updates         | 2077509   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -142619.9999999991\n",
      "Num timesteps: 8400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -123267.00\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -158099.99999999808\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -130959.99999999965\n",
      "No more requests.\n",
      "Total reward for this episode is -115520.00000000068\n",
      "------------------------------------\n",
      "| average_route_length | 5.57      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.29e+04  |\n",
      "|    ep_rew_mean       | -1.24e+05 |\n",
      "|    exploration_rate  | 0.464     |\n",
      "| time/                |           |\n",
      "|    episodes          | 460       |\n",
      "|    fps               | 691       |\n",
      "|    time_elapsed      | 12227     |\n",
      "|    total_timesteps   | 8459752   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0096    |\n",
      "|    n_updates         | 2102437   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -135199.99999999904\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -9359.999999999996\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -82360.0000000008\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -177539.99999999657\n",
      "------------------------------------\n",
      "| average_route_length | 7.64      |\n",
      "| blocked_contiguous   | 0.421     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.45      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.27e+04  |\n",
      "|    ep_rew_mean       | -1.22e+05 |\n",
      "|    exploration_rate  | 0.459     |\n",
      "| time/                |           |\n",
      "|    episodes          | 464       |\n",
      "|    fps               | 691       |\n",
      "|    time_elapsed      | 12353     |\n",
      "|    total_timesteps   | 8539016   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 9.29      |\n",
      "|    n_updates         | 2122253   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -155719.999999998\n",
      "No more requests.\n",
      "Total reward for this episode is -161599.99999999756\n",
      "Num timesteps: 8600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -122027.00\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -22159.999999999894\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -152859.9999999985\n",
      "------------------------------------\n",
      "| average_route_length | 7.33      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.4       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.28e+04  |\n",
      "|    ep_rew_mean       | -1.23e+05 |\n",
      "|    exploration_rate  | 0.453     |\n",
      "| time/                |           |\n",
      "|    episodes          | 468       |\n",
      "|    fps               | 690       |\n",
      "|    time_elapsed      | 12499     |\n",
      "|    total_timesteps   | 8630112   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0032    |\n",
      "|    n_updates         | 2145027   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -93040.00000000131\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -166739.99999999764\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -157859.99999999814\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -137519.9999999997\n",
      "------------------------------------\n",
      "| average_route_length | 6.77      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.31e+04  |\n",
      "|    ep_rew_mean       | -1.25e+05 |\n",
      "|    exploration_rate  | 0.447     |\n",
      "| time/                |           |\n",
      "|    episodes          | 472       |\n",
      "|    fps               | 689       |\n",
      "|    time_elapsed      | 12658     |\n",
      "|    total_timesteps   | 8729284   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00638   |\n",
      "|    n_updates         | 2169820   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -108340.00000000128\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -82160.00000000087\n",
      "Num timesteps: 8800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -124804.80\n",
      "No more requests.\n",
      "Total reward for this episode is -137099.99999999974\n",
      "No more requests.\n",
      "Total reward for this episode is -109140.00000000151\n",
      "------------------------------------\n",
      "| average_route_length | 6         |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0.0526    |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.34e+04  |\n",
      "|    ep_rew_mean       | -1.26e+05 |\n",
      "|    exploration_rate  | 0.441     |\n",
      "| time/                |           |\n",
      "|    episodes          | 476       |\n",
      "|    fps               | 688       |\n",
      "|    time_elapsed      | 12816     |\n",
      "|    total_timesteps   | 8828148   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00177   |\n",
      "|    n_updates         | 2194536   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -107460.0000000014\n",
      "No more requests.\n",
      "Total reward for this episode is -107500.0000000015\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -108900.00000000141\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -122060.0000000007\n",
      "------------------------------------\n",
      "| average_route_length | 5.69      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.33e+04  |\n",
      "|    ep_rew_mean       | -1.23e+05 |\n",
      "|    exploration_rate  | 0.435     |\n",
      "| time/                |           |\n",
      "|    episodes          | 480       |\n",
      "|    fps               | 688       |\n",
      "|    time_elapsed      | 12972     |\n",
      "|    total_timesteps   | 8925175   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00691   |\n",
      "|    n_updates         | 2218793   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -84000.00000000114\n",
      "No more requests.\n",
      "Total reward for this episode is -122320.00000000044\n",
      "No more requests.\n",
      "Total reward for this episode is -101660.00000000128\n",
      "Num timesteps: 9000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -120896.20\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -161699.99999999776\n",
      "------------------------------------\n",
      "| average_route_length | 8.33      |\n",
      "| blocked_contiguous   | 0.368     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.4       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.34e+04  |\n",
      "|    ep_rew_mean       | -1.22e+05 |\n",
      "|    exploration_rate  | 0.429     |\n",
      "| time/                |           |\n",
      "|    episodes          | 484       |\n",
      "|    fps               | 687       |\n",
      "|    time_elapsed      | 13128     |\n",
      "|    total_timesteps   | 9022259   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 1.45      |\n",
      "|    n_updates         | 2243064   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -129499.99999999983\n",
      "No more requests.\n",
      "Total reward for this episode is -112400.00000000084\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -124740.0000000001\n",
      "No more requests.\n",
      "Total reward for this episode is -128819.99999999978\n",
      "------------------------------------\n",
      "| average_route_length | 6.31      |\n",
      "| blocked_contiguous   | 0.368     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.35e+04  |\n",
      "|    ep_rew_mean       | -1.22e+05 |\n",
      "|    exploration_rate  | 0.422     |\n",
      "| time/                |           |\n",
      "|    episodes          | 488       |\n",
      "|    fps               | 686       |\n",
      "|    time_elapsed      | 13284     |\n",
      "|    total_timesteps   | 9119890   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0036    |\n",
      "|    n_updates         | 2267472   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -82720.00000000092\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -85440.00000000105\n",
      "No more requests.\n",
      "Total reward for this episode is -79200.00000000089\n",
      "Num timesteps: 9200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -120149.00\n",
      "No more requests.\n",
      "Total reward for this episode is -110140.00000000116\n",
      "------------------------------------\n",
      "| average_route_length | 5.86      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.34e+04  |\n",
      "|    ep_rew_mean       | -1.21e+05 |\n",
      "|    exploration_rate  | 0.416     |\n",
      "| time/                |           |\n",
      "|    episodes          | 492       |\n",
      "|    fps               | 685       |\n",
      "|    time_elapsed      | 13439     |\n",
      "|    total_timesteps   | 9215847   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00953   |\n",
      "|    n_updates         | 2291461   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -154719.9999999978\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -105620.00000000162\n",
      "No more requests.\n",
      "Total reward for this episode is -149059.99999999837\n",
      "No more requests.\n",
      "Total reward for this episode is -101380.00000000146\n",
      "------------------------------------\n",
      "| average_route_length | 5.71      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.36e+04  |\n",
      "|    ep_rew_mean       | -1.21e+05 |\n",
      "|    exploration_rate  | 0.41      |\n",
      "| time/                |           |\n",
      "|    episodes          | 496       |\n",
      "|    fps               | 684       |\n",
      "|    time_elapsed      | 13595     |\n",
      "|    total_timesteps   | 9312302   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.031     |\n",
      "|    n_updates         | 2315575   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -126819.99999999999\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -117360.00000000144\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -124200.00000000077\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -9419.999999999996\n",
      "------------------------------------\n",
      "| average_route_length | 5         |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.8       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.34e+04  |\n",
      "|    ep_rew_mean       | -1.21e+05 |\n",
      "|    exploration_rate  | 0.405     |\n",
      "| time/                |           |\n",
      "|    episodes          | 500       |\n",
      "|    fps               | 684       |\n",
      "|    time_elapsed      | 13722     |\n",
      "|    total_timesteps   | 9389774   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0191    |\n",
      "|    n_updates         | 2334943   |\n",
      "------------------------------------\n",
      "Num timesteps: 9400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -120503.80\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -147779.99999999857\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -119840.00000000092\n",
      "No more requests.\n",
      "Total reward for this episode is -100400.00000000116\n",
      "No more requests.\n",
      "Total reward for this episode is -98980.00000000112\n",
      "------------------------------------\n",
      "| average_route_length | 5.71      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.34e+04  |\n",
      "|    ep_rew_mean       | -1.19e+05 |\n",
      "|    exploration_rate  | 0.399     |\n",
      "| time/                |           |\n",
      "|    episodes          | 504       |\n",
      "|    fps               | 683       |\n",
      "|    time_elapsed      | 13878     |\n",
      "|    total_timesteps   | 9485816   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 23.1      |\n",
      "|    n_updates         | 2358953   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -119300.00000000077\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -125600.00000000045\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -115620.0000000015\n",
      "No more requests.\n",
      "Total reward for this episode is -77760.00000000076\n",
      "------------------------------------\n",
      "| average_route_length | 5.33      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.35e+04  |\n",
      "|    ep_rew_mean       | -1.19e+05 |\n",
      "|    exploration_rate  | 0.393     |\n",
      "| time/                |           |\n",
      "|    episodes          | 508       |\n",
      "|    fps               | 682       |\n",
      "|    time_elapsed      | 14036     |\n",
      "|    total_timesteps   | 9582336   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00248   |\n",
      "|    n_updates         | 2383083   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Num timesteps: 9600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -119004.00\n",
      "No more requests.\n",
      "Total reward for this episode is -142539.99999999898\n",
      "No more requests.\n",
      "Total reward for this episode is -118040.00000000121\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -127240.00000000032\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -119280.00000000147\n",
      "------------------------------------\n",
      "| average_route_length | 6.31      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.36e+04  |\n",
      "|    ep_rew_mean       | -1.19e+05 |\n",
      "|    exploration_rate  | 0.387     |\n",
      "| time/                |           |\n",
      "|    episodes          | 512       |\n",
      "|    fps               | 681       |\n",
      "|    time_elapsed      | 14196     |\n",
      "|    total_timesteps   | 9680137   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 6.54      |\n",
      "|    n_updates         | 2407534   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -144099.99999999916\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -29079.99999999987\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -92100.00000000096\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -87660.00000000081\n",
      "------------------------------------\n",
      "| average_route_length | 5.14      |\n",
      "| blocked_contiguous   | 0.158     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.34e+04  |\n",
      "|    ep_rew_mean       | -1.17e+05 |\n",
      "|    exploration_rate  | 0.381     |\n",
      "| time/                |           |\n",
      "|    episodes          | 516       |\n",
      "|    fps               | 681       |\n",
      "|    time_elapsed      | 14344     |\n",
      "|    total_timesteps   | 9769673   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0157    |\n",
      "|    n_updates         | 2429918   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -140779.9999999997\n",
      "Num timesteps: 9800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -117663.60\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -98040.00000000109\n",
      "No more requests.\n",
      "Total reward for this episode is -93820.00000000103\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -146499.99999999904\n",
      "------------------------------------\n",
      "| average_route_length | 7         |\n",
      "| blocked_contiguous   | 0.368     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.4       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.36e+04  |\n",
      "|    ep_rew_mean       | -1.18e+05 |\n",
      "|    exploration_rate  | 0.375     |\n",
      "| time/                |           |\n",
      "|    episodes          | 520       |\n",
      "|    fps               | 680       |\n",
      "|    time_elapsed      | 14504     |\n",
      "|    total_timesteps   | 9867940   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00987   |\n",
      "|    n_updates         | 2454484   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -68640.00000000057\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -91500.0000000009\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -136739.99999999988\n",
      "No more requests.\n",
      "Total reward for this episode is -79280.00000000089\n",
      "------------------------------------\n",
      "| average_route_length | 5.33      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.36e+04  |\n",
      "|    ep_rew_mean       | -1.16e+05 |\n",
      "|    exploration_rate  | 0.369     |\n",
      "| time/                |           |\n",
      "|    episodes          | 524       |\n",
      "|    fps               | 679       |\n",
      "|    time_elapsed      | 14662     |\n",
      "|    total_timesteps   | 9963061   |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0322    |\n",
      "|    n_updates         | 2478265   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -109120.00000000116\n",
      "Num timesteps: 10000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -115933.60\n",
      "No more requests.\n",
      "Total reward for this episode is -94340.0000000011\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -95740.000000001\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -120860.00000000122\n",
      "------------------------------------\n",
      "| average_route_length | 6.62      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.36e+04  |\n",
      "|    ep_rew_mean       | -1.16e+05 |\n",
      "|    exploration_rate  | 0.363     |\n",
      "| time/                |           |\n",
      "|    episodes          | 528       |\n",
      "|    fps               | 678       |\n",
      "|    time_elapsed      | 14819     |\n",
      "|    total_timesteps   | 10057983  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.013     |\n",
      "|    n_updates         | 2501995   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -90880.00000000093\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -109620.00000000108\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -96300.00000000092\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -154919.99999999892\n",
      "------------------------------------\n",
      "| average_route_length | 7.64      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.45      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.37e+04  |\n",
      "|    ep_rew_mean       | -1.16e+05 |\n",
      "|    exploration_rate  | 0.357     |\n",
      "| time/                |           |\n",
      "|    episodes          | 532       |\n",
      "|    fps               | 677       |\n",
      "|    time_elapsed      | 14976     |\n",
      "|    total_timesteps   | 10152363  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00769   |\n",
      "|    n_updates         | 2525590   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -112340.0000000012\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -93980.00000000099\n",
      "Num timesteps: 10200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -115821.80\n",
      "No more requests.\n",
      "Total reward for this episode is -97600.00000000112\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -97100.00000000096\n",
      "------------------------------------\n",
      "| average_route_length | 5.57      |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.38e+04  |\n",
      "|    ep_rew_mean       | -1.16e+05 |\n",
      "|    exploration_rate  | 0.351     |\n",
      "| time/                |           |\n",
      "|    episodes          | 536       |\n",
      "|    fps               | 677       |\n",
      "|    time_elapsed      | 15132     |\n",
      "|    total_timesteps   | 10246383  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00256   |\n",
      "|    n_updates         | 2549095   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -89820.00000000087\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -95020.00000000115\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -120800.00000000055\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -111900.00000000141\n",
      "------------------------------------\n",
      "| average_route_length | 6.31      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.38e+04  |\n",
      "|    ep_rew_mean       | -1.15e+05 |\n",
      "|    exploration_rate  | 0.345     |\n",
      "| time/                |           |\n",
      "|    episodes          | 540       |\n",
      "|    fps               | 676       |\n",
      "|    time_elapsed      | 15290     |\n",
      "|    total_timesteps   | 10341944  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00551   |\n",
      "|    n_updates         | 2572985   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -103960.00000000103\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -110240.00000000121\n",
      "Num timesteps: 10400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -114448.20\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -112100.00000000141\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -134100.00000000015\n",
      "------------------------------------\n",
      "| average_route_length | 7.67      |\n",
      "| blocked_contiguous   | 0.368     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.4       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.37e+04  |\n",
      "|    ep_rew_mean       | -1.14e+05 |\n",
      "|    exploration_rate  | 0.339     |\n",
      "| time/                |           |\n",
      "|    episodes          | 544       |\n",
      "|    fps               | 675       |\n",
      "|    time_elapsed      | 15447     |\n",
      "|    total_timesteps   | 10435817  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00577   |\n",
      "|    n_updates         | 2596454   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -105720.00000000108\n",
      "No more requests.\n",
      "Total reward for this episode is -90200.00000000079\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -106620.00000000124\n",
      "No more requests.\n",
      "Total reward for this episode is -105940.00000000137\n",
      "------------------------------------\n",
      "| average_route_length | 5.69      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0.0526    |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.37e+04  |\n",
      "|    ep_rew_mean       | -1.13e+05 |\n",
      "|    exploration_rate  | 0.333     |\n",
      "| time/                |           |\n",
      "|    episodes          | 548       |\n",
      "|    fps               | 674       |\n",
      "|    time_elapsed      | 15603     |\n",
      "|    total_timesteps   | 10529055  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0253    |\n",
      "|    n_updates         | 2619763   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -65580.00000000052\n",
      "No more requests.\n",
      "Total reward for this episode is -90720.00000000093\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -167839.9999999977\n",
      "Num timesteps: 10600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -113039.20\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -109240.00000000115\n",
      "------------------------------------\n",
      "| average_route_length | 6.46      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0.0526    |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.36e+04  |\n",
      "|    ep_rew_mean       | -1.12e+05 |\n",
      "|    exploration_rate  | 0.327     |\n",
      "| time/                |           |\n",
      "|    episodes          | 552       |\n",
      "|    fps               | 674       |\n",
      "|    time_elapsed      | 15761     |\n",
      "|    total_timesteps   | 10624144  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00207   |\n",
      "|    n_updates         | 2643535   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -100600.00000000121\n",
      "No more requests.\n",
      "Total reward for this episode is -105460.00000000103\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -4539.9999999999945\n",
      "No more requests.\n",
      "Total reward for this episode is -126760.00000000003\n",
      "-----------------------------------\n",
      "| average_route_length | 6.83     |\n",
      "| blocked_contiguous   | 0.421    |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.4      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.34e+04 |\n",
      "|    ep_rew_mean       | -1.1e+05 |\n",
      "|    exploration_rate  | 0.322    |\n",
      "| time/                |          |\n",
      "|    episodes          | 556      |\n",
      "|    fps               | 673      |\n",
      "|    time_elapsed      | 15891    |\n",
      "|    total_timesteps   | 10701709 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00881  |\n",
      "|    n_updates         | 2662927  |\n",
      "-----------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -104960.0000000011\n",
      "No more requests.\n",
      "Total reward for this episode is -100680.00000000097\n",
      "No more requests.\n",
      "Total reward for this episode is -102680.0000000009\n",
      "No more requests.\n",
      "Total reward for this episode is -105000.00000000096\n",
      "------------------------------------\n",
      "| average_route_length | 6.77      |\n",
      "| blocked_contiguous   | 0.368     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.34e+04  |\n",
      "|    ep_rew_mean       | -1.08e+05 |\n",
      "|    exploration_rate  | 0.316     |\n",
      "| time/                |           |\n",
      "|    episodes          | 560       |\n",
      "|    fps               | 672       |\n",
      "|    time_elapsed      | 16047     |\n",
      "|    total_timesteps   | 10794830  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00337   |\n",
      "|    n_updates         | 2686207   |\n",
      "------------------------------------\n",
      "Num timesteps: 10800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -108394.80\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -87720.00000000081\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -129339.99999999919\n",
      "No more requests.\n",
      "Total reward for this episode is -75500.0000000008\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -102000.000000001\n",
      "------------------------------------\n",
      "| average_route_length | 5.85      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.35e+04  |\n",
      "|    ep_rew_mean       | -1.08e+05 |\n",
      "|    exploration_rate  | 0.31      |\n",
      "| time/                |           |\n",
      "|    episodes          | 564       |\n",
      "|    fps               | 671       |\n",
      "|    time_elapsed      | 16204     |\n",
      "|    total_timesteps   | 10888227  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00947   |\n",
      "|    n_updates         | 2709556   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -112340.00000000044\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -78500.00000000063\n",
      "No more requests.\n",
      "Total reward for this episode is -65980.00000000047\n",
      "No more requests.\n",
      "Total reward for this episode is -98880.00000000064\n",
      "------------------------------------\n",
      "| average_route_length | 7.17      |\n",
      "| blocked_contiguous   | 0.421     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.4       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.34e+04  |\n",
      "|    ep_rew_mean       | -1.07e+05 |\n",
      "|    exploration_rate  | 0.305     |\n",
      "| time/                |           |\n",
      "|    episodes          | 568       |\n",
      "|    fps               | 670       |\n",
      "|    time_elapsed      | 16361     |\n",
      "|    total_timesteps   | 10973678  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0282    |\n",
      "|    n_updates         | 2730919   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -82680.0000000005\n",
      "Num timesteps: 11000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -107045.00\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -101420.00000000079\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -103580.00000000089\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -63300.00000000049\n",
      "------------------------------------\n",
      "| average_route_length | 4.53      |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.31e+04  |\n",
      "|    ep_rew_mean       | -1.05e+05 |\n",
      "|    exploration_rate  | 0.301     |\n",
      "| time/                |           |\n",
      "|    episodes          | 572       |\n",
      "|    fps               | 668       |\n",
      "|    time_elapsed      | 16517     |\n",
      "|    total_timesteps   | 11041195  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000628  |\n",
      "|    n_updates         | 2747798   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -104960.00000000114\n",
      "No more requests.\n",
      "Total reward for this episode is -68520.00000000061\n",
      "No more requests.\n",
      "Total reward for this episode is -66380.00000000036\n",
      "No more requests.\n",
      "Total reward for this episode is -80320.00000000068\n",
      "------------------------------------\n",
      "| average_route_length | 5.29      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.31e+04  |\n",
      "|    ep_rew_mean       | -1.04e+05 |\n",
      "|    exploration_rate  | 0.295     |\n",
      "| time/                |           |\n",
      "|    episodes          | 576       |\n",
      "|    fps               | 668       |\n",
      "|    time_elapsed      | 16673     |\n",
      "|    total_timesteps   | 11138728  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00394   |\n",
      "|    n_updates         | 2772181   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -108840.00000000131\n",
      "No more requests.\n",
      "Total reward for this episode is -88660.0000000011\n",
      "Num timesteps: 11200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -103873.60\n",
      "No more requests.\n",
      "Total reward for this episode is -62720.000000000495\n",
      "No more requests.\n",
      "Total reward for this episode is -66040.00000000045\n",
      "------------------------------------\n",
      "| average_route_length | 5.07      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.31e+04  |\n",
      "|    ep_rew_mean       | -1.03e+05 |\n",
      "|    exploration_rate  | 0.288     |\n",
      "| time/                |           |\n",
      "|    episodes          | 580       |\n",
      "|    fps               | 667       |\n",
      "|    time_elapsed      | 16829     |\n",
      "|    total_timesteps   | 11240151  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00757   |\n",
      "|    n_updates         | 2797537   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -84240.0000000007\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -103820.00000000099\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -100760.000000001\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -94760.00000000106\n",
      "------------------------------------\n",
      "| average_route_length | 5.71      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.32e+04  |\n",
      "|    ep_rew_mean       | -1.02e+05 |\n",
      "|    exploration_rate  | 0.282     |\n",
      "| time/                |           |\n",
      "|    episodes          | 584       |\n",
      "|    fps               | 667       |\n",
      "|    time_elapsed      | 16987     |\n",
      "|    total_timesteps   | 11343139  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0118    |\n",
      "|    n_updates         | 2823284   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -87280.00000000077\n",
      "No more requests.\n",
      "Total reward for this episode is -111640.0000000007\n",
      "Num timesteps: 11400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -101453.80\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -103880.00000000106\n",
      "No more requests.\n",
      "Total reward for this episode is -62720.00000000046\n",
      "------------------------------------\n",
      "| average_route_length | 4.93      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.33e+04  |\n",
      "|    ep_rew_mean       | -1.01e+05 |\n",
      "|    exploration_rate  | 0.275     |\n",
      "| time/                |           |\n",
      "|    episodes          | 588       |\n",
      "|    fps               | 667       |\n",
      "|    time_elapsed      | 17145     |\n",
      "|    total_timesteps   | 11445408  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 7.36      |\n",
      "|    n_updates         | 2848851   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -79540.0000000007\n",
      "No more requests.\n",
      "Total reward for this episode is -60500.000000000386\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -102640.00000000092\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -117880.00000000118\n",
      "-----------------------------------\n",
      "| average_route_length | 6.83     |\n",
      "| blocked_contiguous   | 0.263    |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.4      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.33e+04 |\n",
      "|    ep_rew_mean       | -1e+05   |\n",
      "|    exploration_rate  | 0.269    |\n",
      "| time/                |          |\n",
      "|    episodes          | 592      |\n",
      "|    fps               | 667      |\n",
      "|    time_elapsed      | 17305    |\n",
      "|    total_timesteps   | 11549005 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00532  |\n",
      "|    n_updates         | 2874751  |\n",
      "-----------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -55240.00000000041\n",
      "No more requests.\n",
      "Total reward for this episode is -76320.00000000067\n",
      "Num timesteps: 11600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -99122.60\n",
      "No more requests.\n",
      "Total reward for this episode is -65420.00000000068\n",
      "No more requests.\n",
      "Total reward for this episode is -83160.00000000065\n",
      "------------------------------------\n",
      "| average_route_length | 6         |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.34e+04  |\n",
      "|    ep_rew_mean       | -9.81e+04 |\n",
      "|    exploration_rate  | 0.262     |\n",
      "| time/                |           |\n",
      "|    episodes          | 596       |\n",
      "|    fps               | 667       |\n",
      "|    time_elapsed      | 17462     |\n",
      "|    total_timesteps   | 11647966  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00165   |\n",
      "|    n_updates         | 2899491   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -4659.999999999994\n",
      "No more requests.\n",
      "Total reward for this episode is -39739.99999999996\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -99860.00000000092\n",
      "No more requests.\n",
      "Total reward for this episode is -82880.00000000079\n",
      "------------------------------------\n",
      "| average_route_length | 5.14      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.34e+04  |\n",
      "|    ep_rew_mean       | -9.66e+04 |\n",
      "|    exploration_rate  | 0.257     |\n",
      "| time/                |           |\n",
      "|    episodes          | 600       |\n",
      "|    fps               | 666       |\n",
      "|    time_elapsed      | 17592     |\n",
      "|    total_timesteps   | 11730200  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00496   |\n",
      "|    n_updates         | 2920049   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -59460.000000000415\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -92280.00000000077\n",
      "Stop hitting yourself\n",
      "Num timesteps: 11800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -95438.60\n",
      "No more requests.\n",
      "Total reward for this episode is -93920.00000000086\n",
      "No more requests.\n",
      "Total reward for this episode is -95360.00000000079\n",
      "------------------------------------\n",
      "| average_route_length | 6.15      |\n",
      "| blocked_contiguous   | 0.368     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.34e+04  |\n",
      "|    ep_rew_mean       | -9.53e+04 |\n",
      "|    exploration_rate  | 0.251     |\n",
      "| time/                |           |\n",
      "|    episodes          | 604       |\n",
      "|    fps               | 666       |\n",
      "|    time_elapsed      | 17748     |\n",
      "|    total_timesteps   | 11827708  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 24.2      |\n",
      "|    n_updates         | 2944426   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -94740.0000000008\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -116640.00000000115\n",
      "No more requests.\n",
      "Total reward for this episode is -96000.00000000087\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -94980.00000000077\n",
      "------------------------------------\n",
      "| average_route_length | 6.31      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.34e+04  |\n",
      "|    ep_rew_mean       | -9.51e+04 |\n",
      "|    exploration_rate  | 0.245     |\n",
      "| time/                |           |\n",
      "|    episodes          | 608       |\n",
      "|    fps               | 665       |\n",
      "|    time_elapsed      | 17905     |\n",
      "|    total_timesteps   | 11924224  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0037    |\n",
      "|    n_updates         | 2968555   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -57420.00000000015\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -66220.0000000003\n",
      "No more requests.\n",
      "Total reward for this episode is -88700.00000000074\n",
      "Num timesteps: 12000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -93433.60\n",
      "No more requests.\n",
      "Total reward for this episode is -113000.00000000112\n",
      "------------------------------------\n",
      "| average_route_length | 6.5       |\n",
      "| blocked_contiguous   | 0.421     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.4       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.34e+04  |\n",
      "|    ep_rew_mean       | -9.35e+04 |\n",
      "|    exploration_rate  | 0.239     |\n",
      "| time/                |           |\n",
      "|    episodes          | 612       |\n",
      "|    fps               | 665       |\n",
      "|    time_elapsed      | 18062     |\n",
      "|    total_timesteps   | 12021808  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0245    |\n",
      "|    n_updates         | 2992951   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -88860.00000000074\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -92100.0000000007\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -38299.99999999996\n",
      "No more requests.\n",
      "Total reward for this episode is -69940.00000000038\n",
      "------------------------------------\n",
      "| average_route_length | 4.86      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.35e+04  |\n",
      "|    ep_rew_mean       | -9.28e+04 |\n",
      "|    exploration_rate  | 0.232     |\n",
      "| time/                |           |\n",
      "|    episodes          | 616       |\n",
      "|    fps               | 665       |\n",
      "|    time_elapsed      | 18220     |\n",
      "|    total_timesteps   | 12119873  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00277   |\n",
      "|    n_updates         | 3017468   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -107080.00000000105\n",
      "No more requests.\n",
      "Total reward for this episode is -60580.00000000047\n",
      "No more requests.\n",
      "Total reward for this episode is -57620.00000000037\n",
      "Num timesteps: 12200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -91879.40\n",
      "No more requests.\n",
      "Total reward for this episode is -90840.00000000074\n",
      "------------------------------------\n",
      "| average_route_length | 6.31      |\n",
      "| blocked_contiguous   | 0.368     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.35e+04  |\n",
      "|    ep_rew_mean       | -9.13e+04 |\n",
      "|    exploration_rate  | 0.226     |\n",
      "| time/                |           |\n",
      "|    episodes          | 620       |\n",
      "|    fps               | 664       |\n",
      "|    time_elapsed      | 18376     |\n",
      "|    total_timesteps   | 12216483  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00663   |\n",
      "|    n_updates         | 3041620   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -93420.00000000083\n",
      "No more requests.\n",
      "Total reward for this episode is -53580.000000000175\n",
      "No more requests.\n",
      "Total reward for this episode is -71080.00000000054\n",
      "No more requests.\n",
      "Total reward for this episode is -72260.00000000052\n",
      "------------------------------------\n",
      "| average_route_length | 5.43      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.35e+04  |\n",
      "|    ep_rew_mean       | -9.06e+04 |\n",
      "|    exploration_rate  | 0.22      |\n",
      "| time/                |           |\n",
      "|    episodes          | 624       |\n",
      "|    fps               | 664       |\n",
      "|    time_elapsed      | 18533     |\n",
      "|    total_timesteps   | 12312803  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 25.8      |\n",
      "|    n_updates         | 3065700   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -90680.00000000073\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -106240.00000000092\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -110080.00000000099\n",
      "Num timesteps: 12400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -90542.60\n",
      "No more requests.\n",
      "Total reward for this episode is -91480.00000000073\n",
      "------------------------------------\n",
      "| average_route_length | 6.46      |\n",
      "| blocked_contiguous   | 0.368     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.35e+04  |\n",
      "|    ep_rew_mean       | -9.02e+04 |\n",
      "|    exploration_rate  | 0.214     |\n",
      "| time/                |           |\n",
      "|    episodes          | 628       |\n",
      "|    fps               | 663       |\n",
      "|    time_elapsed      | 18692     |\n",
      "|    total_timesteps   | 12410439  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0097    |\n",
      "|    n_updates         | 3090109   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -107860.0000000009\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -73540.00000000058\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -70680.00000000055\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -69820.00000000038\n",
      "------------------------------------\n",
      "| average_route_length | 5.29      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.36e+04  |\n",
      "|    ep_rew_mean       | -8.88e+04 |\n",
      "|    exploration_rate  | 0.208     |\n",
      "| time/                |           |\n",
      "|    episodes          | 632       |\n",
      "|    fps               | 663       |\n",
      "|    time_elapsed      | 18850     |\n",
      "|    total_timesteps   | 12508098  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0198    |\n",
      "|    n_updates         | 3114524   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -66260.00000000025\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -104200.00000000086\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -104340.00000000084\n",
      "Stop hitting yourself\n",
      "Num timesteps: 12600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -88451.40\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -88940.00000000071\n",
      "------------------------------------\n",
      "| average_route_length | 6.15      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.36e+04  |\n",
      "|    ep_rew_mean       | -8.83e+04 |\n",
      "|    exploration_rate  | 0.202     |\n",
      "| time/                |           |\n",
      "|    episodes          | 636       |\n",
      "|    fps               | 663       |\n",
      "|    time_elapsed      | 19011     |\n",
      "|    total_timesteps   | 12606701  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00371   |\n",
      "|    n_updates         | 3139175   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -100500.00000000081\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -67440.00000000029\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -85200.00000000067\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -5659.999999999995\n",
      "------------------------------------\n",
      "| average_route_length | 5         |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.7       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.34e+04  |\n",
      "|    ep_rew_mean       | -8.68e+04 |\n",
      "|    exploration_rate  | 0.197     |\n",
      "| time/                |           |\n",
      "|    episodes          | 640       |\n",
      "|    fps               | 662       |\n",
      "|    time_elapsed      | 19141     |\n",
      "|    total_timesteps   | 12685819  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00429   |\n",
      "|    n_updates         | 3158954   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -107820.00000000084\n",
      "No more requests.\n",
      "Total reward for this episode is -71120.00000000057\n",
      "No more requests.\n",
      "Total reward for this episode is -77220.00000000041\n",
      "No more requests.\n",
      "Total reward for this episode is -57660.00000000028\n",
      "------------------------------------\n",
      "| average_route_length | 4.71      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.35e+04  |\n",
      "|    ep_rew_mean       | -8.53e+04 |\n",
      "|    exploration_rate  | 0.19      |\n",
      "| time/                |           |\n",
      "|    episodes          | 644       |\n",
      "|    fps               | 662       |\n",
      "|    time_elapsed      | 19298     |\n",
      "|    total_timesteps   | 12782421  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00897   |\n",
      "|    n_updates         | 3183105   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "Num timesteps: 12800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -85309.00\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -100600.00000000087\n",
      "No more requests.\n",
      "Total reward for this episode is -63160.00000000028\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -79440.00000000041\n",
      "No more requests.\n",
      "Total reward for this episode is -54720.000000000415\n",
      "------------------------------------\n",
      "| average_route_length | 5.07      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.35e+04  |\n",
      "|    ep_rew_mean       | -8.42e+04 |\n",
      "|    exploration_rate  | 0.184     |\n",
      "| time/                |           |\n",
      "|    episodes          | 648       |\n",
      "|    fps               | 661       |\n",
      "|    time_elapsed      | 19454     |\n",
      "|    total_timesteps   | 12877255  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 312       |\n",
      "|    n_updates         | 3206813   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -86560.00000000054\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -28339.99999999988\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -12860.000000000002\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -80780.00000000041\n",
      "------------------------------------\n",
      "| average_route_length | 5.69      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.33e+04  |\n",
      "|    ep_rew_mean       | -8.21e+04 |\n",
      "|    exploration_rate  | 0.18      |\n",
      "| time/                |           |\n",
      "|    episodes          | 652       |\n",
      "|    fps               | 661       |\n",
      "|    time_elapsed      | 19578     |\n",
      "|    total_timesteps   | 12953087  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0501    |\n",
      "|    n_updates         | 3225771   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -64480.000000000255\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "Num timesteps: 13000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -81693.20\n",
      "No more requests.\n",
      "Total reward for this episode is -83980.00000000051\n",
      "No more requests.\n",
      "Total reward for this episode is -36099.999999999935\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -78300.00000000036\n",
      "------------------------------------\n",
      "| average_route_length | 5.85      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.35e+04  |\n",
      "|    ep_rew_mean       | -8.13e+04 |\n",
      "|    exploration_rate  | 0.174     |\n",
      "| time/                |           |\n",
      "|    episodes          | 656       |\n",
      "|    fps               | 661       |\n",
      "|    time_elapsed      | 19734     |\n",
      "|    total_timesteps   | 13048460  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 2.14      |\n",
      "|    n_updates         | 3249614   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -78860.00000000044\n",
      "No more requests.\n",
      "Total reward for this episode is -84880.00000000061\n",
      "No more requests.\n",
      "Total reward for this episode is -80800.00000000048\n",
      "No more requests.\n",
      "Total reward for this episode is -62860.00000000042\n",
      "------------------------------------\n",
      "| average_route_length | 5.29      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.35e+04  |\n",
      "|    ep_rew_mean       | -8.03e+04 |\n",
      "|    exploration_rate  | 0.168     |\n",
      "| time/                |           |\n",
      "|    episodes          | 660       |\n",
      "|    fps               | 660       |\n",
      "|    time_elapsed      | 19890     |\n",
      "|    total_timesteps   | 13143564  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00159   |\n",
      "|    n_updates         | 3273390   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -93680.00000000067\n",
      "No more requests.\n",
      "Total reward for this episode is -83760.00000000044\n",
      "Num timesteps: 13200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -79859.60\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -58420.00000000021\n",
      "No more requests.\n",
      "Total reward for this episode is -22559.99999999991\n",
      "------------------------------------\n",
      "| average_route_length | 4         |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.2       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.35e+04  |\n",
      "|    ep_rew_mean       | -7.89e+04 |\n",
      "|    exploration_rate  | 0.162     |\n",
      "| time/                |           |\n",
      "|    episodes          | 664       |\n",
      "|    fps               | 660       |\n",
      "|    time_elapsed      | 20046     |\n",
      "|    total_timesteps   | 13238175  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00206   |\n",
      "|    n_updates         | 3297043   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -58240.000000000146\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -96320.0000000006\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -28779.999999999894\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -114160.00000000055\n",
      "------------------------------------\n",
      "| average_route_length | 8         |\n",
      "| blocked_contiguous   | 0.368     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.45      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.36e+04  |\n",
      "|    ep_rew_mean       | -7.83e+04 |\n",
      "|    exploration_rate  | 0.156     |\n",
      "| time/                |           |\n",
      "|    episodes          | 668       |\n",
      "|    fps               | 659       |\n",
      "|    time_elapsed      | 20204     |\n",
      "|    total_timesteps   | 13333374  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 17.8      |\n",
      "|    n_updates         | 3320843   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -64120.00000000029\n",
      "No more requests.\n",
      "Total reward for this episode is -62520.00000000016\n",
      "Num timesteps: 13400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -77737.80\n",
      "No more requests.\n",
      "Total reward for this episode is -57360.00000000018\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -43540.000000000044\n",
      "------------------------------------\n",
      "| average_route_length | 5.07      |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.39e+04  |\n",
      "|    ep_rew_mean       | -7.71e+04 |\n",
      "|    exploration_rate  | 0.15      |\n",
      "| time/                |           |\n",
      "|    episodes          | 672       |\n",
      "|    fps               | 659       |\n",
      "|    time_elapsed      | 20360     |\n",
      "|    total_timesteps   | 13426984  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 10.7      |\n",
      "|    n_updates         | 3344245   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -54760.00000000015\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -11860.0\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -4539.999999999996\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -60900.00000000032\n",
      "------------------------------------\n",
      "| average_route_length | 5.29      |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.35e+04  |\n",
      "|    ep_rew_mean       | -7.52e+04 |\n",
      "|    exploration_rate  | 0.146     |\n",
      "| time/                |           |\n",
      "|    episodes          | 676       |\n",
      "|    fps               | 659       |\n",
      "|    time_elapsed      | 20459     |\n",
      "|    total_timesteps   | 13487002  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0468    |\n",
      "|    n_updates         | 3359250   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -54100.00000000014\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -77860.00000000044\n",
      "No more requests.\n",
      "Total reward for this episode is -86300.00000000036\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -55140.00000000013\n",
      "------------------------------------\n",
      "| average_route_length | 5.14      |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.34e+04  |\n",
      "|    ep_rew_mean       | -7.46e+04 |\n",
      "|    exploration_rate  | 0.14      |\n",
      "| time/                |           |\n",
      "|    episodes          | 680       |\n",
      "|    fps               | 658       |\n",
      "|    time_elapsed      | 20617     |\n",
      "|    total_timesteps   | 13581953  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00152   |\n",
      "|    n_updates         | 3382988   |\n",
      "------------------------------------\n",
      "Num timesteps: 13600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -74550.00\n",
      "No more requests.\n",
      "Total reward for this episode is -41060.000000000044\n",
      "No more requests.\n",
      "Total reward for this episode is -74180.00000000029\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -73460.00000000028\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -107260.00000000054\n",
      "------------------------------------\n",
      "| average_route_length | 7.82      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.45      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.33e+04  |\n",
      "|    ep_rew_mean       | -7.37e+04 |\n",
      "|    exploration_rate  | 0.134     |\n",
      "| time/                |           |\n",
      "|    episodes          | 684       |\n",
      "|    fps               | 658       |\n",
      "|    time_elapsed      | 20775     |\n",
      "|    total_timesteps   | 13676481  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0165    |\n",
      "|    n_updates         | 3406620   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -4659.999999999996\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -54700.00000000008\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -42120.00000000014\n",
      "No more requests.\n",
      "Total reward for this episode is -20919.999999999924\n",
      "------------------------------------\n",
      "| average_route_length | 4.12      |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.2       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.31e+04  |\n",
      "|    ep_rew_mean       | -7.11e+04 |\n",
      "|    exploration_rate  | 0.129     |\n",
      "| time/                |           |\n",
      "|    episodes          | 688       |\n",
      "|    fps               | 657       |\n",
      "|    time_elapsed      | 20908     |\n",
      "|    total_timesteps   | 13756385  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00129   |\n",
      "|    n_updates         | 3426596   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -25759.999999999924\n",
      "Num timesteps: 13800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -70596.20\n",
      "No more requests.\n",
      "Total reward for this episode is -58140.00000000019\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -53820.000000000095\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -68560.00000000025\n",
      "------------------------------------\n",
      "| average_route_length | 5.54      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.3e+04   |\n",
      "|    ep_rew_mean       | -6.98e+04 |\n",
      "|    exploration_rate  | 0.123     |\n",
      "| time/                |           |\n",
      "|    episodes          | 692       |\n",
      "|    fps               | 657       |\n",
      "|    time_elapsed      | 21064     |\n",
      "|    total_timesteps   | 13850669  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00224   |\n",
      "|    n_updates         | 3450167   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -35419.999999999905\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -55360.00000000015\n",
      "No more requests.\n",
      "Total reward for this episode is -40440.00000000002\n",
      "No more requests.\n",
      "Total reward for this episode is -19899.999999999935\n",
      "------------------------------------\n",
      "| average_route_length | 4.12      |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.2       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.3e+04   |\n",
      "|    ep_rew_mean       | -6.85e+04 |\n",
      "|    exploration_rate  | 0.117     |\n",
      "| time/                |           |\n",
      "|    episodes          | 696       |\n",
      "|    fps               | 657       |\n",
      "|    time_elapsed      | 21220     |\n",
      "|    total_timesteps   | 13944376  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0114    |\n",
      "|    n_updates         | 3473593   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -67780.00000000025\n",
      "No more requests.\n",
      "Total reward for this episode is -26419.99999999993\n",
      "Num timesteps: 14000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -69016.60\n",
      "No more requests.\n",
      "Total reward for this episode is -39300.00000000003\n",
      "No more requests.\n",
      "Total reward for this episode is -49080.0\n",
      "------------------------------------\n",
      "| average_route_length | 4.71      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.31e+04  |\n",
      "|    ep_rew_mean       | -6.81e+04 |\n",
      "|    exploration_rate  | 0.111     |\n",
      "| time/                |           |\n",
      "|    episodes          | 700       |\n",
      "|    fps               | 656       |\n",
      "|    time_elapsed      | 21376     |\n",
      "|    total_timesteps   | 14038320  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00986   |\n",
      "|    n_updates         | 3497079   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -51680.00000000005\n",
      "No more requests.\n",
      "Total reward for this episode is -38619.99999999997\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -83400.00000000032\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -52740.00000000013\n",
      "------------------------------------\n",
      "| average_route_length | 5.14      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.3e+04   |\n",
      "|    ep_rew_mean       | -6.69e+04 |\n",
      "|    exploration_rate  | 0.105     |\n",
      "| time/                |           |\n",
      "|    episodes          | 704       |\n",
      "|    fps               | 656       |\n",
      "|    time_elapsed      | 21532     |\n",
      "|    total_timesteps   | 14131613  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00175   |\n",
      "|    n_updates         | 3520403   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -40559.99999999999\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -50520.00000000001\n",
      "Num timesteps: 14200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -65619.20\n",
      "No more requests.\n",
      "Total reward for this episode is -6059.9999999999545\n",
      "No more requests.\n",
      "Total reward for this episode is -36400.00000000002\n",
      "------------------------------------\n",
      "| average_route_length | 4.67      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.3e+04   |\n",
      "|    ep_rew_mean       | -6.41e+04 |\n",
      "|    exploration_rate  | 0.0991    |\n",
      "| time/                |           |\n",
      "|    episodes          | 708       |\n",
      "|    fps               | 655       |\n",
      "|    time_elapsed      | 21691     |\n",
      "|    total_timesteps   | 14225419  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00568   |\n",
      "|    n_updates         | 3543854   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -33259.99999999993\n",
      "No more requests.\n",
      "Total reward for this episode is -33839.999999999905\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -64660.000000000175\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 2400.0000000000055\n",
      "------------------------------------\n",
      "| average_route_length | 4         |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.6       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.28e+04  |\n",
      "|    ep_rew_mean       | -6.22e+04 |\n",
      "|    exploration_rate  | 0.094     |\n",
      "| time/                |           |\n",
      "|    episodes          | 712       |\n",
      "|    fps               | 655       |\n",
      "|    time_elapsed      | 21825     |\n",
      "|    total_timesteps   | 14305686  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00157   |\n",
      "|    n_updates         | 3563921   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -37239.99999999994\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -53560.000000000095\n",
      "No more requests.\n",
      "Total reward for this episode is -20359.999999999935\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -33799.99999999992\n",
      "------------------------------------\n",
      "| average_route_length | 4.4       |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.28e+04  |\n",
      "|    ep_rew_mean       | -6.08e+04 |\n",
      "|    exploration_rate  | 0.0881    |\n",
      "| time/                |           |\n",
      "|    episodes          | 716       |\n",
      "|    fps               | 655       |\n",
      "|    time_elapsed      | 21981     |\n",
      "|    total_timesteps   | 14399194  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00327   |\n",
      "|    n_updates         | 3587298   |\n",
      "------------------------------------\n",
      "Num timesteps: 14400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -60831.20\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -4599.999999999997\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -46919.99999999999\n",
      "No more requests.\n",
      "Total reward for this episode is -18339.99999999997\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -33419.99999999993\n",
      "------------------------------------\n",
      "| average_route_length | 4.53      |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.26e+04  |\n",
      "|    ep_rew_mean       | -5.87e+04 |\n",
      "|    exploration_rate  | 0.0832    |\n",
      "| time/                |           |\n",
      "|    episodes          | 720       |\n",
      "|    fps               | 654       |\n",
      "|    time_elapsed      | 22111     |\n",
      "|    total_timesteps   | 14476079  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0172    |\n",
      "|    n_updates         | 3606519   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -34979.99999999994\n",
      "No more requests.\n",
      "Total reward for this episode is -48080.00000000009\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -51420.00000000006\n",
      "No more requests.\n",
      "Total reward for this episode is -19799.999999999956\n",
      "------------------------------------\n",
      "| average_route_length | 4.38      |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.2       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.26e+04  |\n",
      "|    ep_rew_mean       | -5.73e+04 |\n",
      "|    exploration_rate  | 0.0773    |\n",
      "| time/                |           |\n",
      "|    episodes          | 724       |\n",
      "|    fps               | 654       |\n",
      "|    time_elapsed      | 22267     |\n",
      "|    total_timesteps   | 14568270  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0176    |\n",
      "|    n_updates         | 3629567   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -77060.00000000022\n",
      "Num timesteps: 14600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -57204.40\n",
      "No more requests.\n",
      "Total reward for this episode is -16959.999999999964\n",
      "No more requests.\n",
      "Total reward for this episode is -22519.99999999995\n",
      "No more requests.\n",
      "Total reward for this episode is -5159.999999999935\n",
      "------------------------------------\n",
      "| average_route_length | 3.65      |\n",
      "| blocked_contiguous   | 0.105     |\n",
      "| blocked_continuous   | 0.0526    |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.25e+04  |\n",
      "|    ep_rew_mean       | -5.47e+04 |\n",
      "|    exploration_rate  | 0.0715    |\n",
      "| time/                |           |\n",
      "|    episodes          | 728       |\n",
      "|    fps               | 653       |\n",
      "|    time_elapsed      | 22423     |\n",
      "|    total_timesteps   | 14660432  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0139    |\n",
      "|    n_updates         | 3652607   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -35599.99999999998\n",
      "No more requests.\n",
      "Total reward for this episode is -35039.99999999993\n",
      "No more requests.\n",
      "Total reward for this episode is -20619.99999999994\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -4380.000000000049\n",
      "------------------------------------\n",
      "| average_route_length | 3.88      |\n",
      "| blocked_contiguous   | 0.105     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.25e+04  |\n",
      "|    ep_rew_mean       | -5.25e+04 |\n",
      "|    exploration_rate  | 0.0656    |\n",
      "| time/                |           |\n",
      "|    episodes          | 732       |\n",
      "|    fps               | 653       |\n",
      "|    time_elapsed      | 22580     |\n",
      "|    total_timesteps   | 14753395  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00873   |\n",
      "|    n_updates         | 3675848   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -59500.000000000044\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -44680.00000000003\n",
      "Num timesteps: 14800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -51857.40\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -58820.00000000003\n",
      "No more requests.\n",
      "Total reward for this episode is -15179.999999999969\n",
      "------------------------------------\n",
      "| average_route_length | 4         |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.2       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.24e+04  |\n",
      "|    ep_rew_mean       | -5.09e+04 |\n",
      "|    exploration_rate  | 0.0598    |\n",
      "| time/                |           |\n",
      "|    episodes          | 736       |\n",
      "|    fps               | 652       |\n",
      "|    time_elapsed      | 22736     |\n",
      "|    total_timesteps   | 14845024  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00232   |\n",
      "|    n_updates         | 3698755   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -60220.00000000005\n",
      "No more requests.\n",
      "Total reward for this episode is -31259.999999999975\n",
      "No more requests.\n",
      "Total reward for this episode is -30859.999999999953\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -14879.999999999956\n",
      "------------------------------------\n",
      "| average_route_length | 4         |\n",
      "| blocked_contiguous   | 0.158     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.2       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.25e+04  |\n",
      "|    ep_rew_mean       | -4.97e+04 |\n",
      "|    exploration_rate  | 0.0541    |\n",
      "| time/                |           |\n",
      "|    episodes          | 740       |\n",
      "|    fps               | 652       |\n",
      "|    time_elapsed      | 22892     |\n",
      "|    total_timesteps   | 14935962  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0214    |\n",
      "|    n_updates         | 3721490   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -17059.999999999975\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -66060.00000000009\n",
      "Num timesteps: 15000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -48708.20\n",
      "No more requests.\n",
      "Total reward for this episode is -11200.000000000013\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -39179.99999999996\n",
      "------------------------------------\n",
      "| average_route_length | 4.71      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.24e+04  |\n",
      "|    ep_rew_mean       | -4.79e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 744       |\n",
      "|    fps               | 651       |\n",
      "|    time_elapsed      | 23048     |\n",
      "|    total_timesteps   | 15027247  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.77      |\n",
      "|    n_updates         | 3744311   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -14359.99999999997\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -84720.00000000019\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -70360.0000000001\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -55900.00000000004\n",
      "------------------------------------\n",
      "| average_route_length | 5.54      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.24e+04  |\n",
      "|    ep_rew_mean       | -4.71e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 748       |\n",
      "|    fps               | 651       |\n",
      "|    time_elapsed      | 23204     |\n",
      "|    total_timesteps   | 15119110  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00507   |\n",
      "|    n_updates         | 3767277   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -44960.00000000001\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -58820.00000000007\n",
      "No more requests.\n",
      "Total reward for this episode is -11140.000000000011\n",
      "Num timesteps: 15200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -47012.60\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -40800.000000000015\n",
      "------------------------------------\n",
      "| average_route_length | 4.71      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.26e+04  |\n",
      "|    ep_rew_mean       | -4.66e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 752       |\n",
      "|    fps               | 651       |\n",
      "|    time_elapsed      | 23361     |\n",
      "|    total_timesteps   | 15210239  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00968   |\n",
      "|    n_updates         | 3790059   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -52360.00000000005\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -67660.0000000001\n",
      "No more requests.\n",
      "Total reward for this episode is -31259.999999999967\n",
      "No more requests.\n",
      "Total reward for this episode is -44420.00000000004\n",
      "------------------------------------\n",
      "| average_route_length | 5.14      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.25e+04  |\n",
      "|    ep_rew_mean       | -4.59e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 756       |\n",
      "|    fps               | 650       |\n",
      "|    time_elapsed      | 23517     |\n",
      "|    total_timesteps   | 15295703  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00299   |\n",
      "|    n_updates         | 3811425   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -29099.999999999978\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -16860.000000000004\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -15860.000000000002\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -56240.000000000044\n",
      "-----------------------------------\n",
      "| average_route_length | 5.69     |\n",
      "| blocked_contiguous   | 0.263    |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.35     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.2e+04  |\n",
      "|    ep_rew_mean       | -4.4e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 760      |\n",
      "|    fps               | 650      |\n",
      "|    time_elapsed      | 23602    |\n",
      "|    total_timesteps   | 15344441 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0158   |\n",
      "|    n_updates         | 3823610  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -31339.999999999953\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -42980.00000000001\n",
      "Num timesteps: 15400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -43005.40\n",
      "No more requests.\n",
      "Total reward for this episode is -32439.999999999985\n",
      "No more requests.\n",
      "Total reward for this episode is -26019.99999999998\n",
      "------------------------------------\n",
      "| average_route_length | 4.27      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.2e+04   |\n",
      "|    ep_rew_mean       | -4.28e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 764       |\n",
      "|    fps               | 649       |\n",
      "|    time_elapsed      | 23759     |\n",
      "|    total_timesteps   | 15436163  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0045    |\n",
      "|    n_updates         | 3846540   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -29259.999999999967\n",
      "No more requests.\n",
      "Total reward for this episode is -56000.000000000044\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -44239.99999999999\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -42199.999999999985\n",
      "------------------------------------\n",
      "| average_route_length | 5         |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.19e+04  |\n",
      "|    ep_rew_mean       | -4.15e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 768       |\n",
      "|    fps               | 649       |\n",
      "|    time_elapsed      | 23915     |\n",
      "|    total_timesteps   | 15524351  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.017     |\n",
      "|    n_updates         | 3868587   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -16919.999999999975\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -31499.99999999994\n",
      "No more requests.\n",
      "Total reward for this episode is 10460.000000000033\n",
      "Num timesteps: 15600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -40063.80\n",
      "No more requests.\n",
      "Total reward for this episode is -30579.99999999995\n",
      "------------------------------------\n",
      "| average_route_length | 4.67      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.19e+04  |\n",
      "|    ep_rew_mean       | -3.99e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 772       |\n",
      "|    fps               | 648       |\n",
      "|    time_elapsed      | 24072     |\n",
      "|    total_timesteps   | 15617019  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0159    |\n",
      "|    n_updates         | 3891754   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -44800.00000000002\n",
      "No more requests.\n",
      "Total reward for this episode is -44080.00000000001\n",
      "No more requests.\n",
      "Total reward for this episode is -45440.00000000003\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -30419.99999999997\n",
      "------------------------------------\n",
      "| average_route_length | 4.67      |\n",
      "| blocked_contiguous   | 0.158     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.22e+04  |\n",
      "|    ep_rew_mean       | -4.02e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 776       |\n",
      "|    fps               | 648       |\n",
      "|    time_elapsed      | 24230     |\n",
      "|    total_timesteps   | 15710281  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00224   |\n",
      "|    n_updates         | 3915070   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -28719.99999999996\n",
      "No more requests.\n",
      "Total reward for this episode is -58100.00000000002\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -15159.999999999967\n",
      "Num timesteps: 15800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -39111.40\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -29460.000000000025\n",
      "------------------------------------\n",
      "| average_route_length | 4.53      |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.22e+04  |\n",
      "|    ep_rew_mean       | -3.89e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 780       |\n",
      "|    fps               | 648       |\n",
      "|    time_elapsed      | 24386     |\n",
      "|    total_timesteps   | 15803517  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00797   |\n",
      "|    n_updates         | 3938379   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -56560.00000000004\n",
      "No more requests.\n",
      "Total reward for this episode is -15399.999999999978\n",
      "No more requests.\n",
      "Total reward for this episode is -29219.99999999995\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -56200.000000000044\n",
      "------------------------------------\n",
      "| average_route_length | 5.54      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.22e+04  |\n",
      "|    ep_rew_mean       | -3.76e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 784       |\n",
      "|    fps               | 647       |\n",
      "|    time_elapsed      | 24542     |\n",
      "|    total_timesteps   | 15895760  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00047   |\n",
      "|    n_updates         | 3961439   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -29679.99999999997\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -70780.0000000001\n",
      "No more requests.\n",
      "Total reward for this episode is -44219.99999999997\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -31339.999999999964\n",
      "------------------------------------\n",
      "| average_route_length | 4.8       |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.23e+04  |\n",
      "|    ep_rew_mean       | -3.82e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 788       |\n",
      "|    fps               | 647       |\n",
      "|    time_elapsed      | 24699     |\n",
      "|    total_timesteps   | 15986315  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00391   |\n",
      "|    n_updates         | 3984078   |\n",
      "------------------------------------\n",
      "Num timesteps: 16000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -38220.60\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -44220.0\n",
      "No more requests.\n",
      "Total reward for this episode is -56980.000000000065\n",
      "No more requests.\n",
      "Total reward for this episode is -59480.00000000007\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -69120.00000000016\n",
      "------------------------------------\n",
      "| average_route_length | 6.17      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.4       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.23e+04  |\n",
      "|    ep_rew_mean       | -3.85e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 792       |\n",
      "|    fps               | 646       |\n",
      "|    time_elapsed      | 24855     |\n",
      "|    total_timesteps   | 16076940  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0231    |\n",
      "|    n_updates         | 4006734   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -32019.99999999996\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -43779.99999999998\n",
      "No more requests.\n",
      "Total reward for this episode is -15559.999999999987\n",
      "No more requests.\n",
      "Total reward for this episode is -29699.999999999967\n",
      "------------------------------------\n",
      "| average_route_length | 4.53      |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0.0526    |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.22e+04  |\n",
      "|    ep_rew_mean       | -3.82e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 796       |\n",
      "|    fps               | 646       |\n",
      "|    time_elapsed      | 25012     |\n",
      "|    total_timesteps   | 16167430  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00537   |\n",
      "|    n_updates         | 4029357   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -44860.0\n",
      "Num timesteps: 16200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -37926.00\n",
      "No more requests.\n",
      "Total reward for this episode is -42479.999999999985\n",
      "No more requests.\n",
      "Total reward for this episode is -45600.00000000003\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -55260.00000000005\n",
      "------------------------------------\n",
      "| average_route_length | 5.38      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.22e+04  |\n",
      "|    ep_rew_mean       | -3.82e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 800       |\n",
      "|    fps               | 645       |\n",
      "|    time_elapsed      | 25168     |\n",
      "|    total_timesteps   | 16255366  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00178   |\n",
      "|    n_updates         | 4051341   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -58040.00000000008\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -44560.0\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -14359.999999999956\n",
      "No more requests.\n",
      "Total reward for this episode is -29199.999999999956\n",
      "------------------------------------\n",
      "| average_route_length | 4.67      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.21e+04  |\n",
      "|    ep_rew_mean       | -3.74e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 804       |\n",
      "|    fps               | 645       |\n",
      "|    time_elapsed      | 25324     |\n",
      "|    total_timesteps   | 16345779  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00542   |\n",
      "|    n_updates         | 4073944   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -31559.999999999956\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -26499.99999999996\n",
      "Num timesteps: 16400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -37183.40\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -55180.000000000044\n",
      "No more requests.\n",
      "Total reward for this episode is -40379.99999999999\n",
      "------------------------------------\n",
      "| average_route_length | 4.71      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.21e+04  |\n",
      "|    ep_rew_mean       | -3.76e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 808       |\n",
      "|    fps               | 645       |\n",
      "|    time_elapsed      | 25483     |\n",
      "|    total_timesteps   | 16436738  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0191    |\n",
      "|    n_updates         | 4096684   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -57960.00000000004\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -27739.999999999975\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -43579.99999999999\n",
      "No more requests.\n",
      "Total reward for this episode is -32699.999999999956\n",
      "------------------------------------\n",
      "| average_route_length | 4.93      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.22e+04  |\n",
      "|    ep_rew_mean       | -3.79e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 812       |\n",
      "|    fps               | 644       |\n",
      "|    time_elapsed      | 25639     |\n",
      "|    total_timesteps   | 16527372  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0033    |\n",
      "|    n_updates         | 4119342   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -30159.999999999967\n",
      "No more requests.\n",
      "Total reward for this episode is -18919.999999999964\n",
      "No more requests.\n",
      "Total reward for this episode is -1279.999999999964\n",
      "Num timesteps: 16600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -37333.20\n",
      "No more requests.\n",
      "Total reward for this episode is -17779.999999999956\n",
      "------------------------------------\n",
      "| average_route_length | 4.38      |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.2       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.22e+04  |\n",
      "|    ep_rew_mean       | -3.72e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 816       |\n",
      "|    fps               | 644       |\n",
      "|    time_elapsed      | 25795     |\n",
      "|    total_timesteps   | 16617823  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00482   |\n",
      "|    n_updates         | 4141955   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -33399.99999999994\n",
      "No more requests.\n",
      "Total reward for this episode is -13919.999999999973\n",
      "No more requests.\n",
      "Total reward for this episode is -28219.99999999996\n",
      "No more requests.\n",
      "Total reward for this episode is -15459.99999999997\n",
      "------------------------------------\n",
      "| average_route_length | 4.12      |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.2       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.23e+04  |\n",
      "|    ep_rew_mean       | -3.71e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 820       |\n",
      "|    fps               | 643       |\n",
      "|    time_elapsed      | 25951     |\n",
      "|    total_timesteps   | 16708519  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 50.4      |\n",
      "|    n_updates         | 4164629   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -39059.99999999996\n",
      "No more requests.\n",
      "Total reward for this episode is -19759.999999999956\n",
      "No more requests.\n",
      "Total reward for this episode is -31979.999999999978\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -56520.00000000005\n",
      "-----------------------------------\n",
      "| average_route_length | 5.85     |\n",
      "| blocked_contiguous   | 0.316    |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.35     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.23e+04 |\n",
      "|    ep_rew_mean       | -3.7e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 824      |\n",
      "|    fps               | 643      |\n",
      "|    time_elapsed      | 26108    |\n",
      "|    total_timesteps   | 16799213 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00158  |\n",
      "|    n_updates         | 4187303  |\n",
      "-----------------------------------\n",
      "Num timesteps: 16800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -36986.20\n",
      "No more requests.\n",
      "Total reward for this episode is -4060.0000000000477\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -79860.00000000012\n",
      "No more requests.\n",
      "Total reward for this episode is -17139.99999999997\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -31439.999999999953\n",
      "------------------------------------\n",
      "| average_route_length | 4.93      |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.23e+04  |\n",
      "|    ep_rew_mean       | -3.71e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 828       |\n",
      "|    fps               | 643       |\n",
      "|    time_elapsed      | 26264     |\n",
      "|    total_timesteps   | 16890180  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00234   |\n",
      "|    n_updates         | 4210044   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -42980.000000000044\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -38899.99999999996\n",
      "No more requests.\n",
      "Total reward for this episode is -25839.999999999975\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -28799.999999999978\n",
      "------------------------------------\n",
      "| average_route_length | 4.53      |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.23e+04  |\n",
      "|    ep_rew_mean       | -3.75e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 832       |\n",
      "|    fps               | 642       |\n",
      "|    time_elapsed      | 26420     |\n",
      "|    total_timesteps   | 16981302  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00238   |\n",
      "|    n_updates         | 4232825   |\n",
      "------------------------------------\n",
      "Num timesteps: 17000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -37503.00\n",
      "No more requests.\n",
      "Total reward for this episode is -16399.999999999978\n",
      "No more requests.\n",
      "Total reward for this episode is -17679.99999999998\n",
      "No more requests.\n",
      "Total reward for this episode is -40779.99999999999\n",
      "No more requests.\n",
      "Total reward for this episode is -15520.000000000042\n",
      "------------------------------------\n",
      "| average_route_length | 4.12      |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.2       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.23e+04  |\n",
      "|    ep_rew_mean       | -3.66e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 836       |\n",
      "|    fps               | 642       |\n",
      "|    time_elapsed      | 26576     |\n",
      "|    total_timesteps   | 17072240  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00232   |\n",
      "|    n_updates         | 4255559   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -71060.0000000001\n",
      "No more requests.\n",
      "Total reward for this episode is -15039.99999999998\n",
      "No more requests.\n",
      "Total reward for this episode is -42620.0\n",
      "No more requests.\n",
      "Total reward for this episode is -26899.99999999997\n",
      "------------------------------------\n",
      "| average_route_length | 4.4       |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.23e+04  |\n",
      "|    ep_rew_mean       | -3.67e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 840       |\n",
      "|    fps               | 642       |\n",
      "|    time_elapsed      | 26734     |\n",
      "|    total_timesteps   | 17164351  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00428   |\n",
      "|    n_updates         | 4278587   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -56000.00000000006\n",
      "Num timesteps: 17200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -37093.40\n",
      "No more requests.\n",
      "Total reward for this episode is -17319.99999999998\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -29799.999999999956\n",
      "No more requests.\n",
      "Total reward for this episode is -29859.999999999956\n",
      "------------------------------------\n",
      "| average_route_length | 4.67      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.23e+04  |\n",
      "|    ep_rew_mean       | -3.66e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 844       |\n",
      "|    fps               | 641       |\n",
      "|    time_elapsed      | 26893     |\n",
      "|    total_timesteps   | 17256693  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00222   |\n",
      "|    n_updates         | 4301673   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -28699.99999999995\n",
      "No more requests.\n",
      "Total reward for this episode is -41960.0\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -65820.00000000009\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -56260.00000000002\n",
      "------------------------------------\n",
      "| average_route_length | 5.54      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.23e+04  |\n",
      "|    ep_rew_mean       | -3.62e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 848       |\n",
      "|    fps               | 641       |\n",
      "|    time_elapsed      | 27051     |\n",
      "|    total_timesteps   | 17348549  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0299    |\n",
      "|    n_updates         | 4324637   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -24939.999999999956\n",
      "No more requests.\n",
      "Total reward for this episode is -58200.000000000065\n",
      "Num timesteps: 17400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -35958.40\n",
      "No more requests.\n",
      "Total reward for this episode is -28619.99999999997\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -56700.00000000006\n",
      "------------------------------------\n",
      "| average_route_length | 5.54      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.23e+04  |\n",
      "|    ep_rew_mean       | -3.63e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 852       |\n",
      "|    fps               | 640       |\n",
      "|    time_elapsed      | 27208     |\n",
      "|    total_timesteps   | 17439842  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 33.3      |\n",
      "|    n_updates         | 4347460   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -27799.999999999967\n",
      "No more requests.\n",
      "Total reward for this episode is -43719.999999999985\n",
      "No more requests.\n",
      "Total reward for this episode is -29979.999999999945\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -56500.000000000044\n",
      "------------------------------------\n",
      "| average_route_length | 5.54      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.24e+04  |\n",
      "|    ep_rew_mean       | -3.58e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 856       |\n",
      "|    fps               | 640       |\n",
      "|    time_elapsed      | 27366     |\n",
      "|    total_timesteps   | 17532354  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0147    |\n",
      "|    n_updates         | 4370588   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -60240.0000000001\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -53880.000000000015\n",
      "Num timesteps: 17600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -36390.40\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -41599.99999999995\n",
      "No more requests.\n",
      "Total reward for this episode is -2379.9999999999736\n",
      "-----------------------------------\n",
      "| average_route_length | 4        |\n",
      "| blocked_contiguous   | 0.158    |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.15     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.28e+04 |\n",
      "|    ep_rew_mean       | -3.6e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 860      |\n",
      "|    fps               | 640      |\n",
      "|    time_elapsed      | 27527    |\n",
      "|    total_timesteps   | 17625339 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0537   |\n",
      "|    n_updates         | 4393834  |\n",
      "-----------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -42720.00000000001\n",
      "No more requests.\n",
      "Total reward for this episode is -39360.000000000015\n",
      "No more requests.\n",
      "Total reward for this episode is -16919.99999999997\n",
      "No more requests.\n",
      "Total reward for this episode is -45360.00000000006\n",
      "------------------------------------\n",
      "| average_route_length | 5.29      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.28e+04  |\n",
      "|    ep_rew_mean       | -3.61e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 864       |\n",
      "|    fps               | 639       |\n",
      "|    time_elapsed      | 27684     |\n",
      "|    total_timesteps   | 17716465  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0127    |\n",
      "|    n_updates         | 4416616   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -30499.999999999956\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -43900.000000000015\n",
      "No more requests.\n",
      "Total reward for this episode is -42619.99999999999\n",
      "Num timesteps: 17800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -35998.80\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -31819.99999999996\n",
      "------------------------------------\n",
      "| average_route_length | 4.8       |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.28e+04  |\n",
      "|    ep_rew_mean       | -3.59e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 868       |\n",
      "|    fps               | 639       |\n",
      "|    time_elapsed      | 27840     |\n",
      "|    total_timesteps   | 17807451  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 6.18      |\n",
      "|    n_updates         | 4439362   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -29419.999999999964\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -53420.00000000003\n",
      "No more requests.\n",
      "Total reward for this episode is -16279.999999999969\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -56560.00000000006\n",
      "------------------------------------\n",
      "| average_route_length | 5.54      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.28e+04  |\n",
      "|    ep_rew_mean       | -3.68e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 872       |\n",
      "|    fps               | 639       |\n",
      "|    time_elapsed      | 27996     |\n",
      "|    total_timesteps   | 17898440  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00183   |\n",
      "|    n_updates         | 4462109   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -29219.99999999997\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -44900.0\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -27719.99999999997\n",
      "No more requests.\n",
      "Total reward for this episode is -13400.00000000002\n",
      "------------------------------------\n",
      "| average_route_length | 3.88      |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.2       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.28e+04  |\n",
      "|    ep_rew_mean       | -3.64e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 876       |\n",
      "|    fps               | 638       |\n",
      "|    time_elapsed      | 28153     |\n",
      "|    total_timesteps   | 17986728  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.106     |\n",
      "|    n_updates         | 4484181   |\n",
      "------------------------------------\n",
      "Num timesteps: 18000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -36376.40\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -15799.999999999975\n",
      "No more requests.\n",
      "Total reward for this episode is -16279.999999999982\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -56360.00000000008\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -43840.00000000001\n",
      "------------------------------------\n",
      "| average_route_length | 5.29      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.27e+04  |\n",
      "|    ep_rew_mean       | -3.64e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 880       |\n",
      "|    fps               | 638       |\n",
      "|    time_elapsed      | 28309     |\n",
      "|    total_timesteps   | 18077532  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0171    |\n",
      "|    n_updates         | 4506882   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -28279.99999999996\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -45099.999999999985\n",
      "No more requests.\n",
      "Total reward for this episode is -44360.000000000015\n",
      "No more requests.\n",
      "Total reward for this episode is -42500.0\n",
      "------------------------------------\n",
      "| average_route_length | 5         |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.27e+04  |\n",
      "|    ep_rew_mean       | -3.64e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 884       |\n",
      "|    fps               | 638       |\n",
      "|    time_elapsed      | 28465     |\n",
      "|    total_timesteps   | 18168226  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00739   |\n",
      "|    n_updates         | 4529556   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -27919.99999999997\n",
      "Num timesteps: 18200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -36395.80\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -58940.00000000006\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -15219.999999999982\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -72720.00000000012\n",
      "------------------------------------\n",
      "| average_route_length | 6.67      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.4       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.27e+04  |\n",
      "|    ep_rew_mean       | -3.64e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 888       |\n",
      "|    fps               | 637       |\n",
      "|    time_elapsed      | 28622     |\n",
      "|    total_timesteps   | 18260249  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00481   |\n",
      "|    n_updates         | 4552562   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -30179.999999999985\n",
      "No more requests.\n",
      "Total reward for this episode is -44620.00000000001\n",
      "No more requests.\n",
      "Total reward for this episode is -55440.00000000006\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -29619.999999999975\n",
      "------------------------------------\n",
      "| average_route_length | 4.67      |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.27e+04  |\n",
      "|    ep_rew_mean       | -3.57e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 892       |\n",
      "|    fps               | 637       |\n",
      "|    time_elapsed      | 28778     |\n",
      "|    total_timesteps   | 18351638  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.382     |\n",
      "|    n_updates         | 4575409   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -29539.999999999964\n",
      "No more requests.\n",
      "Total reward for this episode is -46020.000000000015\n",
      "Num timesteps: 18400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -35697.40\n",
      "No more requests.\n",
      "Total reward for this episode is -44440.00000000003\n",
      "No more requests.\n",
      "Total reward for this episode is -44840.00000000002\n",
      "------------------------------------\n",
      "| average_route_length | 5.43      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.28e+04  |\n",
      "|    ep_rew_mean       | -3.61e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 896       |\n",
      "|    fps               | 637       |\n",
      "|    time_elapsed      | 28935     |\n",
      "|    total_timesteps   | 18442651  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 19.6      |\n",
      "|    n_updates         | 4598162   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -44540.000000000015\n",
      "No more requests.\n",
      "Total reward for this episode is -33519.99999999995\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -41380.000000000044\n",
      "No more requests.\n",
      "Total reward for this episode is -32339.999999999964\n",
      "------------------------------------\n",
      "| average_route_length | 4.93      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.28e+04  |\n",
      "|    ep_rew_mean       | -3.58e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 900       |\n",
      "|    fps               | 637       |\n",
      "|    time_elapsed      | 29091     |\n",
      "|    total_timesteps   | 18533576  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00122   |\n",
      "|    n_updates         | 4620893   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -28839.999999999964\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -41900.00000000002\n",
      "Num timesteps: 18600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -35454.80\n",
      "No more requests.\n",
      "Total reward for this episode is -19159.999999999964\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -46760.00000000002\n",
      "------------------------------------\n",
      "| average_route_length | 5.57      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.28e+04  |\n",
      "|    ep_rew_mean       | -3.57e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 904       |\n",
      "|    fps               | 636       |\n",
      "|    time_elapsed      | 29249     |\n",
      "|    total_timesteps   | 18625406  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00138   |\n",
      "|    n_updates         | 4643851   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -74960.00000000015\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -1099.9999999999686\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -41199.999999999956\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -9739.999999999987\n",
      "------------------------------------\n",
      "| average_route_length | 5         |\n",
      "| blocked_contiguous   | 0.0526    |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.6       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.27e+04  |\n",
      "|    ep_rew_mean       | -3.54e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 908       |\n",
      "|    fps               | 636       |\n",
      "|    time_elapsed      | 29387     |\n",
      "|    total_timesteps   | 18706228  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0026    |\n",
      "|    n_updates         | 4664056   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -26579.999999999967\n",
      "No more requests.\n",
      "Total reward for this episode is -58200.000000000044\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -28239.999999999964\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -46720.00000000001\n",
      "------------------------------------\n",
      "| average_route_length | 5.71      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.27e+04  |\n",
      "|    ep_rew_mean       | -3.54e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 912       |\n",
      "|    fps               | 636       |\n",
      "|    time_elapsed      | 29543     |\n",
      "|    total_timesteps   | 18796459  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00698   |\n",
      "|    n_updates         | 4686614   |\n",
      "------------------------------------\n",
      "Num timesteps: 18800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -35382.20\n",
      "No more requests.\n",
      "Total reward for this episode is -44719.99999999999\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -45399.999999999985\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -29719.99999999995\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -56700.000000000015\n",
      "------------------------------------\n",
      "| average_route_length | 5.85      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.27e+04  |\n",
      "|    ep_rew_mean       | -3.62e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 916       |\n",
      "|    fps               | 635       |\n",
      "|    time_elapsed      | 29705     |\n",
      "|    total_timesteps   | 18889361  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00877   |\n",
      "|    n_updates         | 4709840   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -14739.999999999969\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -69760.00000000009\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -57440.00000000006\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -16079.999999999973\n",
      "------------------------------------\n",
      "| average_route_length | 4.38      |\n",
      "| blocked_contiguous   | 0.158     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.2       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.27e+04  |\n",
      "|    ep_rew_mean       | -3.67e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 920       |\n",
      "|    fps               | 635       |\n",
      "|    time_elapsed      | 29864     |\n",
      "|    total_timesteps   | 18981855  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0214    |\n",
      "|    n_updates         | 4732963   |\n",
      "------------------------------------\n",
      "Num timesteps: 19000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -36716.40\n",
      "No more requests.\n",
      "Total reward for this episode is -39480.00000000001\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -45400.00000000002\n",
      "No more requests.\n",
      "Total reward for this episode is -44860.00000000003\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -84820.00000000017\n",
      "------------------------------------\n",
      "| average_route_length | 7.27      |\n",
      "| blocked_contiguous   | 0.368     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.45      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.27e+04  |\n",
      "|    ep_rew_mean       | -3.74e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 924       |\n",
      "|    fps               | 635       |\n",
      "|    time_elapsed      | 30021     |\n",
      "|    total_timesteps   | 19072565  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 54.6      |\n",
      "|    n_updates         | 4755641   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -61200.000000000124\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -43079.99999999999\n",
      "No more requests.\n",
      "Total reward for this episode is -54900.00000000004\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -69560.00000000013\n",
      "------------------------------------\n",
      "| average_route_length | 6.17      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.4       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.27e+04  |\n",
      "|    ep_rew_mean       | -3.83e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 928       |\n",
      "|    fps               | 635       |\n",
      "|    time_elapsed      | 30179     |\n",
      "|    total_timesteps   | 19165148  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 26.9      |\n",
      "|    n_updates         | 4778786   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -39919.99999999997\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -71100.00000000013\n",
      "Num timesteps: 19200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -38533.60\n",
      "No more requests.\n",
      "Total reward for this episode is -28619.999999999975\n",
      "No more requests.\n",
      "Total reward for this episode is -13299.999999999989\n",
      "------------------------------------\n",
      "| average_route_length | 4.12      |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.2       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.25e+04  |\n",
      "|    ep_rew_mean       | -3.84e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 932       |\n",
      "|    fps               | 633       |\n",
      "|    time_elapsed      | 30338     |\n",
      "|    total_timesteps   | 19228728  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.748     |\n",
      "|    n_updates         | 4794681   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -72040.0000000002\n",
      "No more requests.\n",
      "Total reward for this episode is -49560.00000000005\n",
      "No more requests.\n",
      "Total reward for this episode is -32779.99999999997\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 3400.0000000000027\n",
      "-----------------------------------\n",
      "| average_route_length | 3.75     |\n",
      "| blocked_contiguous   | 0        |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.6      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.23e+04 |\n",
      "|    ep_rew_mean       | -3.9e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 936      |\n",
      "|    fps               | 633      |\n",
      "|    time_elapsed      | 30473    |\n",
      "|    total_timesteps   | 19303167 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0293   |\n",
      "|    n_updates         | 4813291  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -15919.999999999967\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -30379.99999999997\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -29799.99999999998\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -72960.00000000017\n",
      "-----------------------------------\n",
      "| average_route_length | 6.67     |\n",
      "| blocked_contiguous   | 0.368    |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.4      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.23e+04 |\n",
      "|    ep_rew_mean       | -3.9e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 940      |\n",
      "|    fps               | 633      |\n",
      "|    time_elapsed      | 30630    |\n",
      "|    total_timesteps   | 19394414 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 312      |\n",
      "|    n_updates         | 4836103  |\n",
      "-----------------------------------\n",
      "Num timesteps: 19400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -39044.20\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -32259.999999999967\n",
      "No more requests.\n",
      "Total reward for this episode is -2659.999999999971\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -40859.999999999956\n",
      "No more requests.\n",
      "Total reward for this episode is -56420.00000000007\n",
      "-----------------------------------\n",
      "| average_route_length | 5.69     |\n",
      "| blocked_contiguous   | 0.368    |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.35     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.23e+04 |\n",
      "|    ep_rew_mean       | -3.9e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 944      |\n",
      "|    fps               | 632      |\n",
      "|    time_elapsed      | 30788    |\n",
      "|    total_timesteps   | 19484903 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00075  |\n",
      "|    n_updates         | 4858725  |\n",
      "-----------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -20779.99999999996\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -41120.0\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -46079.99999999999\n",
      "No more requests.\n",
      "Total reward for this episode is -17599.99999999996\n",
      "------------------------------------\n",
      "| average_route_length | 4.38      |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.2       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.23e+04  |\n",
      "|    ep_rew_mean       | -3.85e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 948       |\n",
      "|    fps               | 632       |\n",
      "|    time_elapsed      | 30945     |\n",
      "|    total_timesteps   | 19574075  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000992  |\n",
      "|    n_updates         | 4881018   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -43980.000000000044\n",
      "Num timesteps: 19600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -38660.20\n",
      "No more requests.\n",
      "Total reward for this episode is -42180.00000000004\n",
      "No more requests.\n",
      "Total reward for this episode is -45620.000000000015\n",
      "No more requests.\n",
      "Total reward for this episode is -819.9999999999591\n",
      "------------------------------------\n",
      "| average_route_length | 3.65      |\n",
      "| blocked_contiguous   | 0.158     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.22e+04  |\n",
      "|    ep_rew_mean       | -3.81e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 952       |\n",
      "|    fps               | 632       |\n",
      "|    time_elapsed      | 31102     |\n",
      "|    total_timesteps   | 19662812  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00312   |\n",
      "|    n_updates         | 4903202   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -41440.0\n",
      "No more requests.\n",
      "Total reward for this episode is -31299.99999999999\n",
      "No more requests.\n",
      "Total reward for this episode is -14099.999999999978\n",
      "No more requests.\n",
      "Total reward for this episode is -57660.00000000008\n",
      "------------------------------------\n",
      "| average_route_length | 5.54      |\n",
      "| blocked_contiguous   | 0.368     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.22e+04  |\n",
      "|    ep_rew_mean       | -3.81e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 956       |\n",
      "|    fps               | 631       |\n",
      "|    time_elapsed      | 31260     |\n",
      "|    total_timesteps   | 19751291  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000898  |\n",
      "|    n_updates         | 4925322   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -19099.999999999967\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -57220.00000000005\n",
      "Num timesteps: 19800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -37703.20\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -39139.99999999997\n",
      "No more requests.\n",
      "Total reward for this episode is -44920.000000000015\n",
      "------------------------------------\n",
      "| average_route_length | 5.43      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.21e+04  |\n",
      "|    ep_rew_mean       | -3.82e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 960       |\n",
      "|    fps               | 631       |\n",
      "|    time_elapsed      | 31419     |\n",
      "|    total_timesteps   | 19840131  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00078   |\n",
      "|    n_updates         | 4947532   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -33439.99999999995\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -27559.999999999967\n",
      "No more requests.\n",
      "Total reward for this episode is -60860.00000000008\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -41520.00000000001\n",
      "------------------------------------\n",
      "| average_route_length | 4.86      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.21e+04  |\n",
      "|    ep_rew_mean       | -3.83e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 964       |\n",
      "|    fps               | 631       |\n",
      "|    time_elapsed      | 31578     |\n",
      "|    total_timesteps   | 19928065  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00259   |\n",
      "|    n_updates         | 4969516   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -1799.9999999999616\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -39.999999999997726\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -62080.00000000007\n",
      "Num timesteps: 20000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -37650.60\n",
      "No more requests.\n",
      "Total reward for this episode is -43319.99999999999\n",
      "------------------------------------\n",
      "| average_route_length | 5.14      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.19e+04  |\n",
      "|    ep_rew_mean       | -3.78e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 968       |\n",
      "|    fps               | 630       |\n",
      "|    time_elapsed      | 31712     |\n",
      "|    total_timesteps   | 20002423  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000944  |\n",
      "|    n_updates         | 4988105   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -55440.00000000011\n",
      "No more requests.\n",
      "Total reward for this episode is -15299.99999999997\n",
      "No more requests.\n",
      "Total reward for this episode is -33200.0\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -56940.00000000003\n",
      "------------------------------------\n",
      "| average_route_length | 5.85      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.19e+04  |\n",
      "|    ep_rew_mean       | -3.78e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 972       |\n",
      "|    fps               | 630       |\n",
      "|    time_elapsed      | 31871     |\n",
      "|    total_timesteps   | 20089722  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000952  |\n",
      "|    n_updates         | 5009930   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -14199.999999999984\n",
      "No more requests.\n",
      "Total reward for this episode is -35599.99999999996\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -40180.000000000015\n",
      "No more requests.\n",
      "Total reward for this episode is 560.0000000001213\n",
      "------------------------------------\n",
      "| average_route_length | 3.44      |\n",
      "| blocked_contiguous   | 0.105     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.1       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.19e+04  |\n",
      "|    ep_rew_mean       | -3.76e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 976       |\n",
      "|    fps               | 629       |\n",
      "|    time_elapsed      | 32027     |\n",
      "|    total_timesteps   | 20176452  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00297   |\n",
      "|    n_updates         | 5031612   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -39599.999999999985\n",
      "Num timesteps: 20200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -37793.40\n",
      "No more requests.\n",
      "Total reward for this episode is -27199.999999999967\n",
      "No more requests.\n",
      "Total reward for this episode is -25959.999999999967\n",
      "No more requests.\n",
      "Total reward for this episode is -57660.00000000004\n",
      "------------------------------------\n",
      "| average_route_length | 6         |\n",
      "| blocked_contiguous   | 0.368     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.18e+04  |\n",
      "|    ep_rew_mean       | -3.77e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 980       |\n",
      "|    fps               | 629       |\n",
      "|    time_elapsed      | 32184     |\n",
      "|    total_timesteps   | 20262228  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00107   |\n",
      "|    n_updates         | 5053056   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -59080.00000000005\n",
      "No more requests.\n",
      "Total reward for this episode is -35019.99999999997\n",
      "No more requests.\n",
      "Total reward for this episode is -14359.999999999987\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -46180.00000000003\n",
      "------------------------------------\n",
      "| average_route_length | 5.43      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.18e+04  |\n",
      "|    ep_rew_mean       | -3.75e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 984       |\n",
      "|    fps               | 629       |\n",
      "|    time_elapsed      | 32346     |\n",
      "|    total_timesteps   | 20352341  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00254   |\n",
      "|    n_updates         | 5075585   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -38519.99999999997\n",
      "No more requests.\n",
      "Total reward for this episode is -43840.0\n",
      "Num timesteps: 20400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -37425.80\n",
      "No more requests.\n",
      "Total reward for this episode is -25079.99999999997\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -33099.99999999988\n",
      "------------------------------------\n",
      "| average_route_length | 4.27      |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.18e+04  |\n",
      "|    ep_rew_mean       | -3.71e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 988       |\n",
      "|    fps               | 628       |\n",
      "|    time_elapsed      | 32503     |\n",
      "|    total_timesteps   | 20438901  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0123    |\n",
      "|    n_updates         | 5097225   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -57180.000000000044\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -56820.00000000003\n",
      "No more requests.\n",
      "Total reward for this episode is -30379.999999999956\n",
      "No more requests.\n",
      "Total reward for this episode is -55720.00000000006\n",
      "------------------------------------\n",
      "| average_route_length | 5.54      |\n",
      "| blocked_contiguous   | 0.368     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.17e+04  |\n",
      "|    ep_rew_mean       | -3.75e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 992       |\n",
      "|    fps               | 628       |\n",
      "|    time_elapsed      | 32659     |\n",
      "|    total_timesteps   | 20524695  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0053    |\n",
      "|    n_updates         | 5118673   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -26899.999999999978\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -72540.00000000012\n",
      "No more requests.\n",
      "Total reward for this episode is -32259.999999999956\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 3220.0000000000045\n",
      "------------------------------------\n",
      "| average_route_length | 3.75      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.6       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.16e+04  |\n",
      "|    ep_rew_mean       | -3.72e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 996       |\n",
      "|    fps               | 628       |\n",
      "|    time_elapsed      | 32793     |\n",
      "|    total_timesteps   | 20599028  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000704  |\n",
      "|    n_updates         | 5137256   |\n",
      "------------------------------------\n",
      "Num timesteps: 20600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -37161.40\n",
      "No more requests.\n",
      "Total reward for this episode is -31319.99999999996\n",
      "No more requests.\n",
      "Total reward for this episode is -38359.99999999997\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -65420.00000000007\n",
      "No more requests.\n",
      "Total reward for this episode is -32859.999999999956\n",
      "------------------------------------\n",
      "| average_route_length | 5.07      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.15e+04  |\n",
      "|    ep_rew_mean       | -3.73e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1000      |\n",
      "|    fps               | 627       |\n",
      "|    time_elapsed      | 32950     |\n",
      "|    total_timesteps   | 20685877  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00394   |\n",
      "|    n_updates         | 5158969   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -29079.999999999967\n",
      "No more requests.\n",
      "Total reward for this episode is -28739.999999999964\n",
      "No more requests.\n",
      "Total reward for this episode is -17199.99999999997\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -58420.00000000007\n",
      "------------------------------------\n",
      "| average_route_length | 6         |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.15e+04  |\n",
      "|    ep_rew_mean       | -3.73e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1004      |\n",
      "|    fps               | 627       |\n",
      "|    time_elapsed      | 33107     |\n",
      "|    total_timesteps   | 20773173  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00375   |\n",
      "|    n_updates         | 5180793   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -45960.00000000002\n",
      "Num timesteps: 20800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -36898.00\n",
      "No more requests.\n",
      "Total reward for this episode is -45960.0\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -16319.999999999973\n",
      "No more requests.\n",
      "Total reward for this episode is -5479.99999999998\n",
      "------------------------------------\n",
      "| average_route_length | 4         |\n",
      "| blocked_contiguous   | 0.158     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.15e+04  |\n",
      "|    ep_rew_mean       | -3.71e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1008      |\n",
      "|    fps               | 627       |\n",
      "|    time_elapsed      | 33267     |\n",
      "|    total_timesteps   | 20861154  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00175   |\n",
      "|    n_updates         | 5202788   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -28739.999999999956\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -56420.00000000004\n",
      "No more requests.\n",
      "Total reward for this episode is -43439.999999999985\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -39519.999999999985\n",
      "-----------------------------------\n",
      "| average_route_length | 4.86     |\n",
      "| blocked_contiguous   | 0.263    |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.3      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.15e+04 |\n",
      "|    ep_rew_mean       | -3.7e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1012     |\n",
      "|    fps               | 626      |\n",
      "|    time_elapsed      | 33425    |\n",
      "|    total_timesteps   | 20948580 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000473 |\n",
      "|    n_updates         | 5224644  |\n",
      "-----------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -58760.00000000013\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -56540.00000000003\n",
      "Num timesteps: 21000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -37291.40\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -30079.99999999996\n",
      "No more requests.\n",
      "Total reward for this episode is -60120.00000000005\n",
      "------------------------------------\n",
      "| average_route_length | 6.31      |\n",
      "| blocked_contiguous   | 0.368     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.15e+04  |\n",
      "|    ep_rew_mean       | -3.75e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1016      |\n",
      "|    fps               | 626       |\n",
      "|    time_elapsed      | 33583     |\n",
      "|    total_timesteps   | 21035733  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000355  |\n",
      "|    n_updates         | 5246433   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -85760.00000000015\n",
      "No more requests.\n",
      "Total reward for this episode is -43320.0\n",
      "No more requests.\n",
      "Total reward for this episode is -31719.999999999964\n",
      "No more requests.\n",
      "Total reward for this episode is -27839.99999999998\n",
      "-----------------------------------\n",
      "| average_route_length | 4.53     |\n",
      "| blocked_contiguous   | 0.263    |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.25     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.14e+04 |\n",
      "|    ep_rew_mean       | -3.8e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1020     |\n",
      "|    fps               | 626      |\n",
      "|    time_elapsed      | 33740    |\n",
      "|    total_timesteps   | 21121791 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000449 |\n",
      "|    n_updates         | 5267947  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -17019.999999999985\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -69120.00000000012\n",
      "No more requests.\n",
      "Total reward for this episode is -56660.00000000006\n",
      "Num timesteps: 21200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -38081.00\n",
      "No more requests.\n",
      "Total reward for this episode is -30099.999999999956\n",
      "------------------------------------\n",
      "| average_route_length | 4.67      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.14e+04  |\n",
      "|    ep_rew_mean       | -3.75e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1024      |\n",
      "|    fps               | 625       |\n",
      "|    time_elapsed      | 33896     |\n",
      "|    total_timesteps   | 21207785  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00114   |\n",
      "|    n_updates         | 5289446   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -1379.9999999999868\n",
      "No more requests.\n",
      "Total reward for this episode is -72280.00000000016\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -58060.00000000007\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -42260.00000000002\n",
      "-----------------------------------\n",
      "| average_route_length | 5        |\n",
      "| blocked_contiguous   | 0.211    |\n",
      "| blocked_continuous   | 0.0526   |\n",
      "| blocking_ratio       | 0.3      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.12e+04 |\n",
      "|    ep_rew_mean       | -3.7e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1028     |\n",
      "|    fps               | 625      |\n",
      "|    time_elapsed      | 34036    |\n",
      "|    total_timesteps   | 21284750 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.544    |\n",
      "|    n_updates         | 5308687  |\n",
      "-----------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -15459.999999999965\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -57280.00000000007\n",
      "No more requests.\n",
      "Total reward for this episode is -43440.00000000002\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -60200.000000000065\n",
      "------------------------------------\n",
      "| average_route_length | 6.15      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.14e+04  |\n",
      "|    ep_rew_mean       | -3.73e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1032      |\n",
      "|    fps               | 625       |\n",
      "|    time_elapsed      | 34194     |\n",
      "|    total_timesteps   | 21371826  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00391   |\n",
      "|    n_updates         | 5330456   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -57020.00000000007\n",
      "Num timesteps: 21400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -37173.80\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -54360.00000000005\n",
      "No more requests.\n",
      "Total reward for this episode is -57780.000000000015\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 20.000000000004547\n",
      "------------------------------------\n",
      "| average_route_length | 4         |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.65      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.14e+04  |\n",
      "|    ep_rew_mean       | -3.74e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1036      |\n",
      "|    fps               | 624       |\n",
      "|    time_elapsed      | 34329     |\n",
      "|    total_timesteps   | 21445151  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00136   |\n",
      "|    n_updates         | 5348787   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -40180.00000000001\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -43499.99999999998\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -30980.00000000002\n",
      "No more requests.\n",
      "Total reward for this episode is -40640.0\n",
      "------------------------------------\n",
      "| average_route_length | 5         |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.14e+04  |\n",
      "|    ep_rew_mean       | -3.73e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1040      |\n",
      "|    fps               | 624       |\n",
      "|    time_elapsed      | 34489     |\n",
      "|    total_timesteps   | 21533267  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 10.7      |\n",
      "|    n_updates         | 5370816   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -44660.000000000015\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -50300.000000000015\n",
      "No more requests.\n",
      "Total reward for this episode is -42480.0\n",
      "Num timesteps: 21600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -37974.60\n",
      "No more requests.\n",
      "Total reward for this episode is -33059.999999999956\n",
      "------------------------------------\n",
      "| average_route_length | 5.07      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.13e+04  |\n",
      "|    ep_rew_mean       | -3.77e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1044      |\n",
      "|    fps               | 624       |\n",
      "|    time_elapsed      | 34646     |\n",
      "|    total_timesteps   | 21619485  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00203   |\n",
      "|    n_updates         | 5392371   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -57860.00000000008\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -42819.99999999999\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -28839.999999999975\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -40020.0\n",
      "------------------------------------\n",
      "| average_route_length | 4.71      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.13e+04  |\n",
      "|    ep_rew_mean       | -3.81e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1048      |\n",
      "|    fps               | 623       |\n",
      "|    time_elapsed      | 34805     |\n",
      "|    total_timesteps   | 21707549  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000373  |\n",
      "|    n_updates         | 5414387   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -41800.0\n",
      "No more requests.\n",
      "Total reward for this episode is -42619.99999999999\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -58900.000000000065\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -39079.99999999996\n",
      "------------------------------------\n",
      "| average_route_length | 4.57      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.13e+04  |\n",
      "|    ep_rew_mean       | -3.85e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1052      |\n",
      "|    fps               | 623       |\n",
      "|    time_elapsed      | 34963     |\n",
      "|    total_timesteps   | 21795320  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00037   |\n",
      "|    n_updates         | 5436329   |\n",
      "------------------------------------\n",
      "Num timesteps: 21800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -38466.60\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -40959.99999999999\n",
      "No more requests.\n",
      "Total reward for this episode is 1019.9999999999733\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -45839.99999999998\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -55000.00000000006\n",
      "------------------------------------\n",
      "| average_route_length | 5.54      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.13e+04  |\n",
      "|    ep_rew_mean       | -3.84e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1056      |\n",
      "|    fps               | 623       |\n",
      "|    time_elapsed      | 35120     |\n",
      "|    total_timesteps   | 21881032  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000607  |\n",
      "|    n_updates         | 5457757   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -44680.0\n",
      "No more requests.\n",
      "Total reward for this episode is -25779.99999999997\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -14439.999999999969\n",
      "No more requests.\n",
      "Total reward for this episode is -27139.999999999975\n",
      "-----------------------------------\n",
      "| average_route_length | 4.4      |\n",
      "| blocked_contiguous   | 0.263    |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.25     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.13e+04 |\n",
      "|    ep_rew_mean       | -3.8e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1060     |\n",
      "|    fps               | 622      |\n",
      "|    time_elapsed      | 35278    |\n",
      "|    total_timesteps   | 21967971 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000621 |\n",
      "|    n_updates         | 5479492  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -41000.0\n",
      "Num timesteps: 22000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -38130.60\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -16619.999999999953\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -69160.00000000015\n",
      "No more requests.\n",
      "Total reward for this episode is -28619.99999999997\n",
      "-----------------------------------\n",
      "| average_route_length | 4.53     |\n",
      "| blocked_contiguous   | 0.263    |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.25     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.13e+04 |\n",
      "|    ep_rew_mean       | -3.8e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1064     |\n",
      "|    fps               | 622      |\n",
      "|    time_elapsed      | 35435    |\n",
      "|    total_timesteps   | 22055311 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00155  |\n",
      "|    n_updates         | 5501327  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -31259.999999999956\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -47159.99999999999\n",
      "No more requests.\n",
      "Total reward for this episode is -28199.99999999996\n",
      "No more requests.\n",
      "Total reward for this episode is -58280.00000000005\n",
      "------------------------------------\n",
      "| average_route_length | 5.85      |\n",
      "| blocked_contiguous   | 0.368     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.14e+04  |\n",
      "|    ep_rew_mean       | -3.86e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1068      |\n",
      "|    fps               | 622       |\n",
      "|    time_elapsed      | 35594     |\n",
      "|    total_timesteps   | 22142970  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00357   |\n",
      "|    n_updates         | 5523242   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -46200.0\n",
      "No more requests.\n",
      "Total reward for this episode is -74940.00000000017\n",
      "Num timesteps: 22200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -38956.40\n",
      "No more requests.\n",
      "Total reward for this episode is -46280.00000000001\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -44220.000000000015\n",
      "-----------------------------------\n",
      "| average_route_length | 5.14     |\n",
      "| blocked_contiguous   | 0.263    |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.3      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.14e+04 |\n",
      "|    ep_rew_mean       | -3.9e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1072     |\n",
      "|    fps               | 621      |\n",
      "|    time_elapsed      | 35754    |\n",
      "|    total_timesteps   | 22231232 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00281  |\n",
      "|    n_updates         | 5545307  |\n",
      "-----------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -46740.0\n",
      "No more requests.\n",
      "Total reward for this episode is -29279.99999999996\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -31679.999999999956\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -48260.000000000015\n",
      "------------------------------------\n",
      "| average_route_length | 5.86      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.14e+04  |\n",
      "|    ep_rew_mean       | -3.96e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1076      |\n",
      "|    fps               | 621       |\n",
      "|    time_elapsed      | 35911     |\n",
      "|    total_timesteps   | 22317708  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000639  |\n",
      "|    n_updates         | 5566926   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -35019.99999999996\n",
      "No more requests.\n",
      "Total reward for this episode is -20099.99999999998\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -32959.999999999956\n",
      "Stop hitting yourself\n",
      "Num timesteps: 22400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -39473.60\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -46999.99999999999\n",
      "------------------------------------\n",
      "| average_route_length | 5.86      |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.14e+04  |\n",
      "|    ep_rew_mean       | -3.94e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1080      |\n",
      "|    fps               | 621       |\n",
      "|    time_elapsed      | 36069     |\n",
      "|    total_timesteps   | 22404736  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000928  |\n",
      "|    n_updates         | 5588683   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -28659.999999999964\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -69120.00000000016\n",
      "No more requests.\n",
      "Total reward for this episode is -41660.0\n",
      "No more requests.\n",
      "Total reward for this episode is -41140.0\n",
      "------------------------------------\n",
      "| average_route_length | 4.86      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.14e+04  |\n",
      "|    ep_rew_mean       | -3.97e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1084      |\n",
      "|    fps               | 620       |\n",
      "|    time_elapsed      | 36227     |\n",
      "|    total_timesteps   | 22491531  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000614  |\n",
      "|    n_updates         | 5610382   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -40020.00000000001\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -41379.999999999956\n",
      "No more requests.\n",
      "Total reward for this episode is -28999.999999999967\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -57280.000000000015\n",
      "-----------------------------------\n",
      "| average_route_length | 6        |\n",
      "| blocked_contiguous   | 0.316    |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.35     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.14e+04 |\n",
      "|    ep_rew_mean       | -4e+04   |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1088     |\n",
      "|    fps               | 620      |\n",
      "|    time_elapsed      | 36385    |\n",
      "|    total_timesteps   | 22578818 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000605 |\n",
      "|    n_updates         | 5632204  |\n",
      "-----------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -7979.999999999985\n",
      "Num timesteps: 22600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -39505.20\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -71600.00000000013\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -43680.000000000015\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -27599.999999999993\n",
      "------------------------------------\n",
      "| average_route_length | 4.53      |\n",
      "| blocked_contiguous   | 0.158     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.13e+04  |\n",
      "|    ep_rew_mean       | -3.94e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1092      |\n",
      "|    fps               | 620       |\n",
      "|    time_elapsed      | 36523     |\n",
      "|    total_timesteps   | 22655428  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000283  |\n",
      "|    n_updates         | 5651356   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -44380.00000000005\n",
      "No more requests.\n",
      "Total reward for this episode is -29499.999999999967\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -43360.000000000015\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -28099.99999999997\n",
      "------------------------------------\n",
      "| average_route_length | 4.4       |\n",
      "| blocked_contiguous   | 0.158     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.15e+04  |\n",
      "|    ep_rew_mean       | -3.95e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1096      |\n",
      "|    fps               | 620       |\n",
      "|    time_elapsed      | 36684     |\n",
      "|    total_timesteps   | 22744263  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00133   |\n",
      "|    n_updates         | 5673565   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -29739.999999999964\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -44680.0\n",
      "Num timesteps: 22800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -39514.40\n",
      "No more requests.\n",
      "Total reward for this episode is -29279.99999999995\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -40859.99999999994\n",
      "------------------------------------\n",
      "| average_route_length | 4.71      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.15e+04  |\n",
      "|    ep_rew_mean       | -3.91e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1100      |\n",
      "|    fps               | 619       |\n",
      "|    time_elapsed      | 36842     |\n",
      "|    total_timesteps   | 22831821  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 17.2      |\n",
      "|    n_updates         | 5695455   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -15379.999999999976\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -56580.000000000044\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -16439.99999999997\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -58340.000000000065\n",
      "------------------------------------\n",
      "| average_route_length | 5.85      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.15e+04  |\n",
      "|    ep_rew_mean       | -3.93e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1104      |\n",
      "|    fps               | 619       |\n",
      "|    time_elapsed      | 36999     |\n",
      "|    total_timesteps   | 22918411  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000157  |\n",
      "|    n_updates         | 5717102   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -47540.00000000006\n",
      "No more requests.\n",
      "Total reward for this episode is -16839.999999999985\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -30079.99999999996\n",
      "Num timesteps: 23000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -39228.20\n",
      "No more requests.\n",
      "Total reward for this episode is -46479.99999999999\n",
      "------------------------------------\n",
      "| average_route_length | 5.71      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.15e+04  |\n",
      "|    ep_rew_mean       | -3.96e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1108      |\n",
      "|    fps               | 619       |\n",
      "|    time_elapsed      | 37158     |\n",
      "|    total_timesteps   | 23006972  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00109   |\n",
      "|    n_updates         | 5739242   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -40380.000000000015\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -43439.99999999998\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -28379.999999999956\n",
      "No more requests.\n",
      "Total reward for this episode is -42079.99999999999\n",
      "------------------------------------\n",
      "| average_route_length | 5         |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.15e+04  |\n",
      "|    ep_rew_mean       | -3.94e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1112      |\n",
      "|    fps               | 618       |\n",
      "|    time_elapsed      | 37318     |\n",
      "|    total_timesteps   | 23095882  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0022    |\n",
      "|    n_updates         | 5761470   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -29559.999999999967\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -53920.00000000001\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -44380.000000000015\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -29359.999999999956\n",
      "------------------------------------\n",
      "| average_route_length | 4.53      |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.15e+04  |\n",
      "|    ep_rew_mean       | -3.89e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1116      |\n",
      "|    fps               | 618       |\n",
      "|    time_elapsed      | 37477     |\n",
      "|    total_timesteps   | 23184494  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0183    |\n",
      "|    n_updates         | 5783623   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Num timesteps: 23200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -38912.00\n",
      "No more requests.\n",
      "Total reward for this episode is -31739.999999999985\n",
      "No more requests.\n",
      "Total reward for this episode is -28519.999999999993\n",
      "No more requests.\n",
      "Total reward for this episode is -29400.0\n",
      "No more requests.\n",
      "Total reward for this episode is -40460.00000000002\n",
      "------------------------------------\n",
      "| average_route_length | 5         |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.15e+04  |\n",
      "|    ep_rew_mean       | -3.83e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1120      |\n",
      "|    fps               | 618       |\n",
      "|    time_elapsed      | 37634     |\n",
      "|    total_timesteps   | 23271149  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00911   |\n",
      "|    n_updates         | 5805287   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -54140.00000000006\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -27259.999999999975\n",
      "No more requests.\n",
      "Total reward for this episode is -30520.0\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -69180.00000000015\n",
      "------------------------------------\n",
      "| average_route_length | 6.17      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.4       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.15e+04  |\n",
      "|    ep_rew_mean       | -3.84e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1124      |\n",
      "|    fps               | 618       |\n",
      "|    time_elapsed      | 37791     |\n",
      "|    total_timesteps   | 23358136  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00034   |\n",
      "|    n_updates         | 5827033   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -71300.0000000001\n",
      "Num timesteps: 23400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -39113.60\n",
      "No more requests.\n",
      "Total reward for this episode is -31939.999999999985\n",
      "No more requests.\n",
      "Total reward for this episode is -14219.999999999982\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -55860.00000000006\n",
      "------------------------------------\n",
      "| average_route_length | 5.54      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.16e+04  |\n",
      "|    ep_rew_mean       | -3.84e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1128      |\n",
      "|    fps               | 617       |\n",
      "|    time_elapsed      | 37949     |\n",
      "|    total_timesteps   | 23445887  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 1.68      |\n",
      "|    n_updates         | 5848971   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -41999.999999999985\n",
      "No more requests.\n",
      "Total reward for this episode is -16439.999999999964\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -55440.000000000065\n",
      "No more requests.\n",
      "Total reward for this episode is -40259.99999999999\n",
      "------------------------------------\n",
      "| average_route_length | 4.71      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.16e+04  |\n",
      "|    ep_rew_mean       | -3.81e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1132      |\n",
      "|    fps               | 617       |\n",
      "|    time_elapsed      | 38108     |\n",
      "|    total_timesteps   | 23534352  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0195    |\n",
      "|    n_updates         | 5871087   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -12199.999999999973\n",
      "No more requests.\n",
      "Total reward for this episode is -26979.99999999997\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "Num timesteps: 23600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -37461.40\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -55240.000000000044\n",
      "No more requests.\n",
      "Total reward for this episode is -29019.99999999996\n",
      "------------------------------------\n",
      "| average_route_length | 4.67      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.18e+04  |\n",
      "|    ep_rew_mean       | -3.76e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1136      |\n",
      "|    fps               | 617       |\n",
      "|    time_elapsed      | 38267     |\n",
      "|    total_timesteps   | 23621740  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000418  |\n",
      "|    n_updates         | 5892934   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -57940.00000000008\n",
      "No more requests.\n",
      "Total reward for this episode is -45780.00000000001\n",
      "No more requests.\n",
      "Total reward for this episode is -28079.99999999997\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -27800.00000000001\n",
      "------------------------------------\n",
      "| average_route_length | 4.4       |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.18e+04  |\n",
      "|    ep_rew_mean       | -3.79e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1140      |\n",
      "|    fps               | 617       |\n",
      "|    time_elapsed      | 38423     |\n",
      "|    total_timesteps   | 23709012  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000843  |\n",
      "|    n_updates         | 5914752   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -67460.00000000006\n",
      "No more requests.\n",
      "Total reward for this episode is -12599.999999999962\n",
      "No more requests.\n",
      "Total reward for this episode is -43260.000000000015\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -43420.00000000001\n",
      "------------------------------------\n",
      "| average_route_length | 5.29      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.18e+04  |\n",
      "|    ep_rew_mean       | -3.76e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1144      |\n",
      "|    fps               | 616       |\n",
      "|    time_elapsed      | 38583     |\n",
      "|    total_timesteps   | 23799085  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000538  |\n",
      "|    n_updates         | 5937271   |\n",
      "------------------------------------\n",
      "Num timesteps: 23800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -37632.40\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -31839.999999999956\n",
      "No more requests.\n",
      "Total reward for this episode is -2620.00000000001\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -29859.999999999956\n",
      "No more requests.\n",
      "Total reward for this episode is -16459.999999999978\n",
      "------------------------------------\n",
      "| average_route_length | 4.25      |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.2       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.18e+04  |\n",
      "|    ep_rew_mean       | -3.69e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1148      |\n",
      "|    fps               | 616       |\n",
      "|    time_elapsed      | 38740     |\n",
      "|    total_timesteps   | 23886807  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000254  |\n",
      "|    n_updates         | 5959201   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -13479.999999999973\n",
      "No more requests.\n",
      "Total reward for this episode is 1200.0000000000314\n",
      "No more requests.\n",
      "Total reward for this episode is -31479.99999999996\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -26539.999999999964\n",
      "------------------------------------\n",
      "| average_route_length | 4.4       |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.18e+04  |\n",
      "|    ep_rew_mean       | -3.58e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1152      |\n",
      "|    fps               | 616       |\n",
      "|    time_elapsed      | 38898     |\n",
      "|    total_timesteps   | 23975033  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 1.38      |\n",
      "|    n_updates         | 5981258   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -29379.999999999956\n",
      "Num timesteps: 24000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -35718.00\n",
      "No more requests.\n",
      "Total reward for this episode is -30499.999999999956\n",
      "No more requests.\n",
      "Total reward for this episode is -43020.00000000001\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -13800.000000000011\n",
      "------------------------------------\n",
      "| average_route_length | 4         |\n",
      "| blocked_contiguous   | 0.158     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.2       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.18e+04  |\n",
      "|    ep_rew_mean       | -3.56e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1156      |\n",
      "|    fps               | 616       |\n",
      "|    time_elapsed      | 39055     |\n",
      "|    total_timesteps   | 24062519  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00733   |\n",
      "|    n_updates         | 6003129   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -57600.000000000065\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -27559.999999999978\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -17579.99999999997\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -61900.00000000008\n",
      "------------------------------------\n",
      "| average_route_length | 6.46      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.18e+04  |\n",
      "|    ep_rew_mean       | -3.61e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1160      |\n",
      "|    fps               | 615       |\n",
      "|    time_elapsed      | 39213     |\n",
      "|    total_timesteps   | 24151279  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00361   |\n",
      "|    n_updates         | 6025319   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -16319.999999999982\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -40620.000000000015\n",
      "Num timesteps: 24200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -36114.20\n",
      "No more requests.\n",
      "Total reward for this episode is -5559.999999999987\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -44200.0\n",
      "------------------------------------\n",
      "| average_route_length | 5.29      |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.18e+04  |\n",
      "|    ep_rew_mean       | -3.56e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1164      |\n",
      "|    fps               | 615       |\n",
      "|    time_elapsed      | 39369     |\n",
      "|    total_timesteps   | 24238598  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 60.2      |\n",
      "|    n_updates         | 6047149   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -27959.999999999978\n",
      "No more requests.\n",
      "Total reward for this episode is -40139.99999999996\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -40960.0\n",
      "No more requests.\n",
      "Total reward for this episode is -3219.9999999999645\n",
      "------------------------------------\n",
      "| average_route_length | 4         |\n",
      "| blocked_contiguous   | 0.158     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.18e+04  |\n",
      "|    ep_rew_mean       | -3.52e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1168      |\n",
      "|    fps               | 615       |\n",
      "|    time_elapsed      | 39525     |\n",
      "|    total_timesteps   | 24326635  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00139   |\n",
      "|    n_updates         | 6069158   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -16959.999999999964\n",
      "No more requests.\n",
      "Total reward for this episode is -880.0000000000296\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -51240.00000000003\n",
      "Num timesteps: 24400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -34338.40\n",
      "No more requests.\n",
      "Total reward for this episode is -28739.999999999953\n",
      "------------------------------------\n",
      "| average_route_length | 4.53      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.18e+04  |\n",
      "|    ep_rew_mean       | -3.42e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1172      |\n",
      "|    fps               | 615       |\n",
      "|    time_elapsed      | 39682     |\n",
      "|    total_timesteps   | 24414115  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000633  |\n",
      "|    n_updates         | 6091028   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -56760.00000000004\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -30479.99999999995\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -46480.000000000015\n",
      "No more requests.\n",
      "Total reward for this episode is -29859.999999999964\n",
      "------------------------------------\n",
      "| average_route_length | 4.67      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.18e+04  |\n",
      "|    ep_rew_mean       | -3.43e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1176      |\n",
      "|    fps               | 615       |\n",
      "|    time_elapsed      | 39838     |\n",
      "|    total_timesteps   | 24501206  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0039    |\n",
      "|    n_updates         | 6112801   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -30419.999999999953\n",
      "No more requests.\n",
      "Total reward for this episode is -30559.999999999953\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -45020.00000000001\n",
      "No more requests.\n",
      "Total reward for this episode is -29979.999999999956\n",
      "------------------------------------\n",
      "| average_route_length | 4.67      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.19e+04  |\n",
      "|    ep_rew_mean       | -3.44e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1180      |\n",
      "|    fps               | 614       |\n",
      "|    time_elapsed      | 39996     |\n",
      "|    total_timesteps   | 24589929  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00173   |\n",
      "|    n_updates         | 6134982   |\n",
      "------------------------------------\n",
      "Num timesteps: 24600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -34371.80\n",
      "No more requests.\n",
      "Total reward for this episode is -27099.999999999956\n",
      "No more requests.\n",
      "Total reward for this episode is -14520.000000000031\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -29159.999999999967\n",
      "No more requests.\n",
      "Total reward for this episode is -38900.000000000015\n",
      "------------------------------------\n",
      "| average_route_length | 4.57      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.19e+04  |\n",
      "|    ep_rew_mean       | -3.38e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1184      |\n",
      "|    fps               | 614       |\n",
      "|    time_elapsed      | 40152     |\n",
      "|    total_timesteps   | 24677697  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.22      |\n",
      "|    n_updates         | 6156924   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 8020.000000000005\n",
      "No more requests.\n",
      "Total reward for this episode is -42900.000000000015\n",
      "No more requests.\n",
      "Total reward for this episode is -43259.99999999999\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -70780.00000000012\n",
      "------------------------------------\n",
      "| average_route_length | 6.33      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.4       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.18e+04  |\n",
      "|    ep_rew_mean       | -3.36e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1188      |\n",
      "|    fps               | 614       |\n",
      "|    time_elapsed      | 40289     |\n",
      "|    total_timesteps   | 24754388  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.1       |\n",
      "|    n_updates         | 6176096   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -57260.00000000006\n",
      "No more requests.\n",
      "Total reward for this episode is -17339.999999999967\n",
      "Num timesteps: 24800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -33427.40\n",
      "No more requests.\n",
      "Total reward for this episode is -53380.00000000009\n",
      "No more requests.\n",
      "Total reward for this episode is -16080.000000000007\n",
      "------------------------------------\n",
      "| average_route_length | 4.12      |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.2       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.19e+04  |\n",
      "|    ep_rew_mean       | -3.35e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1192      |\n",
      "|    fps               | 614       |\n",
      "|    time_elapsed      | 40447     |\n",
      "|    total_timesteps   | 24843143  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00122   |\n",
      "|    n_updates         | 6198285   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -51100.00000000004\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -27879.999999999967\n",
      "No more requests.\n",
      "Total reward for this episode is -1040.0000000000073\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -57060.00000000005\n",
      "------------------------------------\n",
      "| average_route_length | 5.85      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.19e+04  |\n",
      "|    ep_rew_mean       | -3.35e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1196      |\n",
      "|    fps               | 613       |\n",
      "|    time_elapsed      | 40604     |\n",
      "|    total_timesteps   | 24930714  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000265  |\n",
      "|    n_updates         | 6220178   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -52540.00000000002\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -56300.000000000044\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -28080.00000000002\n",
      "Num timesteps: 25000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -33765.80\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -55060.00000000004\n",
      "-----------------------------------\n",
      "| average_route_length | 5.54     |\n",
      "| blocked_contiguous   | 0.316    |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.35     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.19e+04 |\n",
      "|    ep_rew_mean       | -3.4e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1200     |\n",
      "|    fps               | 613      |\n",
      "|    time_elapsed      | 40763    |\n",
      "|    total_timesteps   | 25019874 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0104   |\n",
      "|    n_updates         | 6242468  |\n",
      "-----------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -27279.999999999967\n",
      "No more requests.\n",
      "Total reward for this episode is -30839.999999999964\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -26659.999999999967\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -53700.000000000044\n",
      "------------------------------------\n",
      "| average_route_length | 5.08      |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.19e+04  |\n",
      "|    ep_rew_mean       | -3.38e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1204      |\n",
      "|    fps               | 613       |\n",
      "|    time_elapsed      | 40921     |\n",
      "|    total_timesteps   | 25108925  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00318   |\n",
      "|    n_updates         | 6264731   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -44900.0\n",
      "No more requests.\n",
      "Total reward for this episode is -17319.999999999967\n",
      "No more requests.\n",
      "Total reward for this episode is -41540.0\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -53780.000000000044\n",
      "------------------------------------\n",
      "| average_route_length | 5.23      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.19e+04  |\n",
      "|    ep_rew_mean       | -3.41e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1208      |\n",
      "|    fps               | 613       |\n",
      "|    time_elapsed      | 41078     |\n",
      "|    total_timesteps   | 25196992  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000448  |\n",
      "|    n_updates         | 6286747   |\n",
      "------------------------------------\n",
      "Num timesteps: 25200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -34096.20\n",
      "No more requests.\n",
      "Total reward for this episode is -31379.99999999997\n",
      "No more requests.\n",
      "Total reward for this episode is -15679.999999999964\n",
      "No more requests.\n",
      "Total reward for this episode is -27679.999999999967\n",
      "No more requests.\n",
      "Total reward for this episode is -31899.999999999978\n",
      "------------------------------------\n",
      "| average_route_length | 4.8       |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.19e+04  |\n",
      "|    ep_rew_mean       | -3.38e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1212      |\n",
      "|    fps               | 613       |\n",
      "|    time_elapsed      | 41235     |\n",
      "|    total_timesteps   | 25285665  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00153   |\n",
      "|    n_updates         | 6308916   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -42780.00000000001\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -57640.00000000008\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -52480.000000000015\n",
      "No more requests.\n",
      "Total reward for this episode is -26779.99999999996\n",
      "------------------------------------\n",
      "| average_route_length | 4.4       |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.19e+04  |\n",
      "|    ep_rew_mean       | -3.41e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1216      |\n",
      "|    fps               | 613       |\n",
      "|    time_elapsed      | 41394     |\n",
      "|    total_timesteps   | 25375529  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0794    |\n",
      "|    n_updates         | 6331382   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 3580.000000000004\n",
      "Num timesteps: 25400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -33693.60\n",
      "No more requests.\n",
      "Total reward for this episode is -34619.999999999956\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -27800.000000000033\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -44499.999999999985\n",
      "------------------------------------\n",
      "| average_route_length | 5.29      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.18e+04  |\n",
      "|    ep_rew_mean       | -3.37e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1220      |\n",
      "|    fps               | 612       |\n",
      "|    time_elapsed      | 41531     |\n",
      "|    total_timesteps   | 25452168  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00126   |\n",
      "|    n_updates         | 6350541   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -30739.999999999953\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -59440.000000000044\n",
      "No more requests.\n",
      "Total reward for this episode is -27479.999999999978\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -46860.0\n",
      "------------------------------------\n",
      "| average_route_length | 5.57      |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.18e+04  |\n",
      "|    ep_rew_mean       | -3.33e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1224      |\n",
      "|    fps               | 612       |\n",
      "|    time_elapsed      | 41691     |\n",
      "|    total_timesteps   | 25542650  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000482  |\n",
      "|    n_updates         | 6373162   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -25159.999999999975\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -44979.99999999999\n",
      "Num timesteps: 25600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -32969.20\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -58020.00000000003\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -53780.000000000015\n",
      "------------------------------------\n",
      "| average_route_length | 5.23      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.19e+04  |\n",
      "|    ep_rew_mean       | -3.32e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1228      |\n",
      "|    fps               | 612       |\n",
      "|    time_elapsed      | 41851     |\n",
      "|    total_timesteps   | 25632761  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00145   |\n",
      "|    n_updates         | 6395690   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -43800.0\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -67320.00000000007\n",
      "No more requests.\n",
      "Total reward for this episode is -72380.00000000016\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -69080.00000000009\n",
      "-----------------------------------\n",
      "| average_route_length | 6        |\n",
      "| blocked_contiguous   | 0.263    |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.4      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.19e+04 |\n",
      "|    ep_rew_mean       | -3.4e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1232     |\n",
      "|    fps               | 612      |\n",
      "|    time_elapsed      | 42013    |\n",
      "|    total_timesteps   | 25723665 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000367 |\n",
      "|    n_updates         | 6418416  |\n",
      "-----------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -43079.999999999985\n",
      "No more requests.\n",
      "Total reward for this episode is -42140.00000000001\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -29539.999999999964\n",
      "Num timesteps: 25800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -34261.20\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -31919.99999999997\n",
      "------------------------------------\n",
      "| average_route_length | 4.93      |\n",
      "| blocked_contiguous   | 0.158     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.19e+04  |\n",
      "|    ep_rew_mean       | -3.42e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1236      |\n",
      "|    fps               | 612       |\n",
      "|    time_elapsed      | 42172     |\n",
      "|    total_timesteps   | 25813031  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 31.5      |\n",
      "|    n_updates         | 6440757   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -29799.99999999998\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -27039.999999999964\n",
      "No more requests.\n",
      "Total reward for this episode is -28979.999999999967\n",
      "No more requests.\n",
      "Total reward for this episode is -40780.000000000015\n",
      "------------------------------------\n",
      "| average_route_length | 4.86      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.19e+04  |\n",
      "|    ep_rew_mean       | -3.38e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1240      |\n",
      "|    fps               | 611       |\n",
      "|    time_elapsed      | 42330     |\n",
      "|    total_timesteps   | 25902290  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000702  |\n",
      "|    n_updates         | 6463072   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -28979.999999999956\n",
      "No more requests.\n",
      "Total reward for this episode is -43540.0\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -28199.999999999967\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -43740.00000000003\n",
      "------------------------------------\n",
      "| average_route_length | 5.14      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.19e+04  |\n",
      "|    ep_rew_mean       | -3.37e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1244      |\n",
      "|    fps               | 611       |\n",
      "|    time_elapsed      | 42487     |\n",
      "|    total_timesteps   | 25990531  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00187   |\n",
      "|    n_updates         | 6485132   |\n",
      "------------------------------------\n",
      "Num timesteps: 26000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -33737.40\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -28559.999999999975\n",
      "No more requests.\n",
      "Total reward for this episode is -26319.99999999997\n",
      "No more requests.\n",
      "Total reward for this episode is -46100.0\n",
      "No more requests.\n",
      "Total reward for this episode is -30539.99999999998\n",
      "------------------------------------\n",
      "| average_route_length | 4.67      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.19e+04  |\n",
      "|    ep_rew_mean       | -3.42e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1248      |\n",
      "|    fps               | 611       |\n",
      "|    time_elapsed      | 42644     |\n",
      "|    total_timesteps   | 26078348  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000781  |\n",
      "|    n_updates         | 6507086   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -55420.00000000007\n",
      "No more requests.\n",
      "Total reward for this episode is -15619.999999999982\n",
      "No more requests.\n",
      "Total reward for this episode is -26679.99999999997\n",
      "No more requests.\n",
      "Total reward for this episode is -32819.99999999997\n",
      "------------------------------------\n",
      "| average_route_length | 5.07      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.19e+04  |\n",
      "|    ep_rew_mean       | -3.48e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1252      |\n",
      "|    fps               | 611       |\n",
      "|    time_elapsed      | 42800     |\n",
      "|    total_timesteps   | 26166938  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 22.4      |\n",
      "|    n_updates         | 6529234   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -39300.000000000015\n",
      "Num timesteps: 26200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -34948.40\n",
      "No more requests.\n",
      "Total reward for this episode is -16459.99999999998\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -55840.00000000003\n",
      "No more requests.\n",
      "Total reward for this episode is -27659.999999999975\n",
      "-----------------------------------\n",
      "| average_route_length | 4.53     |\n",
      "| blocked_contiguous   | 0.211    |\n",
      "| blocked_continuous   | 0.0526   |\n",
      "| blocking_ratio       | 0.25     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.19e+04 |\n",
      "|    ep_rew_mean       | -3.5e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1256     |\n",
      "|    fps               | 611      |\n",
      "|    time_elapsed      | 42959    |\n",
      "|    total_timesteps   | 26256615 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00064  |\n",
      "|    n_updates         | 6551653  |\n",
      "-----------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -55760.00000000006\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -55820.00000000004\n",
      "No more requests.\n",
      "Total reward for this episode is -34180.00000000001\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -18860.000000000025\n",
      "------------------------------------\n",
      "| average_route_length | 4.62      |\n",
      "| blocked_contiguous   | 0.158     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.2       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.2e+04   |\n",
      "|    ep_rew_mean       | -3.49e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1260      |\n",
      "|    fps               | 611       |\n",
      "|    time_elapsed      | 43120     |\n",
      "|    total_timesteps   | 26347745  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00461   |\n",
      "|    n_updates         | 6574436   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -40719.999999999956\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -53600.000000000044\n",
      "Num timesteps: 26400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -35236.40\n",
      "No more requests.\n",
      "Total reward for this episode is -27199.99999999996\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -30239.999999999964\n",
      "------------------------------------\n",
      "| average_route_length | 4.67      |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.2e+04   |\n",
      "|    ep_rew_mean       | -3.53e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1264      |\n",
      "|    fps               | 610       |\n",
      "|    time_elapsed      | 43277     |\n",
      "|    total_timesteps   | 26436089  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00166   |\n",
      "|    n_updates         | 6596522   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -51520.00000000001\n",
      "No more requests.\n",
      "Total reward for this episode is -31879.99999999997\n",
      "No more requests.\n",
      "Total reward for this episode is -28239.999999999975\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 13960.000000000005\n",
      "------------------------------------\n",
      "| average_route_length | 3.27      |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.45      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.19e+04  |\n",
      "|    ep_rew_mean       | -3.51e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1268      |\n",
      "|    fps               | 610       |\n",
      "|    time_elapsed      | 43420     |\n",
      "|    total_timesteps   | 26516666  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00391   |\n",
      "|    n_updates         | 6616666   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -15259.999999999976\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -40199.999999999956\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -41139.99999999994\n",
      "Num timesteps: 26600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -35121.80\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -56180.00000000005\n",
      "------------------------------------\n",
      "| average_route_length | 5.69      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0.0526    |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.19e+04  |\n",
      "|    ep_rew_mean       | -3.53e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1272      |\n",
      "|    fps               | 610       |\n",
      "|    time_elapsed      | 43582     |\n",
      "|    total_timesteps   | 26607833  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.265     |\n",
      "|    n_updates         | 6639458   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 1100.0000000000086\n",
      "No more requests.\n",
      "Total reward for this episode is -59980.00000000009\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -55180.000000000065\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -44320.00000000001\n",
      "------------------------------------\n",
      "| average_route_length | 5.29      |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.18e+04  |\n",
      "|    ep_rew_mean       | -3.51e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1276      |\n",
      "|    fps               | 610       |\n",
      "|    time_elapsed      | 43718     |\n",
      "|    total_timesteps   | 26684481  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000761  |\n",
      "|    n_updates         | 6658620   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -29519.99999999997\n",
      "No more requests.\n",
      "Total reward for this episode is -17839.99999999996\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -29439.999999999964\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -50600.00000000002\n",
      "-----------------------------------\n",
      "| average_route_length | 4.77     |\n",
      "| blocked_contiguous   | 0.316    |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.35     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.18e+04 |\n",
      "|    ep_rew_mean       | -3.5e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1280     |\n",
      "|    fps               | 610      |\n",
      "|    time_elapsed      | 43875    |\n",
      "|    total_timesteps   | 26772348 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.0244   |\n",
      "|    n_updates         | 6680586  |\n",
      "-----------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -31139.999999999964\n",
      "Num timesteps: 26800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -35085.20\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -26959.999999999975\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -27379.99999999997\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -33599.99999999996\n",
      "-----------------------------------\n",
      "| average_route_length | 5.07     |\n",
      "| blocked_contiguous   | 0.158    |\n",
      "| blocked_continuous   | 0.0526   |\n",
      "| blocking_ratio       | 0.25     |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.18e+04 |\n",
      "|    ep_rew_mean       | -3.5e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1284     |\n",
      "|    fps               | 610      |\n",
      "|    time_elapsed      | 44033    |\n",
      "|    total_timesteps   | 26861687 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000325 |\n",
      "|    n_updates         | 6702921  |\n",
      "-----------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -12020.000000000024\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -4279.999999999984\n",
      "No more requests.\n",
      "Total reward for this episode is -28899.99999999997\n",
      "No more requests.\n",
      "Total reward for this episode is -44780.0\n",
      "------------------------------------\n",
      "| average_route_length | 5.43      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.2e+04   |\n",
      "|    ep_rew_mean       | -3.44e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1288      |\n",
      "|    fps               | 609       |\n",
      "|    time_elapsed      | 44190     |\n",
      "|    total_timesteps   | 26950754  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000602  |\n",
      "|    n_updates         | 6725188   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -57620.00000000006\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -33579.99999999995\n",
      "Num timesteps: 27000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -34721.00\n",
      "No more requests.\n",
      "Total reward for this episode is -31619.99999999996\n",
      "No more requests.\n",
      "Total reward for this episode is -15019.999999999985\n",
      "------------------------------------\n",
      "| average_route_length | 4.25      |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.2       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.2e+04   |\n",
      "|    ep_rew_mean       | -3.45e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1292      |\n",
      "|    fps               | 609       |\n",
      "|    time_elapsed      | 44346     |\n",
      "|    total_timesteps   | 27038918  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000407  |\n",
      "|    n_updates         | 6747229   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -43540.000000000015\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -44239.99999999999\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -69360.0000000001\n",
      "No more requests.\n",
      "Total reward for this episode is -41720.00000000001\n",
      "-----------------------------------\n",
      "| average_route_length | 5        |\n",
      "| blocked_contiguous   | 0.316    |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.3      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.2e+04  |\n",
      "|    ep_rew_mean       | -3.5e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1296     |\n",
      "|    fps               | 609      |\n",
      "|    time_elapsed      | 44505    |\n",
      "|    total_timesteps   | 27128410 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000557 |\n",
      "|    n_updates         | 6769602  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -29119.999999999975\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -41140.000000000015\n",
      "No more requests.\n",
      "Total reward for this episode is -14560.000000000016\n",
      "Num timesteps: 27200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -34589.60\n",
      "No more requests.\n",
      "Total reward for this episode is -16659.999999999975\n",
      "------------------------------------\n",
      "| average_route_length | 4.5       |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.2       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.2e+04   |\n",
      "|    ep_rew_mean       | -3.42e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1300      |\n",
      "|    fps               | 609       |\n",
      "|    time_elapsed      | 44662     |\n",
      "|    total_timesteps   | 27216897  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00114   |\n",
      "|    n_updates         | 6791724   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -31059.999999999967\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 6960.000000000002\n",
      "No more requests.\n",
      "Total reward for this episode is -30319.99999999996\n",
      "No more requests.\n",
      "Total reward for this episode is -27739.999999999964\n",
      "------------------------------------\n",
      "| average_route_length | 4.4       |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.18e+04  |\n",
      "|    ep_rew_mean       | -3.37e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1304      |\n",
      "|    fps               | 609       |\n",
      "|    time_elapsed      | 44798     |\n",
      "|    total_timesteps   | 27293780  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00171   |\n",
      "|    n_updates         | 6810944   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -30079.99999999996\n",
      "No more requests.\n",
      "Total reward for this episode is -40840.000000000015\n",
      "No more requests.\n",
      "Total reward for this episode is -27359.999999999978\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -28200.000000000022\n",
      "------------------------------------\n",
      "| average_route_length | 4.53      |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.19e+04  |\n",
      "|    ep_rew_mean       | -3.34e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1308      |\n",
      "|    fps               | 609       |\n",
      "|    time_elapsed      | 44954     |\n",
      "|    total_timesteps   | 27382202  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000533  |\n",
      "|    n_updates         | 6833050   |\n",
      "------------------------------------\n",
      "Num timesteps: 27400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -33431.20\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -43480.000000000015\n",
      "No more requests.\n",
      "Total reward for this episode is -34799.99999999996\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 3340.0000000000064\n",
      "No more requests.\n",
      "Total reward for this episode is -42719.999999999985\n",
      "------------------------------------\n",
      "| average_route_length | 5.14      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.17e+04  |\n",
      "|    ep_rew_mean       | -3.34e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1312      |\n",
      "|    fps               | 608       |\n",
      "|    time_elapsed      | 45090     |\n",
      "|    total_timesteps   | 27458619  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0908    |\n",
      "|    n_updates         | 6852154   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -55060.00000000004\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -30379.999999999967\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -42240.00000000003\n",
      "No more requests.\n",
      "Total reward for this episode is -17779.999999999985\n",
      "------------------------------------\n",
      "| average_route_length | 4.38      |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.2       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.17e+04  |\n",
      "|    ep_rew_mean       | -3.32e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1316      |\n",
      "|    fps               | 608       |\n",
      "|    time_elapsed      | 45249     |\n",
      "|    total_timesteps   | 27548295  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00437   |\n",
      "|    n_updates         | 6874573   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -28319.99999999997\n",
      "No more requests.\n",
      "Total reward for this episode is -55140.00000000003\n",
      "Num timesteps: 27600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -33721.40\n",
      "No more requests.\n",
      "Total reward for this episode is -26739.999999999953\n",
      "No more requests.\n",
      "Total reward for this episode is -28719.999999999964\n",
      "------------------------------------\n",
      "| average_route_length | 4.67      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.18e+04  |\n",
      "|    ep_rew_mean       | -3.37e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1320      |\n",
      "|    fps               | 608       |\n",
      "|    time_elapsed      | 45405     |\n",
      "|    total_timesteps   | 27636787  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00142   |\n",
      "|    n_updates         | 6896696   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 6960.0000000000055\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -39859.99999999999\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -41900.000000000015\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -54420.00000000005\n",
      "------------------------------------\n",
      "| average_route_length | 5.38      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.17e+04  |\n",
      "|    ep_rew_mean       | -3.34e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1324      |\n",
      "|    fps               | 608       |\n",
      "|    time_elapsed      | 45544     |\n",
      "|    total_timesteps   | 27715073  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000438  |\n",
      "|    n_updates         | 6916268   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -20899.999999999978\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -41840.0\n",
      "No more requests.\n",
      "Total reward for this episode is -43300.00000000002\n",
      "Num timesteps: 27800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -33183.20\n",
      "No more requests.\n",
      "Total reward for this episode is -16659.999999999978\n",
      "------------------------------------\n",
      "| average_route_length | 4.25      |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.2       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.17e+04  |\n",
      "|    ep_rew_mean       | -3.29e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1328      |\n",
      "|    fps               | 608       |\n",
      "|    time_elapsed      | 45702     |\n",
      "|    total_timesteps   | 27804961  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 35.8      |\n",
      "|    n_updates         | 6938740   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -30299.999999999978\n",
      "No more requests.\n",
      "Total reward for this episode is -32979.99999999999\n",
      "No more requests.\n",
      "Total reward for this episode is -47180.00000000001\n",
      "No more requests.\n",
      "Total reward for this episode is -41020.00000000001\n",
      "------------------------------------\n",
      "| average_route_length | 4.86      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.17e+04  |\n",
      "|    ep_rew_mean       | -3.21e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1332      |\n",
      "|    fps               | 608       |\n",
      "|    time_elapsed      | 45861     |\n",
      "|    total_timesteps   | 27894165  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 31.5      |\n",
      "|    n_updates         | 6961041   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -25479.999999999978\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -15379.999999999984\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -57080.00000000005\n",
      "No more requests.\n",
      "Total reward for this episode is -41600.00000000002\n",
      "-----------------------------------\n",
      "| average_route_length | 5        |\n",
      "| blocked_contiguous   | 0.263    |\n",
      "| blocked_continuous   | 0.0526   |\n",
      "| blocking_ratio       | 0.3      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.17e+04 |\n",
      "|    ep_rew_mean       | -3.2e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1336     |\n",
      "|    fps               | 608      |\n",
      "|    time_elapsed      | 46019    |\n",
      "|    total_timesteps   | 27983729 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000553 |\n",
      "|    n_updates         | 6983432  |\n",
      "-----------------------------------\n",
      "Num timesteps: 28000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -32044.60\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -30739.99999999997\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -43179.99999999999\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -44140.000000000015\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -57720.00000000005\n",
      "------------------------------------\n",
      "| average_route_length | 5.85      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.17e+04  |\n",
      "|    ep_rew_mean       | -3.24e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1340      |\n",
      "|    fps               | 607       |\n",
      "|    time_elapsed      | 46179     |\n",
      "|    total_timesteps   | 28073756  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00137   |\n",
      "|    n_updates         | 7005938   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -2619.999999999954\n",
      "No more requests.\n",
      "Total reward for this episode is -26199.999999999978\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -30999.99999999998\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -15860.000000000002\n",
      "------------------------------------\n",
      "| average_route_length | 6         |\n",
      "| blocked_contiguous   | 0         |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.95      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.15e+04  |\n",
      "|    ep_rew_mean       | -3.17e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1344      |\n",
      "|    fps               | 607       |\n",
      "|    time_elapsed      | 46301     |\n",
      "|    total_timesteps   | 28142733  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000316  |\n",
      "|    n_updates         | 7023183   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is -14360.000000000002\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -16459.999999999975\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -16139.999999999975\n",
      "Num timesteps: 28200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -31190.20\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -51760.000000000015\n",
      "------------------------------------\n",
      "| average_route_length | 5.08      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.35      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.13e+04  |\n",
      "|    ep_rew_mean       | -3.14e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1348      |\n",
      "|    fps               | 607       |\n",
      "|    time_elapsed      | 46423     |\n",
      "|    total_timesteps   | 28211708  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 30.8      |\n",
      "|    n_updates         | 7040426   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -16619.999999999964\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -29439.999999999964\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 6960.0000000000055\n",
      "No more requests.\n",
      "Total reward for this episode is -41420.00000000003\n",
      "------------------------------------\n",
      "| average_route_length | 5         |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.12e+04  |\n",
      "|    ep_rew_mean       | -3.08e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1352      |\n",
      "|    fps               | 607       |\n",
      "|    time_elapsed      | 46562     |\n",
      "|    total_timesteps   | 28289568  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000297  |\n",
      "|    n_updates         | 7059891   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -40680.00000000003\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -50600.000000000015\n",
      "No more requests.\n",
      "Total reward for this episode is -14039.99999999998\n",
      "No more requests.\n",
      "Total reward for this episode is -42979.99999999997\n",
      "-----------------------------------\n",
      "| average_route_length | 5        |\n",
      "| blocked_contiguous   | 0.316    |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.3      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.12e+04 |\n",
      "|    ep_rew_mean       | -3.1e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1356     |\n",
      "|    fps               | 607      |\n",
      "|    time_elapsed      | 46718    |\n",
      "|    total_timesteps   | 28378040 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.00113  |\n",
      "|    n_updates         | 7082009  |\n",
      "-----------------------------------\n",
      "Stop hitting yourself\n",
      "Num timesteps: 28400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -30987.00\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -72840.00000000013\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -59020.00000000004\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -39379.999999999956\n",
      "No more requests.\n",
      "Total reward for this episode is -27799.99999999997\n",
      "------------------------------------\n",
      "| average_route_length | 4.4       |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.12e+04  |\n",
      "|    ep_rew_mean       | -3.14e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1360      |\n",
      "|    fps               | 607       |\n",
      "|    time_elapsed      | 46876     |\n",
      "|    total_timesteps   | 28467136  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00441   |\n",
      "|    n_updates         | 7104283   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -27139.999999999956\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -44360.0\n",
      "No more requests.\n",
      "Total reward for this episode is -43119.99999999999\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -27979.999999999956\n",
      "------------------------------------\n",
      "| average_route_length | 4.4       |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.12e+04  |\n",
      "|    ep_rew_mean       | -3.13e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1364      |\n",
      "|    fps               | 607       |\n",
      "|    time_elapsed      | 47033     |\n",
      "|    total_timesteps   | 28555254  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000801  |\n",
      "|    n_updates         | 7126313   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -29899.999999999967\n",
      "Num timesteps: 28600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -31130.40\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -41959.99999999997\n",
      "No more requests.\n",
      "Total reward for this episode is -42539.99999999998\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -45260.0\n",
      "------------------------------------\n",
      "| average_route_length | 5.43      |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.13e+04  |\n",
      "|    ep_rew_mean       | -3.18e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1368      |\n",
      "|    fps               | 606       |\n",
      "|    time_elapsed      | 47195     |\n",
      "|    total_timesteps   | 28647122  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000894  |\n",
      "|    n_updates         | 7149280   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -29439.999999999953\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -43780.000000000015\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -79860.00000000013\n",
      "No more requests.\n",
      "Total reward for this episode is -27159.999999999967\n",
      "------------------------------------\n",
      "| average_route_length | 4.27      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.13e+04  |\n",
      "|    ep_rew_mean       | -3.22e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1372      |\n",
      "|    fps               | 606       |\n",
      "|    time_elapsed      | 47353     |\n",
      "|    total_timesteps   | 28736805  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000895  |\n",
      "|    n_updates         | 7171701   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -54540.00000000002\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -25080.0\n",
      "Num timesteps: 28800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -32354.60\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -32679.99999999995\n",
      "No more requests.\n",
      "Total reward for this episode is -339.99999999996714\n",
      "------------------------------------\n",
      "| average_route_length | 3.65      |\n",
      "| blocked_contiguous   | 0.158     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.15      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.14e+04  |\n",
      "|    ep_rew_mean       | -3.18e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1376      |\n",
      "|    fps               | 606       |\n",
      "|    time_elapsed      | 47512     |\n",
      "|    total_timesteps   | 28826770  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.752     |\n",
      "|    n_updates         | 7194192   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -69840.00000000012\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -41960.0\n",
      "No more requests.\n",
      "Total reward for this episode is -18559.999999999975\n",
      "No more requests.\n",
      "Total reward for this episode is -14619.999999999973\n",
      "-----------------------------------\n",
      "| average_route_length | 4.12     |\n",
      "| blocked_contiguous   | 0.211    |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.2      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.14e+04 |\n",
      "|    ep_rew_mean       | -3.2e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1380     |\n",
      "|    fps               | 606      |\n",
      "|    time_elapsed      | 47669    |\n",
      "|    total_timesteps   | 28915210 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000618 |\n",
      "|    n_updates         | 7216302  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -43660.0\n",
      "No more requests.\n",
      "Total reward for this episode is -41900.000000000015\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -29679.99999999997\n",
      "Num timesteps: 29000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -32268.20\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -28659.999999999967\n",
      "------------------------------------\n",
      "| average_route_length | 4.67      |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.14e+04  |\n",
      "|    ep_rew_mean       | -3.22e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1384      |\n",
      "|    fps               | 606       |\n",
      "|    time_elapsed      | 47827     |\n",
      "|    total_timesteps   | 29005837  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0106    |\n",
      "|    n_updates         | 7238959   |\n",
      "------------------------------------\n",
      "Too many invalid actions.\n",
      "Total reward for this episode is 11900.000000000005\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -44360.0\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -28599.999999999978\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -24219.999999999975\n",
      "------------------------------------\n",
      "| average_route_length | 3.87      |\n",
      "| blocked_contiguous   | 0.158     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.13e+04  |\n",
      "|    ep_rew_mean       | -3.22e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1388      |\n",
      "|    fps               | 606       |\n",
      "|    time_elapsed      | 47967     |\n",
      "|    total_timesteps   | 29085541  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.026     |\n",
      "|    n_updates         | 7258885   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -29739.999999999964\n",
      "No more requests.\n",
      "Total reward for this episode is -44120.00000000003\n",
      "No more requests.\n",
      "Total reward for this episode is -27139.999999999975\n",
      "No more requests.\n",
      "Total reward for this episode is -42899.999999999985\n",
      "------------------------------------\n",
      "| average_route_length | 5.14      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.13e+04  |\n",
      "|    ep_rew_mean       | -3.22e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1392      |\n",
      "|    fps               | 606       |\n",
      "|    time_elapsed      | 48124     |\n",
      "|    total_timesteps   | 29173661  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 9.44      |\n",
      "|    n_updates         | 7280915   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -16399.99999999998\n",
      "Num timesteps: 29200000\n",
      "Best mean reward: -inf - Last mean reward per episode: -31955.40\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -41840.00000000001\n",
      "No more requests.\n",
      "Total reward for this episode is -29039.99999999997\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -28619.99999999997\n",
      "------------------------------------\n",
      "| average_route_length | 4.53      |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.13e+04  |\n",
      "|    ep_rew_mean       | -3.15e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1396      |\n",
      "|    fps               | 606       |\n",
      "|    time_elapsed      | 48281     |\n",
      "|    total_timesteps   | 29263258  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00184   |\n",
      "|    n_updates         | 7303314   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -57580.000000000044\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -58480.00000000003\n",
      "No more requests.\n",
      "Total reward for this episode is -29999.99999999997\n",
      "No more requests.\n",
      "Total reward for this episode is -41519.99999999999\n",
      "------------------------------------\n",
      "| average_route_length | 5.14      |\n",
      "| blocked_contiguous   | 0.316     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.14e+04  |\n",
      "|    ep_rew_mean       | -3.23e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1400      |\n",
      "|    fps               | 605       |\n",
      "|    time_elapsed      | 48439     |\n",
      "|    total_timesteps   | 29351952  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00448   |\n",
      "|    n_updates         | 7325487   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -28959.999999999964\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -42700.000000000015\n",
      "Num timesteps: 29400000\n",
      "Best mean reward: -inf - Last mean reward per episode: -32739.40\n",
      "No more requests.\n",
      "Total reward for this episode is -14379.999999999984\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -13379.999999999996\n",
      "------------------------------------\n",
      "| average_route_length | 4         |\n",
      "| blocked_contiguous   | 0.158     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.2       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.14e+04  |\n",
      "|    ep_rew_mean       | -3.24e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1404      |\n",
      "|    fps               | 605       |\n",
      "|    time_elapsed      | 48596     |\n",
      "|    total_timesteps   | 29429482  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000457  |\n",
      "|    n_updates         | 7344870   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -40740.00000000001\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -44340.000000000015\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -37659.99999999997\n",
      "Stop hitting yourself\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -43860.000000000015\n",
      "------------------------------------\n",
      "| average_route_length | 5.14      |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.3       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.13e+04  |\n",
      "|    ep_rew_mean       | -3.27e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1408      |\n",
      "|    fps               | 605       |\n",
      "|    time_elapsed      | 48755     |\n",
      "|    total_timesteps   | 29515897  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000693  |\n",
      "|    n_updates         | 7366474   |\n",
      "------------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -42620.00000000001\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -44960.0\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -58380.00000000009\n",
      "Num timesteps: 29600000\n",
      "Best mean reward: -inf - Last mean reward per episode: -33448.40\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -74820.00000000015\n",
      "------------------------------------\n",
      "| average_route_length | 6.83      |\n",
      "| blocked_contiguous   | 0.368     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.4       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.15e+04  |\n",
      "|    ep_rew_mean       | -3.38e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1412      |\n",
      "|    fps               | 605       |\n",
      "|    time_elapsed      | 48915     |\n",
      "|    total_timesteps   | 29606923  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.000487  |\n",
      "|    n_updates         | 7389230   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -40200.0\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -28679.99999999997\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -53080.00000000003\n",
      "No more requests.\n",
      "Total reward for this episode is -41379.999999999985\n",
      "-----------------------------------\n",
      "| average_route_length | 4.86     |\n",
      "| blocked_contiguous   | 0.316    |\n",
      "| blocked_continuous   | 0        |\n",
      "| blocking_ratio       | 0.3      |\n",
      "| rollout/             |          |\n",
      "|    ep_len_mean       | 2.15e+04 |\n",
      "|    ep_rew_mean       | -3.4e+04 |\n",
      "|    exploration_rate  | 0.05     |\n",
      "| time/                |          |\n",
      "|    episodes          | 1416     |\n",
      "|    fps               | 605      |\n",
      "|    time_elapsed      | 49071    |\n",
      "|    total_timesteps   | 29695635 |\n",
      "| train/               |          |\n",
      "|    learning_rate     | 0.0001   |\n",
      "|    loss              | 0.000256 |\n",
      "|    n_updates         | 7411408  |\n",
      "-----------------------------------\n",
      "No more requests.\n",
      "Total reward for this episode is -25339.999999999953\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -28599.999999999967\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -30339.999999999964\n",
      "No more requests.\n",
      "Total reward for this episode is -17399.99999999998\n",
      "------------------------------------\n",
      "| average_route_length | 4.38      |\n",
      "| blocked_contiguous   | 0.211     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.2       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.15e+04  |\n",
      "|    ep_rew_mean       | -3.35e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1420      |\n",
      "|    fps               | 605       |\n",
      "|    time_elapsed      | 49230     |\n",
      "|    total_timesteps   | 29785294  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00231   |\n",
      "|    n_updates         | 7433823   |\n",
      "------------------------------------\n",
      "Num timesteps: 29800000\n",
      "Best mean reward: -inf - Last mean reward per episode: -33472.80\n",
      "No more requests.\n",
      "Total reward for this episode is -44499.99999999999\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -28719.999999999975\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -57240.00000000006\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -16919.999999999975\n",
      "------------------------------------\n",
      "| average_route_length | 4.38      |\n",
      "| blocked_contiguous   | 0.158     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.2       |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.16e+04  |\n",
      "|    ep_rew_mean       | -3.37e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1424      |\n",
      "|    fps               | 604       |\n",
      "|    time_elapsed      | 49389     |\n",
      "|    total_timesteps   | 29875635  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.0176    |\n",
      "|    n_updates         | 7456408   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -43080.0\n",
      "No more requests.\n",
      "Total reward for this episode is -30619.99999999997\n",
      "No more requests.\n",
      "Total reward for this episode is -48060.00000000005\n",
      "No more requests.\n",
      "Total reward for this episode is -31919.99999999996\n",
      "------------------------------------\n",
      "| average_route_length | 4.93      |\n",
      "| blocked_contiguous   | 0.263     |\n",
      "| blocked_continuous   | 0         |\n",
      "| blocking_ratio       | 0.25      |\n",
      "| rollout/             |           |\n",
      "|    ep_len_mean       | 2.16e+04  |\n",
      "|    ep_rew_mean       | -3.41e+04 |\n",
      "|    exploration_rate  | 0.05      |\n",
      "| time/                |           |\n",
      "|    episodes          | 1428      |\n",
      "|    fps               | 604       |\n",
      "|    time_elapsed      | 49547     |\n",
      "|    total_timesteps   | 29965141  |\n",
      "| train/               |           |\n",
      "|    learning_rate     | 0.0001    |\n",
      "|    loss              | 0.00408   |\n",
      "|    n_updates         | 7478785   |\n",
      "------------------------------------\n",
      "Stop hitting yourself\n",
      "No more requests.\n",
      "Total reward for this episode is -48060.00000000006\n",
      "Num timesteps: 30000000\n",
      "Best mean reward: -inf - Last mean reward per episode: -34250.40\n"
     ]
    }
   ],
   "source": [
    "# training agent on 30 000 000 timesteps\n",
    "model_name = \"DQN_VSNL_TL_15\"\n",
    "model.learn(total_timesteps=30000000, callback=[callback, tensor_callback])\n",
    "model.save(model_name)\n",
    "\n",
    "# tensorboard --logdir ./15_traffic_load_model_log/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "984d45af26060c7a3801e889afafe0650c00f7a914aa62aa55fbbb9b9994d455"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('RL_2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
