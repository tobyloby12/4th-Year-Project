{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.3 (SDL 2.0.16, Python 3.7.11)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtrinity-actual\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    }
   ],
   "source": [
    "from optical_network_game.node import *\n",
    "from optical_network_game.link import *\n",
    "from optical_network_game.requests import *\n",
    "from optical_network_game.user import *\n",
    "import gym\n",
    "import pygame, sys\n",
    "from pygame.locals import *\n",
    "from gym import spaces\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#from stable_baselines.common.vec_env import DummyVecEnv\n",
    "#from stable_baselines.deepq.policies import MlpPolicy\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3 import A2C\n",
    "import json\n",
    "import cv2\n",
    "\n",
    "#additional code added by me just for testing\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "#importing IPython's display module to plot images\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython: from IPython import display\n",
    "from itertools import count\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "#importing wandb (weights and biases) for logging\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "import tensorboard\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "#Importing game_gym class for use\n",
    "import importlib\n",
    "import optical_network_game.game_gym\n",
    "importlib.reload(optical_network_game.game_gym)\n",
    "from optical_network_game.game_gym import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq:\n",
    "    :param log_dir: Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: Verbosity level.\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose: int = 1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, 'best_model')\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "          # Retrieve training reward\n",
    "          x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "          if len(x) > 0:\n",
    "              # Mean training reward over the last 100 episodes\n",
    "              mean_reward = np.mean(y[-100:])\n",
    "              if self.verbose > 0:\n",
    "                print(f\"Num timesteps: {self.num_timesteps}\")\n",
    "                print(f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\")\n",
    "\n",
    "              # New best model, you could save the agent here\n",
    "              if mean_reward > self.best_mean_reward:\n",
    "                  self.best_mean_reward = mean_reward\n",
    "                  # Example for saving best model\n",
    "                  if self.verbose > 0:\n",
    "                    print(f\"Saving new best model to {self.save_path}\")\n",
    "                  self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating fixed test topology\n",
    "def createTestTopology():\n",
    "    # testNodes\n",
    "    nodeA = Node(0, 'A', 300, 200)\n",
    "    nodeB = Node(1, 'B', 300, 400)\n",
    "    nodeC = Node(2, 'C', 650, 200)\n",
    "    nodeD = Node(3, 'D', 650, 400)\n",
    "    # testLinks\n",
    "    link1 = Link(0, nodeA, nodeB)\n",
    "    link2 = Link(1, nodeB, nodeC)\n",
    "    link3 = Link(2, nodeB, nodeD)\n",
    "    link4 = Link(3, nodeA, nodeC)\n",
    "    link5 = Link(4, nodeC, nodeD)\n",
    "\n",
    "    nodeList = [nodeA, nodeB, nodeC, nodeD]\n",
    "    linkList = [link1, link2, link3, link4, link5]\n",
    "\n",
    "    # save the links associated to each node in a list\n",
    "    for node in nodeList:\n",
    "        node.setLinks(linkList)\n",
    "    return nodeList, linkList\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create log dir for saving model\n",
    "log_dir = os.path.join(os.getcwd(), \"tmp/\")\n",
    "#os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Create log dir for tensorboard log\n",
    "log_tensorboard = os.path.join(os.getcwd(), \"tensorboard_logs/\")\n",
    "#os.makedirs(log_tensorboard, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights & Biases Hyperparameter Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'random',\n",
      " 'parameters': {'Gamma': {'values': [0.9, 0.92, 0.94, 0.96, 0.98, 0.99]},\n",
      "                'Tau': {'values': [0.4, 0.6, 0.8, 1]},\n",
      "                'learning_rate': {'values': [0.001,\n",
      "                                             0.0025,\n",
      "                                             0.005,\n",
      "                                             0.0075,\n",
      "                                             0.01]}}}\n"
     ]
    }
   ],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random'\n",
    "}\n",
    "\n",
    "hyper_params = {\n",
    "    'learning_rate': {\n",
    "        'values': [0.001, 0.0025, 0.005, 0.0075, 0.01]\n",
    "    },\n",
    "    'Gamma': {\n",
    "        'values': [0.9, 0.92, 0.94, 0.96, 0.98, 0.99]\n",
    "    },\n",
    "    'Tau': {\n",
    "        'values': [0.4, 0.6, 0.8, 1]\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_config['parameters'] = hyper_params\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: faxtqfe9\n",
      "Sweep URL: https://wandb.ai/trinity-actual/DQN_EON_Hyperparameter_Tuning/sweeps/faxtqfe9\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"DQN_EON_Hyperparameter_Tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DQN_HP_Tune(config=None):\n",
    "    from wandb.integration.sb3 import WandbCallback\n",
    "    \n",
    "    run = wandb.init(config=config, settings=wandb.Settings(start_method=\"fork\"), sync_tensorboard=True)\n",
    "\n",
    "    with run:\n",
    "        \n",
    "\n",
    "        config = wandb.config\n",
    "\n",
    "\n",
    "        Model_Name = \"DQN_Tune_070322\"\n",
    "        model_dir = \"./Models\"\n",
    "        log_tensorboard = \"./tensorboard_logs/\"\n",
    "        \n",
    "        # Create and wrap the environment\n",
    "        nodeList, linkList = createTestTopology()\n",
    "        #changed to only have 1 request per episode\n",
    "        #from 6 originally\n",
    "        requestList = generateRequests(nodeList, 6)\n",
    "        user = User()\n",
    "\n",
    "\n",
    "        env = game_gym(nodeList, linkList, requestList, user)\n",
    "        check_env(env)\n",
    "\n",
    "        eveon = Monitor(env, log_tensorboard)\n",
    "\n",
    "        #hyperparameters\n",
    "        timesteps = 500000\n",
    "        learning_starts = 50000\n",
    "        eps_fraction = 0.8\n",
    "        eps_start = 1\n",
    "        eps_end = 0.05\n",
    "        train_freq = (1000, \"step\")\n",
    "        target_update_interval = 50000\n",
    "\n",
    "\n",
    "\n",
    "        model = DQN('MlpPolicy', eveon, tau=config.Tau, learning_starts=learning_starts, buffer_size=10000, verbose=1, device=\"auto\", learning_rate=config.learning_rate, gamma=config.Gamma, exploration_fraction=eps_fraction, exploration_initial_eps=eps_start, exploration_final_eps=eps_end, target_update_interval=target_update_interval, train_freq=train_freq, tensorboard_log=f\"runs/{run.id}\")\n",
    "    \n",
    "        #training with hyperparameter tuning on lr, gamma and tau.\n",
    "        model.learn(total_timesteps=int(timesteps), callback=WandbCallback(verbose=2, model_save_path=f\"models/{run.id}\", model_save_freq=50000), reset_num_timesteps=False)\n",
    "\n",
    "\n",
    "        #Save_Model_Details(model, Model_Name, timesteps)\n",
    "        # # Close the environment\n",
    "        # eveon.close()\n",
    "\n",
    "        #Finishing wandb run\n",
    "        run.finish()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: ddvhln83 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tGamma: 0.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTau: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0075\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\wandb\\run-20220307_182510-ddvhln83</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/trinity-actual/DQN_EON_Hyperparameter_Tuning/runs/ddvhln83\" target=\"_blank\">young-sweep-1</a></strong> to <a href=\"https://wandb.ai/trinity-actual/DQN_EON_Hyperparameter_Tuning\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/trinity-actual/DQN_EON_Hyperparameter_Tuning/sweeps/faxtqfe9\" target=\"_blank\">https://wandb.ai/trinity-actual/DQN_EON_Hyperparameter_Tuning/sweeps/faxtqfe9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "(256, 256, 3)\n",
      "Using cuda device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:221: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 3.93GB > 0.88GB\n",
      "  \"This system does not have apparently enough memory to store the complete \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256, 3)\n",
      "Logging to ./tensorboard_logs/DQN_TUNE_0\n",
      "End episode, cumulative Reward:\n",
      "-6387.5999999999985\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-7779.2000000000035\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-3701.200000000008\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-3644.9000000000024\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1.66e+03  |\n",
      "|    ep_rew_mean      | -5.38e+03 |\n",
      "|    exploration_rate | 0.984     |\n",
      "| time/               |           |\n",
      "|    episodes         | 4         |\n",
      "|    fps              | 136       |\n",
      "|    time_elapsed     | 48        |\n",
      "|    total_timesteps  | 6647      |\n",
      "-----------------------------------\n",
      "End episode, cumulative Reward:\n",
      "-4473.200000000001\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-3336.100000000006\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-2773.500000000001\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-8536.399999999998\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1.62e+03  |\n",
      "|    ep_rew_mean      | -5.08e+03 |\n",
      "|    exploration_rate | 0.969     |\n",
      "| time/               |           |\n",
      "|    episodes         | 8         |\n",
      "|    fps              | 134       |\n",
      "|    time_elapsed     | 96        |\n",
      "|    total_timesteps  | 12965     |\n",
      "-----------------------------------\n",
      "End episode, cumulative Reward:\n",
      "-11753.3\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-4354.800000000005\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-2822.2999999999993\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-8190.600000000002\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1.68e+03  |\n",
      "|    ep_rew_mean      | -5.65e+03 |\n",
      "|    exploration_rate | 0.952     |\n",
      "| time/               |           |\n",
      "|    episodes         | 12        |\n",
      "|    fps              | 131       |\n",
      "|    time_elapsed     | 153       |\n",
      "|    total_timesteps  | 20102     |\n",
      "-----------------------------------\n",
      "End episode, cumulative Reward:\n",
      "-2028.0999999999976\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-3694.2000000000044\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-4043.800000000001\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-8592.899999999992\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1.61e+03  |\n",
      "|    ep_rew_mean      | -5.38e+03 |\n",
      "|    exploration_rate | 0.939     |\n",
      "| time/               |           |\n",
      "|    episodes         | 16        |\n",
      "|    fps              | 132       |\n",
      "|    time_elapsed     | 194       |\n",
      "|    total_timesteps  | 25794     |\n",
      "-----------------------------------\n",
      "End episode, cumulative Reward:\n",
      "-6683.099999999999\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-6852.100000000012\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-5072.0\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-6791.60000000001\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1.64e+03  |\n",
      "|    ep_rew_mean      | -5.57e+03 |\n",
      "|    exploration_rate | 0.922     |\n",
      "| time/               |           |\n",
      "|    episodes         | 20        |\n",
      "|    fps              | 133       |\n",
      "|    time_elapsed     | 244       |\n",
      "|    total_timesteps  | 32716     |\n",
      "-----------------------------------\n",
      "End episode, cumulative Reward:\n",
      "-5653.700000000007\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-6800.6\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-4329.2000000000035\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-6169.000000000003\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.62e+03 |\n",
      "|    ep_rew_mean      | -5.6e+03 |\n",
      "|    exploration_rate | 0.908    |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 136      |\n",
      "|    time_elapsed     | 284      |\n",
      "|    total_timesteps  | 38903    |\n",
      "----------------------------------\n",
      "End episode, cumulative Reward:\n",
      "-3897.3000000000015\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-7431.500000000008\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-2175.0000000000014\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-4512.500000000008\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1.64e+03  |\n",
      "|    ep_rew_mean      | -5.44e+03 |\n",
      "|    exploration_rate | 0.891     |\n",
      "| time/               |           |\n",
      "|    episodes         | 28        |\n",
      "|    fps              | 137       |\n",
      "|    time_elapsed     | 332       |\n",
      "|    total_timesteps  | 45812     |\n",
      "-----------------------------------\n",
      "End episode, cumulative Reward:\n",
      "-5065.300000000001\n",
      "(256, 256, 3)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20a7c4e396e44c59bc61b8dd4b51619a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">young-sweep-1</strong>: <a href=\"https://wandb.ai/trinity-actual/DQN_EON_Hyperparameter_Tuning/runs/ddvhln83\" target=\"_blank\">https://wandb.ai/trinity-actual/DQN_EON_Hyperparameter_Tuning/runs/ddvhln83</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220307_182510-ddvhln83\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run ddvhln83 errored: OSError('symbolic link privilege not held')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run ddvhln83 errored: OSError('symbolic link privilege not held')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 0yfm3nz6 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tGamma: 0.94\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTau: 0.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0025\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Error communicating with wandb process\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m try: wandb.init(settings=wandb.Settings(start_method='fork'))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m or:  wandb.init(settings=wandb.Settings(start_method='thread'))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m For more info see: https://docs.wandb.ai/library/init#init-start-error\n",
      "Run 0yfm3nz6 errored: UsageError(\"Error communicating with wandb process\\ntry: wandb.init(settings=wandb.Settings(start_method='fork'))\\nor:  wandb.init(settings=wandb.Settings(start_method='thread'))\\nFor more info see: https://docs.wandb.ai/library/init#init-start-error\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 0yfm3nz6 errored: UsageError(\"Error communicating with wandb process\\ntry: wandb.init(settings=wandb.Settings(start_method='fork'))\\nor:  wandb.init(settings=wandb.Settings(start_method='thread'))\\nFor more info see: https://docs.wandb.ai/library/init#init-start-error\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem at: C:\\Users\\f4nyt\\AppData\\Local\\Temp/ipykernel_16772/3935149704.py 4 DQN_HP_Tune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Sweep Agent: Waiting for job.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Job received.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: wbsns0tk with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tGamma: 0.94\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTau: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\wandb\\run-20220307_183633-wbsns0tk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/trinity-actual/DQN_EON_Hyperparameter_Tuning/runs/wbsns0tk\" target=\"_blank\">vivid-sweep-3</a></strong> to <a href=\"https://wandb.ai/trinity-actual/DQN_EON_Hyperparameter_Tuning\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/trinity-actual/DQN_EON_Hyperparameter_Tuning/sweeps/faxtqfe9\" target=\"_blank\">https://wandb.ai/trinity-actual/DQN_EON_Hyperparameter_Tuning/sweeps/faxtqfe9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "(256, 256, 3)\n",
      "Request Timed Out, cumulative Reward:\n",
      "0\n",
      "(256, 256, 3)\n",
      "Using cuda device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:221: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 3.93GB > 0.90GB\n",
      "  \"This system does not have apparently enough memory to store the complete \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256, 3)\n",
      "Logging to ./tensorboard_logs/DQN_TUNE_0\n",
      "Request Timed Out, cumulative Reward:\n",
      "-42.5\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-17352.80000000003\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-3764.200000000006\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-6355.999999999996\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1.54e+03  |\n",
      "|    ep_rew_mean      | -6.88e+03 |\n",
      "|    exploration_rate | 0.985     |\n",
      "| time/               |           |\n",
      "|    episodes         | 4         |\n",
      "|    fps              | 152       |\n",
      "|    time_elapsed     | 40        |\n",
      "|    total_timesteps  | 6158      |\n",
      "-----------------------------------\n",
      "End episode, cumulative Reward:\n",
      "-5616.599999999993\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-4017.2000000000035\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-8904.700000000008\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-4998.299999999996\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1.61e+03  |\n",
      "|    ep_rew_mean      | -6.38e+03 |\n",
      "|    exploration_rate | 0.969     |\n",
      "| time/               |           |\n",
      "|    episodes         | 8         |\n",
      "|    fps              | 156       |\n",
      "|    time_elapsed     | 82        |\n",
      "|    total_timesteps  | 12869     |\n",
      "-----------------------------------\n",
      "End episode, cumulative Reward:\n",
      "-8750.59999999999\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-5085.7000000000035\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-7137.299999999995\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-4002.8\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1.58e+03  |\n",
      "|    ep_rew_mean      | -6.34e+03 |\n",
      "|    exploration_rate | 0.955     |\n",
      "| time/               |           |\n",
      "|    episodes         | 12        |\n",
      "|    fps              | 153       |\n",
      "|    time_elapsed     | 123       |\n",
      "|    total_timesteps  | 18987     |\n",
      "-----------------------------------\n",
      "End episode, cumulative Reward:\n",
      "-8555.399999999998\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-5026.599999999995\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-2634.699999999998\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-7842.600000000006\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1.53e+03  |\n",
      "|    ep_rew_mean      | -6.26e+03 |\n",
      "|    exploration_rate | 0.942     |\n",
      "| time/               |           |\n",
      "|    episodes         | 16        |\n",
      "|    fps              | 150       |\n",
      "|    time_elapsed     | 163       |\n",
      "|    total_timesteps  | 24537     |\n",
      "-----------------------------------\n",
      "End episode, cumulative Reward:\n",
      "-5010.499999999994\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-6063.4999999999945\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-6785.799999999999\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-10234.199999999993\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1.58e+03  |\n",
      "|    ep_rew_mean      | -6.41e+03 |\n",
      "|    exploration_rate | 0.925     |\n",
      "| time/               |           |\n",
      "|    episodes         | 20        |\n",
      "|    fps              | 110       |\n",
      "|    time_elapsed     | 287       |\n",
      "|    total_timesteps  | 31697     |\n",
      "-----------------------------------\n",
      "End episode, cumulative Reward:\n",
      "-5902.600000000004\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-12280.499999999996\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-6697.999999999994\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-13774.39999999998\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1.67e+03  |\n",
      "|    ep_rew_mean      | -6.95e+03 |\n",
      "|    exploration_rate | 0.905     |\n",
      "| time/               |           |\n",
      "|    episodes         | 24        |\n",
      "|    fps              | 117       |\n",
      "|    time_elapsed     | 342       |\n",
      "|    total_timesteps  | 40103     |\n",
      "-----------------------------------\n",
      "End episode, cumulative Reward:\n",
      "-12005.700000000012\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-6511.499999999999\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-12667.600000000022\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-8398.900000000005\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1.73e+03  |\n",
      "|    ep_rew_mean      | -7.37e+03 |\n",
      "|    exploration_rate | 0.885     |\n",
      "| time/               |           |\n",
      "|    episodes         | 28        |\n",
      "|    fps              | 123       |\n",
      "|    time_elapsed     | 389       |\n",
      "|    total_timesteps  | 48323     |\n",
      "-----------------------------------\n",
      "End episode, cumulative Reward:\n",
      "-4366.0999999999985\n",
      "(256, 256, 3)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c63005a4f9f94cde9a3f436a6a858489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">vivid-sweep-3</strong>: <a href=\"https://wandb.ai/trinity-actual/DQN_EON_Hyperparameter_Tuning/runs/wbsns0tk\" target=\"_blank\">https://wandb.ai/trinity-actual/DQN_EON_Hyperparameter_Tuning/runs/wbsns0tk</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220307_183633-wbsns0tk\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run wbsns0tk errored: OSError('symbolic link privilege not held')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run wbsns0tk errored: OSError('symbolic link privilege not held')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: 4he8329k with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tGamma: 0.92\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTau: 0.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.01\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Error communicating with wandb process\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m try: wandb.init(settings=wandb.Settings(start_method='fork'))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m or:  wandb.init(settings=wandb.Settings(start_method='thread'))\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m For more info see: https://docs.wandb.ai/library/init#init-start-error\n",
      "Run 4he8329k errored: UsageError(\"Error communicating with wandb process\\ntry: wandb.init(settings=wandb.Settings(start_method='fork'))\\nor:  wandb.init(settings=wandb.Settings(start_method='thread'))\\nFor more info see: https://docs.wandb.ai/library/init#init-start-error\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run 4he8329k errored: UsageError(\"Error communicating with wandb process\\ntry: wandb.init(settings=wandb.Settings(start_method='fork'))\\nor:  wandb.init(settings=wandb.Settings(start_method='thread'))\\nFor more info see: https://docs.wandb.ai/library/init#init-start-error\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem at: C:\\Users\\f4nyt\\AppData\\Local\\Temp/ipykernel_16772/3935149704.py 4 DQN_HP_Tune\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: z7kb96cj with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tGamma: 0.98\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tTau: 1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0075\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\wandb\\run-20220307_184720-z7kb96cj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/trinity-actual/DQN_EON_Hyperparameter_Tuning/runs/z7kb96cj\" target=\"_blank\">twilight-sweep-5</a></strong> to <a href=\"https://wandb.ai/trinity-actual/DQN_EON_Hyperparameter_Tuning\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>Sweep page:  <a href=\"https://wandb.ai/trinity-actual/DQN_EON_Hyperparameter_Tuning/sweeps/faxtqfe9\" target=\"_blank\">https://wandb.ai/trinity-actual/DQN_EON_Hyperparameter_Tuning/sweeps/faxtqfe9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "(256, 256, 3)\n",
      "Request Timed Out, cumulative Reward:\n",
      "0\n",
      "(256, 256, 3)\n",
      "Using cuda device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:221: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 3.93GB > 0.33GB\n",
      "  \"This system does not have apparently enough memory to store the complete \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 256, 3)\n",
      "Logging to ./tensorboard_logs/DQN_TUNE_0\n",
      "Request Timed Out, cumulative Reward:\n",
      "-1557.0\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-2919.0999999999985\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-6893.900000000006\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-5641.800000000002\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1.51e+03  |\n",
      "|    ep_rew_mean      | -4.25e+03 |\n",
      "|    exploration_rate | 0.986     |\n",
      "| time/               |           |\n",
      "|    episodes         | 4         |\n",
      "|    fps              | 139       |\n",
      "|    time_elapsed     | 43        |\n",
      "|    total_timesteps  | 6043      |\n",
      "-----------------------------------\n",
      "End episode, cumulative Reward:\n",
      "-4724.8\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-4732.399999999999\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-5104.0\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-3646.900000000006\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 1.48e+03 |\n",
      "|    ep_rew_mean      | -4.4e+03 |\n",
      "|    exploration_rate | 0.972    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 142      |\n",
      "|    time_elapsed     | 83       |\n",
      "|    total_timesteps  | 11867    |\n",
      "----------------------------------\n",
      "End episode, cumulative Reward:\n",
      "-5554.200000000001\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-6315.7000000000135\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-4299.300000000008\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-2905.399999999999\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1.52e+03  |\n",
      "|    ep_rew_mean      | -4.52e+03 |\n",
      "|    exploration_rate | 0.957     |\n",
      "| time/               |           |\n",
      "|    episodes         | 12        |\n",
      "|    fps              | 149       |\n",
      "|    time_elapsed     | 122       |\n",
      "|    total_timesteps  | 18291     |\n",
      "-----------------------------------\n",
      "End episode, cumulative Reward:\n",
      "-6021.700000000002\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-4135.400000000002\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-4839.200000000004\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-3909.800000000007\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1.54e+03  |\n",
      "|    ep_rew_mean      | -4.57e+03 |\n",
      "|    exploration_rate | 0.942     |\n",
      "| time/               |           |\n",
      "|    episodes         | 16        |\n",
      "|    fps              | 153       |\n",
      "|    time_elapsed     | 160       |\n",
      "|    total_timesteps  | 24628     |\n",
      "-----------------------------------\n",
      "End episode, cumulative Reward:\n",
      "-12399.000000000007\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-5551.000000000003\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-6190.100000000005\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-3162.8999999999987\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1.6e+03   |\n",
      "|    ep_rew_mean      | -5.02e+03 |\n",
      "|    exploration_rate | 0.924     |\n",
      "| time/               |           |\n",
      "|    episodes         | 20        |\n",
      "|    fps              | 156       |\n",
      "|    time_elapsed     | 205       |\n",
      "|    total_timesteps  | 32082     |\n",
      "-----------------------------------\n",
      "End episode, cumulative Reward:\n",
      "-3351.900000000008\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-2049.799999999999\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-4586.200000000001\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-6526.1000000000095\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1.59e+03  |\n",
      "|    ep_rew_mean      | -4.88e+03 |\n",
      "|    exploration_rate | 0.91      |\n",
      "| time/               |           |\n",
      "|    episodes         | 24        |\n",
      "|    fps              | 158       |\n",
      "|    time_elapsed     | 240       |\n",
      "|    total_timesteps  | 38058     |\n",
      "-----------------------------------\n",
      "End episode, cumulative Reward:\n",
      "-3626.8999999999996\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-6504.199999999998\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-5465.700000000005\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-5542.600000000003\n",
      "(256, 256, 3)\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 1.6e+03   |\n",
      "|    ep_rew_mean      | -4.93e+03 |\n",
      "|    exploration_rate | 0.894     |\n",
      "| time/               |           |\n",
      "|    episodes         | 28        |\n",
      "|    fps              | 159       |\n",
      "|    time_elapsed     | 279       |\n",
      "|    total_timesteps  | 44739     |\n",
      "-----------------------------------\n",
      "End episode, cumulative Reward:\n",
      "-4731.100000000008\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-5094.3\n",
      "(256, 256, 3)\n",
      "End episode, cumulative Reward:\n",
      "-3319.2999999999993\n",
      "(256, 256, 3)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:red\">(failed 1).</strong> Press Ctrl-C to abort syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc5bc824a2694182b149b737eb1bb624",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">twilight-sweep-5</strong>: <a href=\"https://wandb.ai/trinity-actual/DQN_EON_Hyperparameter_Tuning/runs/z7kb96cj\" target=\"_blank\">https://wandb.ai/trinity-actual/DQN_EON_Hyperparameter_Tuning/runs/z7kb96cj</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220307_184720-z7kb96cj\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Run z7kb96cj errored: OSError('symbolic link privilege not held')\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Run z7kb96cj errored: OSError('symbolic link privilege not held')\n"
     ]
    }
   ],
   "source": [
    "#testing launching agent which runs train 20 times\n",
    "#uses random hp values\n",
    "wandb.agent(sweep_id, DQN_HP_Tune, count=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DQN_Train(lr, gamma, eps_start, eps_end, eps_fraction, train_freq, target_update_interval, tau, learning_starts, timesteps, Model_Name):\n",
    "    '''Function which conducts model training for DQN on the EON Game Environment\n",
    "    Returns Model object and Model Name\n",
    "\n",
    "    Input Arguements:\n",
    "    1) lr = learning rate\n",
    "    2) gamma = discount factor\n",
    "    3) eps_start = Starting Exploration Rate Value \n",
    "    4) eps_end = Ending Exploration Rate Value\n",
    "    5) exploration_fraction = Fraction of timesteps you want the exploration value to decay (Between eps_start and eps_end)\n",
    "    6) train_freq = frequency of updating the model (either per number of episodes or timesteps)\n",
    "    7) target_update_interval = frequency of updating the target network\n",
    "    8) Tau = The soft update coefficient (\"Polyak update\", between 0 and 1) default 1 for hard update\n",
    "    9) learning_starts = Number of timesteps before model starting doing training updates\n",
    "    10) timesteps = Number of timesteps to train the model through\n",
    "    11) Model_Name = String name of model to be used to save as.\n",
    "\n",
    "    The tensorboard logging is saved in /tensorboard_logs with the respective name for the run given by Model_Name\n",
    "\n",
    "    Model is trained on the TestTopology with 6 request links generated, uses the base callback which saves the best model based on mean_ep_rewards\n",
    "    '''\n",
    "    from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "    #main function with callback\n",
    "    \n",
    "    #Start a wandb instance\n",
    "    wandb.init(\n",
    "        project=\"DQN_EON\",\n",
    "        sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n",
    "    )\n",
    "\n",
    "    # Create and wrap the environment\n",
    "    nodeList, linkList = createTestTopology()\n",
    "    #changed to only have 1 request per episode\n",
    "    #from 6 originally\n",
    "    requestList = generateRequests(nodeList, 6)\n",
    "    user = User()\n",
    "\n",
    "\n",
    "    env = game_gym(nodeList, linkList, requestList, user)\n",
    "    check_env(env)\n",
    "\n",
    "    eveon = Monitor(env, log_tensorboard)\n",
    "\n",
    "    # Create the callback: check every 10000 steps\n",
    "    callback = SaveOnBestTrainingRewardCallback(check_freq=10000, log_dir=log_tensorboard)\n",
    "    \n",
    "    model_dir = os.path.join(\"Models/\", str(Model_Name))\n",
    "\n",
    "    #hyperparameters testing\n",
    "    #previously used\n",
    "    #lr = 0.01\n",
    "    #gamma = 0.7\n",
    "    #eps_start = 1\n",
    "    #eps_end = 0.05\n",
    "    #train_freq = (1000, \"step\")\n",
    "    #target_update_interval = 50000\n",
    "    #soft update tau value\n",
    "    #tau = 0.4\n",
    "    policy_kwargs = {\n",
    "        'net_arch':[64,64] #MLP hidden layer size\n",
    "    }\n",
    "\n",
    "\n",
    "    # Train the agent\n",
    "    #model = DQN('MlpPolicy', eveon, verbose=2, buffer_size=100)\n",
    "    model = DQN('MlpPolicy', eveon, tau=tau, learning_starts=learning_starts, buffer_size=10000, verbose=1, device=\"auto\", learning_rate=lr, gamma=gamma, exploration_fraction=eps_fraction, exploration_initial_eps=eps_start, exploration_final_eps=eps_end, target_update_interval=target_update_interval, train_freq=train_freq, tensorboard_log=\"./tensorboard_logs/\")\n",
    "        \n",
    "\n",
    "    #Debug Print\n",
    "    print(\"Carry out training on: \" + str(Model_Name))\n",
    "    #model.learn(total_timesteps=int(timesteps), tb_log_name=str(Model_Name), callback=callback, reset_num_timesteps=False)\n",
    "    #training with wandb callback instead\n",
    "    #model.learn(total_timesteps=int(timesteps), tb_log_name=str(Model_Name), callback=WandbCallback(model_save_path=model_dir, model_save_freq=50000), reset_num_timesteps=False)\n",
    "    \n",
    "\n",
    "    Save_Model_Details(model, Model_Name, timesteps)\n",
    "    # # Close the environment\n",
    "    # eveon.close()\n",
    "\n",
    "    #Finishing wandb run\n",
    "    wandb.finish()\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training and saving model\n",
    "model = DQN_Train(lr=0.005, gamma=0.98, eps_start=1, eps_end=0.05, eps_fraction=0.8, train_freq=(100, \"step\"), target_update_interval=50000, tau=0.5, learning_starts=50000, timesteps=500000, Model_Name=\"DQN_Date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Save_Model_Details(Model, Model_Name, timesteps):\n",
    "    '''Creating Folder in \"Models\" Folder and Saving Trained Model\n",
    "    \n",
    "    Input Arguements:\n",
    "    1) Model = Trained Model object to be saved.\n",
    "    2) Model_Name = String name of the model to be saved\n",
    "    3) timesteps = Number of timesteps used in training.\n",
    "\n",
    "\n",
    "    '''\n",
    "    #Creating folder to store the created model in \"Models\"\n",
    "    model_dir = os.path.join(\"Models/\", str(Model_Name) + \"_Full_Train\")\n",
    "    # Create log dir for tensorboard log\n",
    "    os.makedirs(model_dir, exist_ok=False)\n",
    "    \n",
    "    #Saving Trained Model\n",
    "    Model.save(model_dir)\n",
    "    print(str(Model_Name) + \" Successfully Saved.\")\n",
    "    print(\"Remember to shift the Monitor.csv and best_model.zip files into the created model folder manually!!\")\n",
    "\n",
    "    #Plotting Rewards for show\n",
    "    plot_results([log_tensorboard], timesteps, results_plotter.X_TIMESTEPS, \"DQN EON\")\n",
    "    plt.show()\n",
    "    plt.savefig(str(model_dir) + \" Episode rewards over timestamps.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Debug\n",
    "#torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Tensorboard Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-92f3dbb2c1193487\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-92f3dbb2c1193487\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#To access tensorboard logging, select the --logdir where the logs are at (in this case tensorboard_logs)\n",
    "%tensorboard --logdir tensorboard_logs\n",
    "\n",
    "#after this you can go to the following link http://localhost:6006/\n",
    "#Alternatively you can run the same command in a command prompt while in the source directory (without the %)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_Model(Model_Dir, Model_Name, device):\n",
    "    '''Loading Saved model Zip file into Object\n",
    "    \n",
    "    Input Arguements:\n",
    "    1) Model_Dir = Path to saved model file\n",
    "    2) Model_Name = Str of Model file name\n",
    "    3) Device = Device to run on, either \"cpu\" or \"cuda\"\n",
    "\n",
    "    Returns loaded model\n",
    "    '''\n",
    "\n",
    "    model_dir = os.path.join(str(Model_Dir), str(Model_Name))\n",
    "\n",
    "    load_model = DQN.load(str(model_dir),  device=device)\n",
    "    print(\"Successfully loaded \" + str(Model_Name) + \"!\")\n",
    "\n",
    "    return load_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:221: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 3.93GB > 1.18GB\n",
      "  \"This system does not have apparently enough memory to store the complete \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded DQNEveon_1mil_030322.zip!\n"
     ]
    }
   ],
   "source": [
    "loaded_model = Load_Model(\"Models/030322_Model_1mil_sofar\", \"DQNEveon_1mil_030322.zip\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:221: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 3.93GB > 3.78GB\n",
      "  \"This system does not have apparently enough memory to store the complete \"\n"
     ]
    }
   ],
   "source": [
    "#test2 is the successful one (but still just spams enter key)\n",
    "#LOADING MODEL FROM ZIP\n",
    "#loaded_model = DQN.load(\"DQNEveon_test2\")\n",
    "#loaded_model = DQN.load(\"DQNEveon_testing_surface_250222\")\n",
    "\n",
    "#2mil timestep model load\n",
    "#NOTE that old model on more complex game not compatible with new observation window\n",
    "#loaded_model = DQN.load(\"DQNEveon_2mil_270222\", device=\"cpu\")\n",
    "#best model load during 2mil training\n",
    "#loaded_model = DQN.load(\"./Models/270222_Model_2mil/best_model.zip\", device=\"cpu\")\n",
    "\n",
    "#loading 1mil semi simplified model with new rewards\n",
    "#loaded_model = DQN.load(\"DQNEveon_1mil_030322.zip\")\n",
    "#loaded_model = DQN.load(\"DQNEveon_2mil_030322.zip\")\n",
    "#loaded_model = DQN.load(\"DQNEveon_2+3mil_030322.zip\")\n",
    "#loaded_model = DQN.load(\"./Models/030322_Model_1mil_sofar/best_model.zip\")\n",
    "\n",
    "\n",
    "#loaded_model = DQN.load(\"DQNEveon_040322.zip\")\n",
    "#loaded_model = DQN.load(\"./tensorboard_logs/best_model.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Agent Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodeList, linkList = createTestTopology()\n",
    "\n",
    "#changed to only have 1 request per episode\n",
    "#from 6 originally\n",
    "requestList = generateRequests(nodeList, 6)\n",
    "\n",
    "user = User()\n",
    "eveon = game_gym(nodeList, linkList, requestList, user)\n",
    "\n",
    "check_env(eveon, warn=True)\n",
    "\n",
    "# THIS IS THE TESTING LOOP OF THE AGENT PLAYING THE GAME|\n",
    "obs = eveon.reset()\n",
    "while True :\n",
    "\n",
    "    #enable this if using older models with the wider observation space\n",
    "    #obs = cv2.resize(obs, dsize=(600, 1000))\n",
    "    \n",
    "    \n",
    "    #trying to test if deterministic true or false changes model actions\n",
    "    action, states_ = loaded_model.predict(obs, deterministic=True)\n",
    "    # action = 6\n",
    "    obs, rewards, dones, info = eveon.step(action)\n",
    "    \n",
    "    print(\"Action:\")\n",
    "    print(action)\n",
    "    #time.sleep(1)\n",
    "    #clear_output(wait=True)\n",
    "\n",
    "\n",
    "    if dones == True:\n",
    "        #debug print\n",
    "        print(\"########################Reward Obtained:\")\n",
    "        print(eveon.reward)\n",
    "        # with open('info.json', 'w') as outfile:\n",
    "        #     json.dump(info, outfile)\n",
    "\n",
    "        eveon.reset()\n",
    "\n",
    "    eveon.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Action Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent taking random actions\n",
    "nodeList, linkList = createTestTopology()\n",
    "\n",
    "#changed to only have 1 request per episode\n",
    "#from 6 originally\n",
    "requestList = generateRequests(nodeList, 6)\n",
    "\n",
    "user = User()\n",
    "eveon = game_gym(nodeList, linkList, requestList, user)\n",
    "\n",
    "check_env(eveon, warn=True)\n",
    "#resets the environment\n",
    "obs = eveon.reset()\n",
    "\n",
    "for step in range(2000):\n",
    "\teveon.render()\n",
    "\n",
    "\trand_action = eveon.action_space.sample()\n",
    "\teveon.step(rand_action)\n",
    "\tprint(rand_action)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test main function\n",
    "nodeList, linkList = createTestTopology()\n",
    "requestList = generateRequests(nodeList, 6)\n",
    "\n",
    "user = User()\n",
    "eveon = game_gym(nodeList, linkList, requestList, user)\n",
    "\n",
    "check_env(eveon, warn=True)\n",
    "\n",
    "#defining the agent\n",
    "model = DQN('MlpPolicy', eveon, verbose=1, buffer_size=100, device='cuda')\n",
    "\n",
    "screen = eveon.render()\n",
    "#storing episode_durations during training to plot them\n",
    "#creating empty list to store\n",
    "#adding plot to check the training process?\n",
    "episode_durations = []\n",
    "timestep = 0\n",
    "\n",
    "\n",
    "#trying the double nested for loop\n",
    "\n",
    "for episode in range(100):\n",
    "\n",
    "#resets the environment\n",
    "obs = eveon.reset()\n",
    "\n",
    "#nested for loop iterate over time step\n",
    "for timesetp in count():\n",
    "\n",
    "    action, states_ = model.predict(obs, deterministic=True)\n",
    "    # action = 6\n",
    "    \n",
    "    obs, rewards, dones, info = eveon.step(action)\n",
    "    \n",
    "\n",
    "    print(action)\n",
    "    \n",
    "    #this only runs when the agent actually competed the level\n",
    "    if dones == True:\n",
    "        #debug print\n",
    "        print(\"########################Reward Obtained:\")\n",
    "        print(eveon.reward)\n",
    "\n",
    "        episode_durations.append(timestep)\n",
    "        plot(episode_durations, 100)\n",
    "        \n",
    "        # with open('info.json', 'w') as outfile:\n",
    "        #     json.dump(info, outfile)\n",
    "        timestep += 1\n",
    "        eveon.reset()\n",
    "\n",
    "    eveon.render()\n",
    "\n",
    "    screen = eveon.render('rgb_array')\n",
    "    plt.figure()\n",
    "    plt.imshow(screen)\n",
    "    plt.title('test screen')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40b93cbb1699659d2cb0e714698cbe72f80d4ea3079150298ec50c78e4aa7ede"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('RL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
