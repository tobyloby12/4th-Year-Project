{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.3 (SDL 2.0.16, Python 3.7.11)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtrinity-actual\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    }
   ],
   "source": [
    "from optical_network_game.node import *\n",
    "from optical_network_game.link import *\n",
    "from optical_network_game.requests import *\n",
    "from optical_network_game.user import *\n",
    "import gym\n",
    "import pygame, sys\n",
    "from pygame.locals import *\n",
    "from gym import spaces\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#from stable_baselines.common.vec_env import DummyVecEnv\n",
    "#from stable_baselines.deepq.policies import MlpPolicy\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3 import A2C\n",
    "import json\n",
    "import cv2\n",
    "\n",
    "#additional code added by me just for testing\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "#importing IPython's display module to plot images\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython: from IPython import display\n",
    "from itertools import count\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "#importing wandb (weights and biases) for logging\n",
    "import wandb\n",
    "wandb.login()\n",
    "\n",
    "import tensorboard\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "#Importing game_gym class for use\n",
    "import importlib\n",
    "import optical_network_game.game_gym\n",
    "importlib.reload(optical_network_game.game_gym)\n",
    "from optical_network_game.game_gym import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq:\n",
    "    :param log_dir: Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: Verbosity level.\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose: int = 1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, 'best_model')\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "          # Retrieve training reward\n",
    "          x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "          if len(x) > 0:\n",
    "              # Mean training reward over the last 100 episodes\n",
    "              mean_reward = np.mean(y[-100:])\n",
    "              if self.verbose > 0:\n",
    "                print(f\"Num timesteps: {self.num_timesteps}\")\n",
    "                print(f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\")\n",
    "\n",
    "              # New best model, you could save the agent here\n",
    "              if mean_reward > self.best_mean_reward:\n",
    "                  self.best_mean_reward = mean_reward\n",
    "                  # Example for saving best model\n",
    "                  if self.verbose > 0:\n",
    "                    print(f\"Saving new best model to {self.save_path}\")\n",
    "                  self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating fixed test topology\n",
    "def createTestTopology():\n",
    "    # testNodes\n",
    "    nodeA = Node(0, 'A', 300, 200)\n",
    "    nodeB = Node(1, 'B', 300, 400)\n",
    "    nodeC = Node(2, 'C', 650, 200)\n",
    "    nodeD = Node(3, 'D', 650, 400)\n",
    "    # testLinks\n",
    "    link1 = Link(0, nodeA, nodeB)\n",
    "    link2 = Link(1, nodeB, nodeC)\n",
    "    link3 = Link(2, nodeB, nodeD)\n",
    "    link4 = Link(3, nodeA, nodeC)\n",
    "    link5 = Link(4, nodeC, nodeD)\n",
    "\n",
    "    nodeList = [nodeA, nodeB, nodeC, nodeD]\n",
    "    linkList = [link1, link2, link3, link4, link5]\n",
    "\n",
    "    # save the links associated to each node in a list\n",
    "    for node in nodeList:\n",
    "        node.setLinks(linkList)\n",
    "    return nodeList, linkList\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create log dir for saving model\n",
    "log_dir = os.path.join(os.getcwd(), \"tmp/\")\n",
    "#os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Create log dir for tensorboard log\n",
    "log_tensorboard = os.path.join(os.getcwd(), \"tensorboard_logs/\")\n",
    "#os.makedirs(log_tensorboard, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights & Biases Hyperparameter Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'method': 'random',\n",
      " 'parameters': {'Gamma': {'values': [0.9, 0.92, 0.94, 0.96, 0.98, 0.99]},\n",
      "                'Tau': {'values': [0.4, 0.6, 0.8, 1]},\n",
      "                'learning_rate': {'values': [0.001,\n",
      "                                             0.0025,\n",
      "                                             0.005,\n",
      "                                             0.0075,\n",
      "                                             0.01]}}}\n"
     ]
    }
   ],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random'\n",
    "}\n",
    "\n",
    "hyper_params = {\n",
    "    'learning_rate': {\n",
    "        'values': [0.001, 0.0025, 0.005, 0.0075, 0.01]\n",
    "    },\n",
    "    'Gamma': {\n",
    "        'values': [0.9, 0.92, 0.94, 0.96, 0.98, 0.99]\n",
    "    },\n",
    "    'Tau': {\n",
    "        'values': [0.4, 0.6, 0.8, 1]\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_config['parameters'] = hyper_params\n",
    "\n",
    "import pprint\n",
    "pprint.pprint(sweep_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: faxtqfe9\n",
      "Sweep URL: https://wandb.ai/trinity-actual/DQN_EON_Hyperparameter_Tuning/sweeps/faxtqfe9\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"DQN_EON_Hyperparameter_Tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DQN_HP_Tune(config=None):\n",
    "    from wandb.integration.sb3 import WandbCallback\n",
    "    \n",
    "    run = wandb.init(config=config, settings=wandb.Settings(start_method=\"fork\"), sync_tensorboard=True)\n",
    "\n",
    "    with run:\n",
    "        \n",
    "\n",
    "        config = wandb.config\n",
    "\n",
    "\n",
    "        Model_Name = \"DQN_Tune_070322\"\n",
    "        model_dir = \"./Models\"\n",
    "        log_tensorboard = \"./tensorboard_logs/\"\n",
    "        \n",
    "        # Create and wrap the environment\n",
    "        nodeList, linkList = createTestTopology()\n",
    "        #changed to only have 1 request per episode\n",
    "        #from 6 originally\n",
    "        requestList = generateRequests(nodeList, 6)\n",
    "        user = User()\n",
    "\n",
    "\n",
    "        env = game_gym(nodeList, linkList, requestList, user)\n",
    "        check_env(env)\n",
    "\n",
    "        eveon = Monitor(env, log_tensorboard)\n",
    "\n",
    "        #hyperparameters\n",
    "        timesteps = 500000\n",
    "        learning_starts = 50000\n",
    "        eps_fraction = 0.8\n",
    "        eps_start = 1\n",
    "        eps_end = 0.05\n",
    "        train_freq = (1000, \"step\")\n",
    "        target_update_interval = 50000\n",
    "\n",
    "\n",
    "\n",
    "        model = DQN('MlpPolicy', eveon, tau=config.Tau, learning_starts=learning_starts, buffer_size=10000, verbose=1, device=\"auto\", learning_rate=config.learning_rate, gamma=config.Gamma, exploration_fraction=eps_fraction, exploration_initial_eps=eps_start, exploration_final_eps=eps_end, target_update_interval=target_update_interval, train_freq=train_freq, tensorboard_log=f\"runs/{run.id}\")\n",
    "    \n",
    "        #training with hyperparameter tuning on lr, gamma and tau.\n",
    "        model.learn(total_timesteps=int(timesteps), callback=WandbCallback(verbose=2, model_save_path=f\"models/{run.id}\", model_save_freq=50000), reset_num_timesteps=False)\n",
    "\n",
    "\n",
    "        #Save_Model_Details(model, Model_Name, timesteps)\n",
    "        # # Close the environment\n",
    "        # eveon.close()\n",
    "\n",
    "        #Finishing wandb run\n",
    "        run.finish()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing launching agent which runs train 20 times\n",
    "#uses random hp values\n",
    "wandb.agent(sweep_id, DQN_HP_Tune, count=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DQN_Train(lr, gamma, eps_start, eps_end, eps_fraction, train_freq, target_update_interval, tau, learning_starts, timesteps, Model_Name):\n",
    "    '''Function which conducts model training for DQN on the EON Game Environment\n",
    "    Returns Model object and Model Name\n",
    "\n",
    "    Input Arguements:\n",
    "    1) lr = learning rate\n",
    "    2) gamma = discount factor\n",
    "    3) eps_start = Starting Exploration Rate Value \n",
    "    4) eps_end = Ending Exploration Rate Value\n",
    "    5) exploration_fraction = Fraction of timesteps you want the exploration value to decay (Between eps_start and eps_end)\n",
    "    6) train_freq = frequency of updating the model (either per number of episodes or timesteps)\n",
    "    7) target_update_interval = frequency of updating the target network\n",
    "    8) Tau = The soft update coefficient (\"Polyak update\", between 0 and 1) default 1 for hard update\n",
    "    9) learning_starts = Number of timesteps before model starting doing training updates\n",
    "    10) timesteps = Number of timesteps to train the model through\n",
    "    11) Model_Name = String name of model to be used to save as.\n",
    "\n",
    "    The tensorboard logging is saved in /tensorboard_logs with the respective name for the run given by Model_Name\n",
    "\n",
    "    Model is trained on the TestTopology with 6 request links generated, uses the base callback which saves the best model based on mean_ep_rewards\n",
    "    '''\n",
    "    #from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "    #main function with callback\n",
    "    \n",
    "    #Start a wandb instance\n",
    "    #wandb.init(\n",
    "    #    project=\"DQN_EON\",\n",
    "    #    sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n",
    "    #)\n",
    "\n",
    "    # Create and wrap the environment\n",
    "    nodeList, linkList = createTestTopology()\n",
    "    #changed to only have 1 request per episode\n",
    "    #from 6 originally\n",
    "    requestList = generateRequests(nodeList, 6)\n",
    "    user = User()\n",
    "\n",
    "\n",
    "    env = game_gym(nodeList, linkList, requestList, user)\n",
    "    check_env(env)\n",
    "\n",
    "    eveon = Monitor(env, log_tensorboard)\n",
    "\n",
    "    # Create the callback: check every 10000 steps\n",
    "    callback = SaveOnBestTrainingRewardCallback(check_freq=10000, log_dir=log_tensorboard)\n",
    "    \n",
    "    model_dir = os.path.join(\"Models/\", str(Model_Name))\n",
    "\n",
    "    #hyperparameters testing\n",
    "    #previously used\n",
    "    #lr = 0.01\n",
    "    #gamma = 0.7\n",
    "    #eps_start = 1\n",
    "    #eps_end = 0.05\n",
    "    #train_freq = (1000, \"step\")\n",
    "    #target_update_interval = 50000\n",
    "    #soft update tau value\n",
    "    #tau = 0.4\n",
    "    policy_kwargs = {\n",
    "        'net_arch':[64,64] #MLP hidden layer size\n",
    "    }\n",
    "\n",
    "\n",
    "    # Train the agent\n",
    "    #model = DQN('MlpPolicy', eveon, verbose=2, buffer_size=100)\n",
    "    model = DQN('MlpPolicy', eveon, gradient_steps=-1, tau=tau, learning_starts=learning_starts, buffer_size=10000, verbose=1, device=\"auto\", learning_rate=lr, gamma=gamma, exploration_fraction=eps_fraction, exploration_initial_eps=eps_start, exploration_final_eps=eps_end, target_update_interval=target_update_interval, train_freq=train_freq, tensorboard_log=\"./tensorboard_logs/\")\n",
    "        \n",
    "\n",
    "    #Debug Print\n",
    "    print(\"Carry out training on: \" + str(Model_Name))\n",
    "    model.learn(total_timesteps=int(timesteps), tb_log_name=str(Model_Name), callback=callback, reset_num_timesteps=False)\n",
    "    #training with wandb callback instead\n",
    "    #model.learn(total_timesteps=int(timesteps), tb_log_name=str(Model_Name), callback=WandbCallback(model_save_path=model_dir, model_save_freq=50000), reset_num_timesteps=False)\n",
    "    \n",
    "\n",
    "    Save_Model_Details(model, Model_Name, timesteps)\n",
    "    # # Close the environment\n",
    "    # eveon.close()\n",
    "\n",
    "    #Finishing wandb run\n",
    "    #wandb.finish()\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3syo9tw7) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb4d606356994c09aeed0e89691a0387",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">dashing-sponge-2</strong>: <a href=\"https://wandb.ai/trinity-actual/DQN_EON/runs/3syo9tw7\" target=\"_blank\">https://wandb.ai/trinity-actual/DQN_EON/runs/3syo9tw7</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20220310_133248-3syo9tw7\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3syo9tw7). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\wandb\\run-20220310_164643-3kwsexcn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/trinity-actual/DQN_EON/runs/3kwsexcn\" target=\"_blank\">wobbly-firefly-3</a></strong> to <a href=\"https://wandb.ai/trinity-actual/DQN_EON\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "(256, 256, 3)\n",
      "Request Timed Out, cumulative Reward:\n",
      "0\n",
      "(256, 256, 3)\n",
      "Using cuda device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:221: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 3.93GB > 2.88GB\n",
      "  \"This system does not have apparently enough memory to store the complete \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carry out training on: DQN_100322\n",
      "(256, 256, 3)\n",
      "Logging to ./tensorboard_logs/DQN_100322_0\n",
      "Too many 3 link connections made.\n",
      "-427.09999999999934\n",
      "(256, 256, 3)\n",
      "Too many 3 link connections made.\n",
      "-394.0999999999995\n",
      "(256, 256, 3)\n",
      "Too many 3 link connections made.\n",
      "-436.9999999999993\n",
      "(256, 256, 3)\n",
      "Too many 3 link connections made.\n",
      "-405.99999999999926\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 538      |\n",
      "|    ep_rew_mean      | -767     |\n",
      "|    exploration_rate | 0.993    |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 164      |\n",
      "|    time_elapsed     | 13       |\n",
      "|    total_timesteps  | 2154     |\n",
      "----------------------------------\n",
      "Too many 3 link connections made.\n",
      "-224.70000000000007\n",
      "(256, 256, 3)\n",
      "Too many 3 link connections made.\n",
      "-225.4000000000001\n",
      "(256, 256, 3)\n",
      "Too many 3 link connections made.\n",
      "-413.99999999999926\n",
      "(256, 256, 3)\n",
      "Too many 3 link connections made.\n",
      "-391.9999999999995\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 447      |\n",
      "|    ep_rew_mean      | -716     |\n",
      "|    exploration_rate | 0.989    |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 164      |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 3573     |\n",
      "----------------------------------\n",
      "Request Timed Out, cumulative Reward:\n",
      "-349.59999999999934\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-598.5999999999985\n",
      "(256, 256, 3)\n",
      "Request Timed Out, cumulative Reward:\n",
      "-86.70000000000003\n",
      "(256, 256, 3)\n",
      "Too many 3 link connections made.\n",
      "-207.50000000000006\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 431      |\n",
      "|    ep_rew_mean      | -644     |\n",
      "|    exploration_rate | 0.984    |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 47       |\n",
      "|    time_elapsed     | 109      |\n",
      "|    total_timesteps  | 5170     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.002    |\n",
      "|    loss             | 1.13     |\n",
      "|    n_updates        | 2000     |\n",
      "----------------------------------\n",
      "Request Timed Out, cumulative Reward:\n",
      "-309.7999999999998\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-339.39999999999975\n",
      "(256, 256, 3)\n",
      "Request Timed Out, cumulative Reward:\n",
      "-324.19999999999925\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-193.49999999999966\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 478      |\n",
      "|    ep_rew_mean      | -599     |\n",
      "|    exploration_rate | 0.976    |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 37       |\n",
      "|    time_elapsed     | 205      |\n",
      "|    total_timesteps  | 7640     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.002    |\n",
      "|    loss             | 0.987    |\n",
      "|    n_updates        | 4000     |\n",
      "----------------------------------\n",
      "Too many 3 link connections made.\n",
      "-184.6\n",
      "(256, 256, 3)\n",
      "Too many 3 link connections made.\n",
      "-165.3\n",
      "(256, 256, 3)\n",
      "Request Timed Out, cumulative Reward:\n",
      "-151.3\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-175.40000000000026\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 419      |\n",
      "|    ep_rew_mean      | -567     |\n",
      "|    exploration_rate | 0.973    |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 33       |\n",
      "|    time_elapsed     | 252      |\n",
      "|    total_timesteps  | 8382     |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.002    |\n",
      "|    loss             | 0.985    |\n",
      "|    n_updates        | 5000     |\n",
      "----------------------------------\n",
      "Request Timed Out, cumulative Reward:\n",
      "-291.799999999999\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-366.2999999999994\n",
      "(256, 256, 3)\n",
      "Too many 3 link connections made.\n",
      "-187.10000000000005\n",
      "(256, 256, 3)\n",
      "Num timesteps: 10000\n",
      "Best mean reward: -inf - Last mean reward per episode: -559.54\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\tensorboard_logs/best_model\n",
      "Request Timed Out, cumulative Reward:\n",
      "-130.1\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 417      |\n",
      "|    ep_rew_mean      | -544     |\n",
      "|    exploration_rate | 0.968    |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 26       |\n",
      "|    time_elapsed     | 371      |\n",
      "|    total_timesteps  | 10001    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.002    |\n",
      "|    loss             | 1.09     |\n",
      "|    n_updates        | 7000     |\n",
      "----------------------------------\n",
      "Too many 3 link connections made.\n",
      "-136.2\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-495.7999999999988\n",
      "(256, 256, 3)\n",
      "Request Timed Out, cumulative Reward:\n",
      "-175.59999999999988\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-276.3\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 404      |\n",
      "|    ep_rew_mean      | -541     |\n",
      "|    exploration_rate | 0.964    |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 26       |\n",
      "|    time_elapsed     | 420      |\n",
      "|    total_timesteps  | 11318    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.002    |\n",
      "|    loss             | 1.17     |\n",
      "|    n_updates        | 8000     |\n",
      "----------------------------------\n",
      "Request Timed Out, cumulative Reward:\n",
      "-435.9999999999988\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-391.5999999999991\n",
      "(256, 256, 3)\n",
      "Request Timed Out, cumulative Reward:\n",
      "-215.59999999999968\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-408.89999999999975\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 429      |\n",
      "|    ep_rew_mean      | -541     |\n",
      "|    exploration_rate | 0.957    |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 26       |\n",
      "|    time_elapsed     | 513      |\n",
      "|    total_timesteps  | 13733    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.002    |\n",
      "|    loss             | 1.06     |\n",
      "|    n_updates        | 10000    |\n",
      "----------------------------------\n",
      "Request Timed Out, cumulative Reward:\n",
      "-219.5999999999998\n",
      "(256, 256, 3)\n",
      "Too many 3 link connections made.\n",
      "-367.1999999999998\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-333.29999999999967\n",
      "(256, 256, 3)\n",
      "Request Timed Out, cumulative Reward:\n",
      "-64.20000000000006\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 417      |\n",
      "|    ep_rew_mean      | -529     |\n",
      "|    exploration_rate | 0.952    |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 25       |\n",
      "|    time_elapsed     | 598      |\n",
      "|    total_timesteps  | 15001    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.002    |\n",
      "|    loss             | 1.07     |\n",
      "|    n_updates        | 12000    |\n",
      "----------------------------------\n",
      "Too many invalid spectrum allocations.\n",
      "-466.8999999999983\n",
      "(256, 256, 3)\n",
      "Request Timed Out, cumulative Reward:\n",
      "-113.00000000000013\n",
      "(256, 256, 3)\n",
      "Too many 3 link connections made.\n",
      "-541.199999999999\n",
      "(256, 256, 3)\n",
      "Request Timed Out, cumulative Reward:\n",
      "-18.099999999999994\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 425      |\n",
      "|    ep_rew_mean      | -524     |\n",
      "|    exploration_rate | 0.946    |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 24       |\n",
      "|    time_elapsed     | 688      |\n",
      "|    total_timesteps  | 17001    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.002    |\n",
      "|    loss             | 0.983    |\n",
      "|    n_updates        | 14000    |\n",
      "----------------------------------\n",
      "Too many 3 link connections made.\n",
      "-207.20000000000005\n",
      "(256, 256, 3)\n",
      "Too many 3 link connections made.\n",
      "-354.999999999999\n",
      "(256, 256, 3)\n",
      "Request Timed Out, cumulative Reward:\n",
      "-256.99999999999955\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-443.7999999999987\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 428      |\n",
      "|    ep_rew_mean      | -529     |\n",
      "|    exploration_rate | 0.94     |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 25       |\n",
      "|    time_elapsed     | 745      |\n",
      "|    total_timesteps  | 18820    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.002    |\n",
      "|    loss             | 0.897    |\n",
      "|    n_updates        | 15000    |\n",
      "----------------------------------\n",
      "Request Timed Out, cumulative Reward:\n",
      "-155.10000000000002\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-588.9999999999998\n",
      "(256, 256, 3)\n",
      "Num timesteps: 20000\n",
      "Best mean reward: -559.54 - Last mean reward per episode: -530.08\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\tensorboard_logs/best_model\n",
      "Request Timed Out, cumulative Reward:\n",
      "-121.4000000000001\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-226.2\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 419      |\n",
      "|    ep_rew_mean      | -523     |\n",
      "|    exploration_rate | 0.936    |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 23       |\n",
      "|    time_elapsed     | 843      |\n",
      "|    total_timesteps  | 20121    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.002    |\n",
      "|    loss             | 0.935    |\n",
      "|    n_updates        | 17000    |\n",
      "----------------------------------\n",
      "Too many 3 link connections made.\n",
      "-437.6999999999992\n",
      "(256, 256, 3)\n",
      "Too many 3 link connections made.\n",
      "-210.80000000000007\n",
      "(256, 256, 3)\n",
      "Request Timed Out, cumulative Reward:\n",
      "-156.0\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-426.3999999999992\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 416      |\n",
      "|    ep_rew_mean      | -527     |\n",
      "|    exploration_rate | 0.931    |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 24       |\n",
      "|    time_elapsed     | 891      |\n",
      "|    total_timesteps  | 21646    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.002    |\n",
      "|    loss             | 0.872    |\n",
      "|    n_updates        | 18000    |\n",
      "----------------------------------\n",
      "Request Timed Out, cumulative Reward:\n",
      "-218.19999999999962\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-572.5999999999985\n",
      "(256, 256, 3)\n",
      "Request Timed Out, cumulative Reward:\n",
      "-169.69999999999987\n",
      "(256, 256, 3)\n",
      "Request Timed Out, cumulative Reward:\n",
      "-537.3999999999983\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 429      |\n",
      "|    ep_rew_mean      | -523     |\n",
      "|    exploration_rate | 0.924    |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 23       |\n",
      "|    time_elapsed     | 1022     |\n",
      "|    total_timesteps  | 24001    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.002    |\n",
      "|    loss             | 0.897    |\n",
      "|    n_updates        | 21000    |\n",
      "----------------------------------\n",
      "Too many 3 link connections made.\n",
      "-304.39999999999986\n",
      "(256, 256, 3)\n",
      "Request Timed Out, cumulative Reward:\n",
      "-372.3999999999989\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-331.69999999999993\n",
      "(256, 256, 3)\n",
      "Too many 3 link connections made.\n",
      "-417.5999999999991\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 431      |\n",
      "|    ep_rew_mean      | -530     |\n",
      "|    exploration_rate | 0.918    |\n",
      "| time/               |          |\n",
      "|    episodes         | 60       |\n",
      "|    fps              | 24       |\n",
      "|    time_elapsed     | 1074     |\n",
      "|    total_timesteps  | 25868    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.002    |\n",
      "|    loss             | 0.888    |\n",
      "|    n_updates        | 22000    |\n",
      "----------------------------------\n",
      "Request Timed Out, cumulative Reward:\n",
      "-85.20000000000006\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-487.99999999999864\n",
      "(256, 256, 3)\n",
      "Request Timed Out, cumulative Reward:\n",
      "-214.39999999999955\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-410.3999999999994\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 431      |\n",
      "|    ep_rew_mean      | -527     |\n",
      "|    exploration_rate | 0.913    |\n",
      "| time/               |          |\n",
      "|    episodes         | 64       |\n",
      "|    fps              | 23       |\n",
      "|    time_elapsed     | 1163     |\n",
      "|    total_timesteps  | 27590    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.002    |\n",
      "|    loss             | 0.744    |\n",
      "|    n_updates        | 24000    |\n",
      "----------------------------------\n",
      "Request Timed Out, cumulative Reward:\n",
      "-276.59999999999945\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-161.50000000000014\n",
      "(256, 256, 3)\n",
      "Request Timed Out, cumulative Reward:\n",
      "-448.1999999999986\n",
      "(256, 256, 3)\n",
      "Too many 3 link connections made.\n",
      "-293.09999999999985\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 433      |\n",
      "|    ep_rew_mean      | -524     |\n",
      "|    exploration_rate | 0.907    |\n",
      "| time/               |          |\n",
      "|    episodes         | 68       |\n",
      "|    fps              | 23       |\n",
      "|    time_elapsed     | 1253     |\n",
      "|    total_timesteps  | 29472    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.002    |\n",
      "|    loss             | 0.714    |\n",
      "|    n_updates        | 26000    |\n",
      "----------------------------------\n",
      "Num timesteps: 30000\n",
      "Best mean reward: -530.08 - Last mean reward per episode: -523.56\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\tensorboard_logs/best_model\n",
      "Request Timed Out, cumulative Reward:\n",
      "-366.499999999999\n",
      "(256, 256, 3)\n",
      "Too many 3 link connections made.\n",
      "-287.89999999999986\n",
      "(256, 256, 3)\n",
      "Too many 3 link connections made.\n",
      "-342.6999999999989\n",
      "(256, 256, 3)\n",
      "Request Timed Out, cumulative Reward:\n",
      "-31.89999999999999\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 431      |\n",
      "|    ep_rew_mean      | -520     |\n",
      "|    exploration_rate | 0.902    |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 22       |\n",
      "|    time_elapsed     | 1350     |\n",
      "|    total_timesteps  | 31001    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.002    |\n",
      "|    loss             | 0.873    |\n",
      "|    n_updates        | 28000    |\n",
      "----------------------------------\n",
      "Too many 3 link connections made.\n",
      "-304.5999999999992\n",
      "(256, 256, 3)\n",
      "Too many 3 link connections made.\n",
      "-351.49999999999926\n",
      "(256, 256, 3)\n",
      "Request Timed Out, cumulative Reward:\n",
      "-70.80000000000003\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-457.79999999999865\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 431      |\n",
      "|    ep_rew_mean      | -522     |\n",
      "|    exploration_rate | 0.896    |\n",
      "| time/               |          |\n",
      "|    episodes         | 76       |\n",
      "|    fps              | 23       |\n",
      "|    time_elapsed     | 1401     |\n",
      "|    total_timesteps  | 32786    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.002    |\n",
      "|    loss             | 0.748    |\n",
      "|    n_updates        | 29000    |\n",
      "----------------------------------\n",
      "Request Timed Out, cumulative Reward:\n",
      "-168.7999999999999\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-257.8999999999993\n",
      "(256, 256, 3)\n",
      "Request Timed Out, cumulative Reward:\n",
      "-105.80000000000013\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-347.2999999999995\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 432      |\n",
      "|    ep_rew_mean      | -516     |\n",
      "|    exploration_rate | 0.891    |\n",
      "| time/               |          |\n",
      "|    episodes         | 80       |\n",
      "|    fps              | 23       |\n",
      "|    time_elapsed     | 1490     |\n",
      "|    total_timesteps  | 34530    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.002    |\n",
      "|    loss             | 0.753    |\n",
      "|    n_updates        | 31000    |\n",
      "----------------------------------\n",
      "Request Timed Out, cumulative Reward:\n",
      "-324.399999999999\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-473.69999999999914\n",
      "(256, 256, 3)\n",
      "Request Timed Out, cumulative Reward:\n",
      "-232.8999999999997\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-351.49999999999903\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 438      |\n",
      "|    ep_rew_mean      | -516     |\n",
      "|    exploration_rate | 0.884    |\n",
      "| time/               |          |\n",
      "|    episodes         | 84       |\n",
      "|    fps              | 23       |\n",
      "|    time_elapsed     | 1582     |\n",
      "|    total_timesteps  | 36780    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.002    |\n",
      "|    loss             | 0.827    |\n",
      "|    n_updates        | 33000    |\n",
      "----------------------------------\n",
      "Request Timed Out, cumulative Reward:\n",
      "-177.39999999999986\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-561.1999999999989\n",
      "(256, 256, 3)\n",
      "Request Timed Out, cumulative Reward:\n",
      "-138.9000000000001\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-353.1999999999995\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 437      |\n",
      "|    ep_rew_mean      | -515     |\n",
      "|    exploration_rate | 0.878    |\n",
      "| time/               |          |\n",
      "|    episodes         | 88       |\n",
      "|    fps              | 23       |\n",
      "|    time_elapsed     | 1670     |\n",
      "|    total_timesteps  | 38497    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.002    |\n",
      "|    loss             | 0.872    |\n",
      "|    n_updates        | 35000    |\n",
      "----------------------------------\n",
      "Too many 3 link connections made.\n",
      "-399.6999999999991\n",
      "(256, 256, 3)\n",
      "Request Timed Out, cumulative Reward:\n",
      "0.6\n",
      "(256, 256, 3)\n",
      "Too many 3 link connections made.\n",
      "-330.199999999999\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-317.6999999999995\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 432      |\n",
      "|    ep_rew_mean      | -516     |\n",
      "|    exploration_rate | 0.874    |\n",
      "| time/               |          |\n",
      "|    episodes         | 92       |\n",
      "|    fps              | 23       |\n",
      "|    time_elapsed     | 1718     |\n",
      "|    total_timesteps  | 39785    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.002    |\n",
      "|    loss             | 0.895    |\n",
      "|    n_updates        | 36000    |\n",
      "----------------------------------\n",
      "Num timesteps: 40000\n",
      "Best mean reward: -523.56 - Last mean reward per episode: -515.57\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\tensorboard_logs/best_model\n",
      "Request Timed Out, cumulative Reward:\n",
      "-220.29999999999995\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-284.7999999999997\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-414.29999999999865\n",
      "(256, 256, 3)\n",
      "Request Timed Out, cumulative Reward:\n",
      "-12.499999999999998\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 427      |\n",
      "|    ep_rew_mean      | -511     |\n",
      "|    exploration_rate | 0.87     |\n",
      "| time/               |          |\n",
      "|    episodes         | 96       |\n",
      "|    fps              | 22       |\n",
      "|    time_elapsed     | 1812     |\n",
      "|    total_timesteps  | 41001    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.002    |\n",
      "|    loss             | 0.913    |\n",
      "|    n_updates        | 38000    |\n",
      "----------------------------------\n",
      "Too many invalid spectrum allocations.\n",
      "-400.99999999999926\n",
      "(256, 256, 3)\n",
      "Request Timed Out, cumulative Reward:\n",
      "-205.99999999999926\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-368.49999999999886\n",
      "(256, 256, 3)\n",
      "Request Timed Out, cumulative Reward:\n",
      "-212.09999999999962\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 430      |\n",
      "|    ep_rew_mean      | -510     |\n",
      "|    exploration_rate | 0.864    |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 22       |\n",
      "|    time_elapsed     | 1903     |\n",
      "|    total_timesteps  | 43001    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.002    |\n",
      "|    loss             | 0.796    |\n",
      "|    n_updates        | 40000    |\n",
      "----------------------------------\n",
      "Too many invalid spectrum allocations.\n",
      "-336.0999999999993\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-280.0999999999992\n",
      "(256, 256, 3)\n",
      "Request Timed Out, cumulative Reward:\n",
      "-1.4\n",
      "(256, 256, 3)\n",
      "Too many 3 link connections made.\n",
      "-384.6999999999991\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 423      |\n",
      "|    ep_rew_mean      | -500     |\n",
      "|    exploration_rate | 0.859    |\n",
      "| time/               |          |\n",
      "|    episodes         | 104      |\n",
      "|    fps              | 22       |\n",
      "|    time_elapsed     | 1951     |\n",
      "|    total_timesteps  | 44485    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.002    |\n",
      "|    loss             | 0.831    |\n",
      "|    n_updates        | 41000    |\n",
      "----------------------------------\n",
      "Too many 3 link connections made.\n",
      "-378.999999999999\n",
      "(256, 256, 3)\n",
      "Request Timed Out, cumulative Reward:\n",
      "-21.299999999999997\n",
      "(256, 256, 3)\n",
      "Too many 3 link connections made.\n",
      "-388.5999999999991\n",
      "(256, 256, 3)\n",
      "Too many 3 link connections made.\n",
      "-354.899999999999\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 424      |\n",
      "|    ep_rew_mean      | -495     |\n",
      "|    exploration_rate | 0.854    |\n",
      "| time/               |          |\n",
      "|    episodes         | 108      |\n",
      "|    fps              | 22       |\n",
      "|    time_elapsed     | 2000     |\n",
      "|    total_timesteps  | 45959    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.002    |\n",
      "|    loss             | 0.945    |\n",
      "|    n_updates        | 42000    |\n",
      "----------------------------------\n",
      "Request Timed Out, cumulative Reward:\n",
      "-22.799999999999994\n",
      "(256, 256, 3)\n",
      "Too many 3 link connections made.\n",
      "-409.89999999999844\n",
      "(256, 256, 3)\n",
      "Request Timed Out, cumulative Reward:\n",
      "-313.1999999999993\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-453.1999999999985\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 426      |\n",
      "|    ep_rew_mean      | -495     |\n",
      "|    exploration_rate | 0.849    |\n",
      "| time/               |          |\n",
      "|    episodes         | 112      |\n",
      "|    fps              | 22       |\n",
      "|    time_elapsed     | 2089     |\n",
      "|    total_timesteps  | 47777    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.002    |\n",
      "|    loss             | 0.776    |\n",
      "|    n_updates        | 44000    |\n",
      "----------------------------------\n",
      "Request Timed Out, cumulative Reward:\n",
      "-224.49999999999986\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-144.30000000000013\n",
      "(256, 256, 3)\n",
      "Too many 3 link connections made.\n",
      "-203.5\n",
      "(256, 256, 3)\n",
      "Request Timed Out, cumulative Reward:\n",
      "-267.89999999999907\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 414      |\n",
      "|    ep_rew_mean      | -492     |\n",
      "|    exploration_rate | 0.845    |\n",
      "| time/               |          |\n",
      "|    episodes         | 116      |\n",
      "|    fps              | 21       |\n",
      "|    time_elapsed     | 2267     |\n",
      "|    total_timesteps  | 49001    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.002    |\n",
      "|    loss             | 0.935    |\n",
      "|    n_updates        | 46000    |\n",
      "----------------------------------\n",
      "Too many 3 link connections made.\n",
      "-275.89999999999975\n",
      "(256, 256, 3)\n",
      "Too many 3 link connections made.\n",
      "-201.50000000000006\n",
      "(256, 256, 3)\n",
      "Num timesteps: 50000\n",
      "Best mean reward: -515.57 - Last mean reward per episode: -493.67\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\tensorboard_logs/best_model\n",
      "Request Timed Out, cumulative Reward:\n",
      "-207.69999999999942\n",
      "(256, 256, 3)\n",
      "Too many 3 link connections made.\n",
      "-335.39999999999907\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 421      |\n",
      "|    ep_rew_mean      | -496     |\n",
      "|    exploration_rate | 0.84     |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 21       |\n",
      "|    time_elapsed     | 2324     |\n",
      "|    total_timesteps  | 50434    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.002    |\n",
      "|    loss             | 0.909    |\n",
      "|    n_updates        | 47000    |\n",
      "----------------------------------\n",
      "Too many invalid spectrum allocations.\n",
      "-244.39999999999924\n",
      "(256, 256, 3)\n",
      "Request Timed Out, cumulative Reward:\n",
      "-52.599999999999994\n",
      "(256, 256, 3)\n",
      "Too many 3 link connections made.\n",
      "-417.09999999999934\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-296.39999999999947\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 418      |\n",
      "|    ep_rew_mean      | -499     |\n",
      "|    exploration_rate | 0.836    |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 21       |\n",
      "|    time_elapsed     | 2372     |\n",
      "|    total_timesteps  | 51806    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.002    |\n",
      "|    loss             | 1.08     |\n",
      "|    n_updates        | 48000    |\n",
      "----------------------------------\n",
      "Request Timed Out, cumulative Reward:\n",
      "-144.40000000000003\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-452.09999999999843\n",
      "(256, 256, 3)\n",
      "Request Timed Out, cumulative Reward:\n",
      "-174.49999999999983\n",
      "(256, 256, 3)\n",
      "Too many 3 link connections made.\n",
      "-388.99999999999983\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 422      |\n",
      "|    ep_rew_mean      | -497     |\n",
      "|    exploration_rate | 0.831    |\n",
      "| time/               |          |\n",
      "|    episodes         | 128      |\n",
      "|    fps              | 21       |\n",
      "|    time_elapsed     | 2461     |\n",
      "|    total_timesteps  | 53487    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.002    |\n",
      "|    loss             | 1.1      |\n",
      "|    n_updates        | 50000    |\n",
      "----------------------------------\n",
      "Too many 3 link connections made.\n",
      "-180.30000000000007\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-287.89999999999935\n",
      "(256, 256, 3)\n",
      "Request Timed Out, cumulative Reward:\n",
      "-8.100000000000001\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-298.49999999999943\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 406      |\n",
      "|    ep_rew_mean      | -494     |\n",
      "|    exploration_rate | 0.828    |\n",
      "| time/               |          |\n",
      "|    episodes         | 132      |\n",
      "|    fps              | 21       |\n",
      "|    time_elapsed     | 2506     |\n",
      "|    total_timesteps  | 54346    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.002    |\n",
      "|    loss             | 1.13     |\n",
      "|    n_updates        | 51000    |\n",
      "----------------------------------\n",
      "Too many invalid spectrum allocations.\n",
      "-447.19999999999897\n",
      "(256, 256, 3)\n",
      "Request Timed Out, cumulative Reward:\n",
      "-124.40000000000008\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-331.399999999999\n",
      "(256, 256, 3)\n",
      "Too many 3 link connections made.\n",
      "-298.7999999999993\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 409      |\n",
      "|    ep_rew_mean      | -499     |\n",
      "|    exploration_rate | 0.823    |\n",
      "| time/               |          |\n",
      "|    episodes         | 136      |\n",
      "|    fps              | 21       |\n",
      "|    time_elapsed     | 2557     |\n",
      "|    total_timesteps  | 55924    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.002    |\n",
      "|    loss             | 1.08     |\n",
      "|    n_updates        | 52000    |\n",
      "----------------------------------\n",
      "Request Timed Out, cumulative Reward:\n",
      "-39.60000000000002\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-374.29999999999893\n",
      "(256, 256, 3)\n",
      "Too many 3 link connections made.\n",
      "-422.4999999999991\n",
      "(256, 256, 3)\n",
      "Request Timed Out, cumulative Reward:\n",
      "-20.399999999999995\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 400      |\n",
      "|    ep_rew_mean      | -496     |\n",
      "|    exploration_rate | 0.819    |\n",
      "| time/               |          |\n",
      "|    episodes         | 140      |\n",
      "|    fps              | 21       |\n",
      "|    time_elapsed     | 2641     |\n",
      "|    total_timesteps  | 57001    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.002    |\n",
      "|    loss             | 1.19     |\n",
      "|    n_updates        | 54000    |\n",
      "----------------------------------\n",
      "Too many 3 link connections made.\n",
      "-268.49999999999915\n",
      "(256, 256, 3)\n",
      "Request Timed Out, cumulative Reward:\n",
      "-321.79999999999825\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-416.5999999999982\n",
      "(256, 256, 3)\n",
      "Request Timed Out, cumulative Reward:\n",
      "-153.99999999999977\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 402      |\n",
      "|    ep_rew_mean      | -492     |\n",
      "|    exploration_rate | 0.813    |\n",
      "| time/               |          |\n",
      "|    episodes         | 144      |\n",
      "|    fps              | 20       |\n",
      "|    time_elapsed     | 2813     |\n",
      "|    total_timesteps  | 59001    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.002    |\n",
      "|    loss             | 0.931    |\n",
      "|    n_updates        | 56000    |\n",
      "----------------------------------\n",
      "Too many 3 link connections made.\n",
      "-190.6000000000001\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-400.2999999999992\n",
      "(256, 256, 3)\n",
      "Num timesteps: 60000\n",
      "Best mean reward: -493.67 - Last mean reward per episode: -493.50\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\tensorboard_logs/best_model\n",
      "Request Timed Out, cumulative Reward:\n",
      "-223.69999999999968\n",
      "(256, 256, 3)\n",
      "Too many 3 link connections made.\n",
      "-303.79999999999984\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 403      |\n",
      "|    ep_rew_mean      | -496     |\n",
      "|    exploration_rate | 0.809    |\n",
      "| time/               |          |\n",
      "|    episodes         | 148      |\n",
      "|    fps              | 21       |\n",
      "|    time_elapsed     | 2865     |\n",
      "|    total_timesteps  | 60466    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.002    |\n",
      "|    loss             | 0.993    |\n",
      "|    n_updates        | 57000    |\n",
      "----------------------------------\n",
      "Request Timed Out, cumulative Reward:\n",
      "-376.4999999999985\n",
      "(256, 256, 3)\n",
      "Too many 3 link connections made.\n",
      "-332.99999999999994\n",
      "(256, 256, 3)\n",
      "Request Timed Out, cumulative Reward:\n",
      "-323.29999999999876\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-327.1999999999995\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 408      |\n",
      "|    ep_rew_mean      | -494     |\n",
      "|    exploration_rate | 0.802    |\n",
      "| time/               |          |\n",
      "|    episodes         | 152      |\n",
      "|    fps              | 21       |\n",
      "|    time_elapsed     | 2956     |\n",
      "|    total_timesteps  | 62496    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.002    |\n",
      "|    loss             | 0.898    |\n",
      "|    n_updates        | 59000    |\n",
      "----------------------------------\n",
      "Too many 3 link connections made.\n",
      "-176.10000000000008\n",
      "(256, 256, 3)\n",
      "Request Timed Out, cumulative Reward:\n",
      "-220.79999999999916\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-433.19999999999857\n",
      "(256, 256, 3)\n",
      "Request Timed Out, cumulative Reward:\n",
      "-130.00000000000006\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 400      |\n",
      "|    ep_rew_mean      | -491     |\n",
      "|    exploration_rate | 0.797    |\n",
      "| time/               |          |\n",
      "|    episodes         | 156      |\n",
      "|    fps              | 21       |\n",
      "|    time_elapsed     | 3044     |\n",
      "|    total_timesteps  | 64001    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.002    |\n",
      "|    loss             | 0.93     |\n",
      "|    n_updates        | 61000    |\n",
      "----------------------------------\n",
      "Too many 3 link connections made.\n",
      "-332.39999999999975\n",
      "(256, 256, 3)\n",
      "Request Timed Out, cumulative Reward:\n",
      "-339.4999999999986\n",
      "(256, 256, 3)\n",
      "Too many 3 link connections made.\n",
      "-285.89999999999964\n",
      "(256, 256, 3)\n",
      "Too many 3 link connections made.\n",
      "-164.50000000000006\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 397      |\n",
      "|    ep_rew_mean      | -489     |\n",
      "|    exploration_rate | 0.792    |\n",
      "| time/               |          |\n",
      "|    episodes         | 160      |\n",
      "|    fps              | 21       |\n",
      "|    time_elapsed     | 3094     |\n",
      "|    total_timesteps  | 65613    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.002    |\n",
      "|    loss             | 0.839    |\n",
      "|    n_updates        | 62000    |\n",
      "----------------------------------\n",
      "Request Timed Out, cumulative Reward:\n",
      "-183.19999999999925\n",
      "(256, 256, 3)\n",
      "Too many 3 link connections made.\n",
      "-324.89999999999975\n",
      "(256, 256, 3)\n",
      "Too many 3 link connections made.\n",
      "-190.0000000000001\n",
      "(256, 256, 3)\n",
      "Too many 3 link connections made.\n",
      "-171.90000000000003\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 392      |\n",
      "|    ep_rew_mean      | -489     |\n",
      "|    exploration_rate | 0.789    |\n",
      "| time/               |          |\n",
      "|    episodes         | 164      |\n",
      "|    fps              | 21       |\n",
      "|    time_elapsed     | 3141     |\n",
      "|    total_timesteps  | 66781    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.002    |\n",
      "|    loss             | 0.772    |\n",
      "|    n_updates        | 63000    |\n",
      "----------------------------------\n",
      "Request Timed Out, cumulative Reward:\n",
      "-194.99999999999972\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-214.69999999999965\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-269.9999999999988\n",
      "(256, 256, 3)\n",
      "Request Timed Out, cumulative Reward:\n",
      "-28.799999999999986\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 385      |\n",
      "|    ep_rew_mean      | -485     |\n",
      "|    exploration_rate | 0.785    |\n",
      "| time/               |          |\n",
      "|    episodes         | 168      |\n",
      "|    fps              | 21       |\n",
      "|    time_elapsed     | 3227     |\n",
      "|    total_timesteps  | 68001    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.002    |\n",
      "|    loss             | 0.805    |\n",
      "|    n_updates        | 65000    |\n",
      "----------------------------------\n",
      "Too many 3 link connections made.\n",
      "-338.8999999999989\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-309.5999999999994\n",
      "(256, 256, 3)\n",
      "Request Timed Out, cumulative Reward:\n",
      "-143.9999999999999\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-320.4999999999984\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 388      |\n",
      "|    ep_rew_mean      | -488     |\n",
      "|    exploration_rate | 0.779    |\n",
      "| time/               |          |\n",
      "|    episodes         | 172      |\n",
      "|    fps              | 21       |\n",
      "|    time_elapsed     | 3280     |\n",
      "|    total_timesteps  | 69760    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.002    |\n",
      "|    loss             | 1.02     |\n",
      "|    n_updates        | 66000    |\n",
      "----------------------------------\n",
      "Num timesteps: 70000\n",
      "Best mean reward: -493.50 - Last mean reward per episode: -487.68\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\tensorboard_logs/best_model\n",
      "Request Timed Out, cumulative Reward:\n",
      "-154.8999999999997\n",
      "(256, 256, 3)\n",
      "Too many 3 link connections made.\n",
      "-284.4\n",
      "(256, 256, 3)\n",
      "Request Timed Out, cumulative Reward:\n",
      "-283.39999999999856\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-295.09999999999957\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 385      |\n",
      "|    ep_rew_mean      | -483     |\n",
      "|    exploration_rate | 0.774    |\n",
      "| time/               |          |\n",
      "|    episodes         | 176      |\n",
      "|    fps              | 21       |\n",
      "|    time_elapsed     | 3371     |\n",
      "|    total_timesteps  | 71328    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.002    |\n",
      "|    loss             | 0.857    |\n",
      "|    n_updates        | 68000    |\n",
      "----------------------------------\n",
      "Too many invalid spectrum allocations.\n",
      "-194.39999999999918\n",
      "(256, 256, 3)\n",
      "Request Timed Out, cumulative Reward:\n",
      "-103.10000000000016\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-241.299999999999\n",
      "(256, 256, 3)\n",
      "Too many invalid spectrum allocations.\n",
      "-236.0999999999998\n",
      "(256, 256, 3)\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 384      |\n",
      "|    ep_rew_mean      | -484     |\n",
      "|    exploration_rate | 0.769    |\n",
      "| time/               |          |\n",
      "|    episodes         | 180      |\n",
      "|    fps              | 21       |\n",
      "|    time_elapsed     | 3421     |\n",
      "|    total_timesteps  | 72931    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.002    |\n",
      "|    loss             | 1.01     |\n",
      "|    n_updates        | 69000    |\n",
      "----------------------------------\n",
      "Request Timed Out, cumulative Reward:\n",
      "-26.099999999999984\n",
      "(256, 256, 3)\n",
      "Num timesteps: 80000\n",
      "Best mean reward: -487.68 - Last mean reward per episode: -481.46\n",
      "Saving new best model to c:\\Users\\f4nyt\\Documents\\GitHub\\4th-Year-Project-Toby\\tensorboard_logs/best_model\n",
      "Num timesteps: 90000\n",
      "Best mean reward: -481.46 - Last mean reward per episode: -481.46\n",
      "Num timesteps: 100000\n",
      "Best mean reward: -481.46 - Last mean reward per episode: -481.46\n",
      "Num timesteps: 110000\n",
      "Best mean reward: -481.46 - Last mean reward per episode: -481.46\n",
      "Num timesteps: 120000\n",
      "Best mean reward: -481.46 - Last mean reward per episode: -481.46\n",
      "Num timesteps: 130000\n",
      "Best mean reward: -481.46 - Last mean reward per episode: -481.46\n",
      "Num timesteps: 140000\n",
      "Best mean reward: -481.46 - Last mean reward per episode: -481.46\n",
      "Num timesteps: 150000\n",
      "Best mean reward: -481.46 - Last mean reward per episode: -481.46\n",
      "Num timesteps: 160000\n",
      "Best mean reward: -481.46 - Last mean reward per episode: -481.46\n",
      "Num timesteps: 170000\n",
      "Best mean reward: -481.46 - Last mean reward per episode: -481.46\n",
      "Num timesteps: 180000\n",
      "Best mean reward: -481.46 - Last mean reward per episode: -481.46\n",
      "Num timesteps: 190000\n",
      "Best mean reward: -481.46 - Last mean reward per episode: -481.46\n",
      "Num timesteps: 200000\n",
      "Best mean reward: -481.46 - Last mean reward per episode: -481.46\n",
      "Num timesteps: 210000\n",
      "Best mean reward: -481.46 - Last mean reward per episode: -481.46\n",
      "Num timesteps: 220000\n",
      "Best mean reward: -481.46 - Last mean reward per episode: -481.46\n",
      "Num timesteps: 230000\n",
      "Best mean reward: -481.46 - Last mean reward per episode: -481.46\n",
      "Num timesteps: 240000\n",
      "Best mean reward: -481.46 - Last mean reward per episode: -481.46\n",
      "Num timesteps: 250000\n",
      "Best mean reward: -481.46 - Last mean reward per episode: -481.46\n",
      "Num timesteps: 260000\n",
      "Best mean reward: -481.46 - Last mean reward per episode: -481.46\n",
      "Num timesteps: 270000\n",
      "Best mean reward: -481.46 - Last mean reward per episode: -481.46\n",
      "Num timesteps: 280000\n",
      "Best mean reward: -481.46 - Last mean reward per episode: -481.46\n",
      "Num timesteps: 290000\n",
      "Best mean reward: -481.46 - Last mean reward per episode: -481.46\n",
      "Num timesteps: 300000\n",
      "Best mean reward: -481.46 - Last mean reward per episode: -481.46\n",
      "Num timesteps: 310000\n",
      "Best mean reward: -481.46 - Last mean reward per episode: -481.46\n",
      "Num timesteps: 320000\n",
      "Best mean reward: -481.46 - Last mean reward per episode: -481.46\n",
      "Num timesteps: 330000\n",
      "Best mean reward: -481.46 - Last mean reward per episode: -481.46\n",
      "Num timesteps: 340000\n",
      "Best mean reward: -481.46 - Last mean reward per episode: -481.46\n",
      "Num timesteps: 350000\n",
      "Best mean reward: -481.46 - Last mean reward per episode: -481.46\n",
      "Num timesteps: 360000\n",
      "Best mean reward: -481.46 - Last mean reward per episode: -481.46\n",
      "Num timesteps: 370000\n",
      "Best mean reward: -481.46 - Last mean reward per episode: -481.46\n",
      "Num timesteps: 380000\n",
      "Best mean reward: -481.46 - Last mean reward per episode: -481.46\n",
      "Num timesteps: 390000\n",
      "Best mean reward: -481.46 - Last mean reward per episode: -481.46\n",
      "Num timesteps: 400000\n",
      "Best mean reward: -481.46 - Last mean reward per episode: -481.46\n",
      "Num timesteps: 410000\n",
      "Best mean reward: -481.46 - Last mean reward per episode: -481.46\n",
      "Num timesteps: 420000\n",
      "Best mean reward: -481.46 - Last mean reward per episode: -481.46\n",
      "Num timesteps: 430000\n",
      "Best mean reward: -481.46 - Last mean reward per episode: -481.46\n",
      "Num timesteps: 440000\n",
      "Best mean reward: -481.46 - Last mean reward per episode: -481.46\n",
      "Num timesteps: 450000\n",
      "Best mean reward: -481.46 - Last mean reward per episode: -481.46\n",
      "Num timesteps: 460000\n",
      "Best mean reward: -481.46 - Last mean reward per episode: -481.46\n",
      "Num timesteps: 470000\n",
      "Best mean reward: -481.46 - Last mean reward per episode: -481.46\n",
      "Num timesteps: 480000\n",
      "Best mean reward: -481.46 - Last mean reward per episode: -481.46\n",
      "Num timesteps: 490000\n",
      "Best mean reward: -481.46 - Last mean reward per episode: -481.46\n",
      "Num timesteps: 500000\n",
      "Best mean reward: -481.46 - Last mean reward per episode: -481.46\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22996/3139557428.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#training and saving model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDQN_Train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.002\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.99\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps_end\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps_fraction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"step\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_update_interval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtau\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_starts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModel_Name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"DQN_100322\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22996/2137756829.py\u001b[0m in \u001b[0;36mDQN_Train\u001b[1;34m(lr, gamma, eps_start, eps_end, eps_fraction, train_freq, target_update_interval, tau, learning_starts, timesteps, Model_Name)\u001b[0m\n\u001b[0;32m     70\u001b[0m     \u001b[1;31m#Debug Print\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Carry out training on: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModel_Name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimesteps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_log_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModel_Name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset_num_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;31m#training with wandb callback instead\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;31m#model.learn(total_timesteps=int(timesteps), tb_log_name=str(Model_Name), callback=WandbCallback(model_save_path=model_dir, model_save_freq=50000), reset_num_timesteps=False)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda\\envs\\RL\\lib\\site-packages\\stable_baselines3\\dqn\\dqn.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    265\u001b[0m             \u001b[0mtb_log_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtb_log_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[0meval_log_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0meval_log_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m             \u001b[0mreset_num_timesteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreset_num_timesteps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m         )\n\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[0;32m    359\u001b[0m                 \u001b[0mlearning_starts\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearning_starts\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m                 \u001b[0mreplay_buffer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 361\u001b[1;33m                 \u001b[0mlog_interval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlog_interval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    362\u001b[0m             )\n\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\off_policy_algorithm.py\u001b[0m in \u001b[0;36mcollect_rollouts\u001b[1;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[0;32m    608\u001b[0m             \u001b[1;31m# For SAC/TD3, the update is dones as the same time as the gradient update\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m             \u001b[1;31m# see https://github.com/hill-a/stable-baselines/issues/900\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_on_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdones\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda\\envs\\RL\\lib\\site-packages\\stable_baselines3\\dqn\\dqn.py\u001b[0m in \u001b[0;36m_on_step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    164\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_calls\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_calls\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_update_interval\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 166\u001b[1;33m             \u001b[0mpolyak_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq_net\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq_net_target\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtau\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    168\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexploration_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexploration_schedule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_progress_remaining\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Programs\\Anaconda\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\utils.py\u001b[0m in \u001b[0;36mpolyak_update\u001b[1;34m(params, target_params, tau)\u001b[0m\n\u001b[0;32m    431\u001b[0m         \u001b[1;31m# zip does not raise an exception if length of parameters does not match.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_param\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip_strict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m             \u001b[0mtarget_param\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtau\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m             \u001b[0mth\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_param\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtau\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtarget_param\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#training and saving model\n",
    "model = DQN_Train(lr=0.002, gamma=0.99, eps_start=1, eps_end=0.05, eps_fraction=0.3, train_freq=(1000, \"step\"), target_update_interval=50000, tau=0.5, learning_starts=3000, timesteps=1000000, Model_Name=\"DQN_100322\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Save_Model_Details(Model, Model_Name, timesteps):\n",
    "    '''Creating Folder in \"Models\" Folder and Saving Trained Model\n",
    "    \n",
    "    Input Arguements:\n",
    "    1) Model = Trained Model object to be saved.\n",
    "    2) Model_Name = String name of the model to be saved\n",
    "    3) timesteps = Number of timesteps used in training.\n",
    "\n",
    "\n",
    "    '''\n",
    "    #Creating folder to store the created model in \"Models\"\n",
    "    model_dir = os.path.join(\"Models/\", str(Model_Name) + \"_Full_Train\")\n",
    "    # Create log dir for tensorboard log\n",
    "    os.makedirs(model_dir, exist_ok=False)\n",
    "    \n",
    "    #Saving Trained Model\n",
    "    Model.save(model_dir)\n",
    "    print(str(Model_Name) + \" Successfully Saved.\")\n",
    "    print(\"Remember to shift the Monitor.csv and best_model.zip files into the created model folder manually!!\")\n",
    "\n",
    "    #Plotting Rewards for show\n",
    "    plot_results([log_tensorboard], timesteps, results_plotter.X_TIMESTEPS, \"DQN EON\")\n",
    "    plt.show()\n",
    "    plt.savefig(str(model_dir) + \" Episode rewards over timestamps.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Debug\n",
    "#torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Tensorboard Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-92f3dbb2c1193487\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-92f3dbb2c1193487\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#To access tensorboard logging, select the --logdir where the logs are at (in this case tensorboard_logs)\n",
    "%tensorboard --logdir tensorboard_logs\n",
    "\n",
    "#after this you can go to the following link http://localhost:6006/\n",
    "#Alternatively you can run the same command in a command prompt while in the source directory (without the %)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_Model(Model_Dir, Model_Name, device):\n",
    "    '''Loading Saved model Zip file into Object\n",
    "    \n",
    "    Input Arguements:\n",
    "    1) Model_Dir = Path to saved model file\n",
    "    2) Model_Name = Str of Model file name\n",
    "    3) Device = Device to run on, either \"cpu\" or \"cuda\"\n",
    "\n",
    "    Returns loaded model\n",
    "    '''\n",
    "\n",
    "    model_dir = os.path.join(str(Model_Dir), str(Model_Name))\n",
    "\n",
    "    load_model = DQN.load(str(model_dir),  device=device)\n",
    "    print(\"Successfully loaded \" + str(Model_Name) + \"!\")\n",
    "\n",
    "    return load_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:221: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 3.93GB > 1.18GB\n",
      "  \"This system does not have apparently enough memory to store the complete \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded DQNEveon_1mil_030322.zip!\n"
     ]
    }
   ],
   "source": [
    "loaded_model = Load_Model(\"Models/030322_Model_1mil_sofar\", \"DQNEveon_1mil_030322.zip\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:221: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 3.93GB > 3.78GB\n",
      "  \"This system does not have apparently enough memory to store the complete \"\n"
     ]
    }
   ],
   "source": [
    "#test2 is the successful one (but still just spams enter key)\n",
    "#LOADING MODEL FROM ZIP\n",
    "#loaded_model = DQN.load(\"DQNEveon_test2\")\n",
    "#loaded_model = DQN.load(\"DQNEveon_testing_surface_250222\")\n",
    "\n",
    "#2mil timestep model load\n",
    "#NOTE that old model on more complex game not compatible with new observation window\n",
    "#loaded_model = DQN.load(\"DQNEveon_2mil_270222\", device=\"cpu\")\n",
    "#best model load during 2mil training\n",
    "#loaded_model = DQN.load(\"./Models/270222_Model_2mil/best_model.zip\", device=\"cpu\")\n",
    "\n",
    "#loading 1mil semi simplified model with new rewards\n",
    "#loaded_model = DQN.load(\"DQNEveon_1mil_030322.zip\")\n",
    "#loaded_model = DQN.load(\"DQNEveon_2mil_030322.zip\")\n",
    "#loaded_model = DQN.load(\"DQNEveon_2+3mil_030322.zip\")\n",
    "#loaded_model = DQN.load(\"./Models/030322_Model_1mil_sofar/best_model.zip\")\n",
    "\n",
    "\n",
    "#loaded_model = DQN.load(\"DQNEveon_040322.zip\")\n",
    "#loaded_model = DQN.load(\"./tensorboard_logs/best_model.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Agent Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodeList, linkList = createTestTopology()\n",
    "\n",
    "#changed to only have 1 request per episode\n",
    "#from 6 originally\n",
    "requestList = generateRequests(nodeList, 6)\n",
    "\n",
    "user = User()\n",
    "eveon = game_gym(nodeList, linkList, requestList, user)\n",
    "\n",
    "check_env(eveon, warn=True)\n",
    "\n",
    "# THIS IS THE TESTING LOOP OF THE AGENT PLAYING THE GAME|\n",
    "obs = eveon.reset()\n",
    "while True :\n",
    "\n",
    "    #enable this if using older models with the wider observation space\n",
    "    #obs = cv2.resize(obs, dsize=(600, 1000))\n",
    "    \n",
    "    \n",
    "    #trying to test if deterministic true or false changes model actions\n",
    "    action, states_ = loaded_model.predict(obs, deterministic=True)\n",
    "    # action = 6\n",
    "    obs, rewards, dones, info = eveon.step(action)\n",
    "    \n",
    "    print(\"Action:\")\n",
    "    print(action)\n",
    "    #time.sleep(1)\n",
    "    #clear_output(wait=True)\n",
    "\n",
    "\n",
    "    if dones == True:\n",
    "        #debug print\n",
    "        print(\"########################Reward Obtained:\")\n",
    "        print(eveon.reward)\n",
    "        # with open('info.json', 'w') as outfile:\n",
    "        #     json.dump(info, outfile)\n",
    "\n",
    "        eveon.reset()\n",
    "\n",
    "    eveon.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Action Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent taking random actions\n",
    "nodeList, linkList = createTestTopology()\n",
    "\n",
    "#changed to only have 1 request per episode\n",
    "#from 6 originally\n",
    "requestList = generateRequests(nodeList, 6)\n",
    "\n",
    "user = User()\n",
    "eveon = game_gym(nodeList, linkList, requestList, user)\n",
    "\n",
    "check_env(eveon, warn=True)\n",
    "#resets the environment\n",
    "obs = eveon.reset()\n",
    "\n",
    "for step in range(2000):\n",
    "\teveon.render()\n",
    "\n",
    "\trand_action = eveon.action_space.sample()\n",
    "\teveon.step(rand_action)\n",
    "\tprint(rand_action)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test main function\n",
    "nodeList, linkList = createTestTopology()\n",
    "requestList = generateRequests(nodeList, 6)\n",
    "\n",
    "user = User()\n",
    "eveon = game_gym(nodeList, linkList, requestList, user)\n",
    "\n",
    "check_env(eveon, warn=True)\n",
    "\n",
    "#defining the agent\n",
    "model = DQN('MlpPolicy', eveon, verbose=1, buffer_size=100, device='cuda')\n",
    "\n",
    "screen = eveon.render()\n",
    "#storing episode_durations during training to plot them\n",
    "#creating empty list to store\n",
    "#adding plot to check the training process?\n",
    "episode_durations = []\n",
    "timestep = 0\n",
    "\n",
    "\n",
    "#trying the double nested for loop\n",
    "\n",
    "for episode in range(100):\n",
    "\n",
    "#resets the environment\n",
    "obs = eveon.reset()\n",
    "\n",
    "#nested for loop iterate over time step\n",
    "for timesetp in count():\n",
    "\n",
    "    action, states_ = model.predict(obs, deterministic=True)\n",
    "    # action = 6\n",
    "    \n",
    "    obs, rewards, dones, info = eveon.step(action)\n",
    "    \n",
    "\n",
    "    print(action)\n",
    "    \n",
    "    #this only runs when the agent actually competed the level\n",
    "    if dones == True:\n",
    "        #debug print\n",
    "        print(\"########################Reward Obtained:\")\n",
    "        print(eveon.reward)\n",
    "\n",
    "        episode_durations.append(timestep)\n",
    "        plot(episode_durations, 100)\n",
    "        \n",
    "        # with open('info.json', 'w') as outfile:\n",
    "        #     json.dump(info, outfile)\n",
    "        timestep += 1\n",
    "        eveon.reset()\n",
    "\n",
    "    eveon.render()\n",
    "\n",
    "    screen = eveon.render('rgb_array')\n",
    "    plt.figure()\n",
    "    plt.imshow(screen)\n",
    "    plt.title('test screen')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40b93cbb1699659d2cb0e714698cbe72f80d4ea3079150298ec50c78e4aa7ede"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('RL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
