{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.0.3 (SDL 2.0.16, Python 3.7.11)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from optical_network_game.node import *\n",
    "from optical_network_game.link import *\n",
    "from optical_network_game.requests import *\n",
    "from optical_network_game.user import *\n",
    "import gym\n",
    "import pygame, sys\n",
    "from pygame.locals import *\n",
    "from gym import spaces\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "\n",
    "\n",
    "from stable_baselines3.common import results_plotter\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy, plot_results\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "#from stable_baselines.common.vec_env import DummyVecEnv\n",
    "#from stable_baselines.deepq.policies import MlpPolicy\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3 import A2C\n",
    "import json\n",
    "import cv2\n",
    "\n",
    "#additional code added by me just for testing\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "#importing IPython's display module to plot images\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython: from IPython import display\n",
    "from itertools import count\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "#importing wandb (weights and biases) for logging\n",
    "import wandb\n",
    "import tensorboard\n",
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "\n",
    "#Importing game_gym class for use\n",
    "import importlib\n",
    "import optical_network_game.game_gym\n",
    "importlib.reload(optical_network_game.game_gym)\n",
    "from optical_network_game.game_gym import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveOnBestTrainingRewardCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model (the check is done every ``check_freq`` steps)\n",
    "    based on the training reward (in practice, we recommend using ``EvalCallback``).\n",
    "\n",
    "    :param check_freq:\n",
    "    :param log_dir: Path to the folder where the model will be saved.\n",
    "      It must contains the file created by the ``Monitor`` wrapper.\n",
    "    :param verbose: Verbosity level.\n",
    "    \"\"\"\n",
    "    def __init__(self, check_freq: int, log_dir: str, verbose: int = 1):\n",
    "        super(SaveOnBestTrainingRewardCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, 'best_model')\n",
    "        self.best_mean_reward = -np.inf\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "\n",
    "          # Retrieve training reward\n",
    "          x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "          if len(x) > 0:\n",
    "              # Mean training reward over the last 100 episodes\n",
    "              mean_reward = np.mean(y[-100:])\n",
    "              if self.verbose > 0:\n",
    "                print(f\"Num timesteps: {self.num_timesteps}\")\n",
    "                print(f\"Best mean reward: {self.best_mean_reward:.2f} - Last mean reward per episode: {mean_reward:.2f}\")\n",
    "\n",
    "              # New best model, you could save the agent here\n",
    "              if mean_reward > self.best_mean_reward:\n",
    "                  self.best_mean_reward = mean_reward\n",
    "                  # Example for saving best model\n",
    "                  if self.verbose > 0:\n",
    "                    print(f\"Saving new best model to {self.save_path}\")\n",
    "                  self.model.save(self.save_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating fixed test topology\n",
    "def createTestTopology():\n",
    "    # testNodes\n",
    "    nodeA = Node(0, 'A', 300, 200)\n",
    "    nodeB = Node(1, 'B', 300, 400)\n",
    "    nodeC = Node(2, 'C', 650, 200)\n",
    "    nodeD = Node(3, 'D', 650, 400)\n",
    "    # testLinks\n",
    "    link1 = Link(0, nodeA, nodeB)\n",
    "    link2 = Link(1, nodeB, nodeC)\n",
    "    link3 = Link(2, nodeB, nodeD)\n",
    "    link4 = Link(3, nodeA, nodeC)\n",
    "    link5 = Link(4, nodeC, nodeD)\n",
    "\n",
    "    nodeList = [nodeA, nodeB, nodeC, nodeD]\n",
    "    linkList = [link1, link2, link3, link4, link5]\n",
    "\n",
    "    # save the links associated to each node in a list\n",
    "    for node in nodeList:\n",
    "        node.setLinks(linkList)\n",
    "    return nodeList, linkList\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create log dir for saving model\n",
    "log_dir = os.path.join(os.getcwd(), \"tmp/\")\n",
    "#os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# Create log dir for tensorboard log\n",
    "log_tensorboard = os.path.join(os.getcwd(), \"tensorboard_logs/\")\n",
    "os.makedirs(log_tensorboard, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DQN_Train(lr, gamma, eps_start, eps_end, eps_fraction, train_freq, target_update_interval, tau, learning_starts, timesteps, Model_Name):\n",
    "    '''Function which conducts model training for DQN on the EON Game Environment\n",
    "    Returns Model object and Model Name\n",
    "\n",
    "    Input Arguements:\n",
    "    1) lr = learning rate\n",
    "    2) gamma = discount factor\n",
    "    3) eps_start = Starting Exploration Rate Value \n",
    "    4) eps_end = Ending Exploration Rate Value\n",
    "    5) exploration_fraction = Fraction of timesteps you want the exploration value to decay (Between eps_start and eps_end)\n",
    "    6) train_freq = frequency of updating the model (either per number of episodes or timesteps)\n",
    "    7) target_update_interval = frequency of updating the target network\n",
    "    8) Tau = The soft update coefficient (\"Polyak update\", between 0 and 1) default 1 for hard update\n",
    "    9) learning_starts = Number of timesteps before model starting doing training updates\n",
    "    10) timesteps = Number of timesteps to train the model through\n",
    "    11) Model_Name = String name of model to be used to save as.\n",
    "\n",
    "    The tensorboard logging is saved in /tensorboard_logs with the respective name for the run given by Model_Name\n",
    "\n",
    "    Model is trained on the TestTopology with 6 request links generated, uses the base callback which saves the best model based on mean_ep_rewards\n",
    "    '''\n",
    "    from wandb.integration.sb3 import WandbCallback\n",
    "\n",
    "    #main function with callback\n",
    "    \n",
    "    #Start a wandb instance\n",
    "    wandb.init(\n",
    "        project=\"DQN_EON\",\n",
    "        sync_tensorboard=True,  # auto-upload sb3's tensorboard metrics\n",
    "    )\n",
    "\n",
    "    # Create and wrap the environment\n",
    "    nodeList, linkList = createTestTopology()\n",
    "    #changed to only have 1 request per episode\n",
    "    #from 6 originally\n",
    "    requestList = generateRequests(nodeList, 6)\n",
    "    user = User()\n",
    "\n",
    "\n",
    "    env = game_gym(nodeList, linkList, requestList, user)\n",
    "    check_env(env)\n",
    "\n",
    "    eveon = Monitor(env, log_tensorboard)\n",
    "\n",
    "    # Create the callback: check every 10000 steps\n",
    "    callback = SaveOnBestTrainingRewardCallback(check_freq=10000, log_dir=log_tensorboard)\n",
    "    \n",
    "    model_dir = os.path.join(\"Models/\", str(Model_Name))\n",
    "\n",
    "    #hyperparameters testing\n",
    "    #previously used\n",
    "    #lr = 0.01\n",
    "    #gamma = 0.7\n",
    "    #eps_start = 1\n",
    "    #eps_end = 0.05\n",
    "    #train_freq = (1000, \"step\")\n",
    "    #target_update_interval = 50000\n",
    "    #soft update tau value\n",
    "    #tau = 0.4\n",
    "    policy_kwargs = {\n",
    "        'net_arch':[64,64] #MLP hidden layer size\n",
    "    }\n",
    "\n",
    "\n",
    "    # Train the agent\n",
    "    #model = DQN('MlpPolicy', eveon, verbose=2, buffer_size=100)\n",
    "    model = DQN('MlpPolicy', eveon, tau=tau, learning_starts=learning_starts, buffer_size=10000, verbose=1, device=\"auto\", learning_rate=lr, gamma=gamma, exploration_fraction=eps_fraction, exploration_initial_eps=eps_start, exploration_final_eps=eps_end, target_update_interval=target_update_interval, train_freq=train_freq, tensorboard_log=\"./tensorboard_logs/\")\n",
    "        \n",
    "\n",
    "    #Debug Print\n",
    "    print(\"Carry out training on: \" + str(Model_Name))\n",
    "    #model.learn(total_timesteps=int(timesteps), tb_log_name=str(Model_Name), callback=callback, reset_num_timesteps=False)\n",
    "    #training with wandb callback instead\n",
    "    model.learn(total_timesteps=int(timesteps), tb_log_name=str(Model_Name), callback=WandbCallback(model_save_path=model_dir, model_save_freq=50000), reset_num_timesteps=False)\n",
    "    \n",
    "    Save_Model_Details(model, Model_Name, timesteps)\n",
    "    # # Close the environment\n",
    "    # eveon.close()\n",
    "\n",
    "    #Finishing wandb run\n",
    "    wandb.finish()\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training and saving model\n",
    "model = DQN_Train(lr=0.005, gamma=0.98, eps_start=1, eps_end=0.05, eps_fraction=0.8, train_freq=(100, \"step\"), target_update_interval=50000, tau=0.5, learning_starts=50000, timesteps=500000, Model_Name=\"DQN_Date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Save_Model_Details(Model, Model_Name, timesteps):\n",
    "    '''Creating Folder in \"Models\" Folder and Saving Trained Model\n",
    "    \n",
    "    Input Arguements:\n",
    "    1) Model = Trained Model object to be saved.\n",
    "    2) Model_Name = String name of the model to be saved\n",
    "    3) timesteps = Number of timesteps used in training.\n",
    "\n",
    "\n",
    "    '''\n",
    "    #Creating folder to store the created model in \"Models\"\n",
    "    model_dir = os.path.join(\"Models/\", str(Model_Name) + \"_Full_Train\")\n",
    "    # Create log dir for tensorboard log\n",
    "    os.makedirs(model_dir, exist_ok=False)\n",
    "    \n",
    "    #Saving Trained Model\n",
    "    Model.save(model_dir)\n",
    "    print(str(Model_Name) + \" Successfully Saved.\")\n",
    "    print(\"Remember to shift the Monitor.csv and best_model.zip files into the created model folder manually!!\")\n",
    "\n",
    "    #Plotting Rewards for show\n",
    "    plot_results([log_tensorboard], timesteps, results_plotter.X_TIMESTEPS, \"DQN EON\")\n",
    "    plt.show()\n",
    "    plt.savefig(str(model_dir) + \" Episode rewards over timestamps.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Debug\n",
    "#torch.cuda.memory_summary(device=None, abbreviated=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing Tensorboard Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-92f3dbb2c1193487\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-92f3dbb2c1193487\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#To access tensorboard logging, select the --logdir where the logs are at (in this case tensorboard_logs)\n",
    "%tensorboard --logdir tensorboard_logs\n",
    "\n",
    "#after this you can go to the following link http://localhost:6006/\n",
    "#Alternatively you can run the same command in a command prompt while in the source directory (without the %)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Load_Model(Model_Dir, Model_Name, device):\n",
    "    '''Loading Saved model Zip file into Object\n",
    "    \n",
    "    Input Arguements:\n",
    "    1) Model_Dir = Path to saved model file\n",
    "    2) Model_Name = Str of Model file name\n",
    "    3) Device = Device to run on, either \"cpu\" or \"cuda\"\n",
    "\n",
    "    Returns loaded model\n",
    "    '''\n",
    "\n",
    "    model_dir = os.path.join(str(Model_Dir), str(Model_Name))\n",
    "\n",
    "    load_model = DQN.load(str(model_dir),  device=device)\n",
    "    print(\"Successfully loaded \" + str(Model_Name) + \"!\")\n",
    "\n",
    "    return load_model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:221: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 3.93GB > 1.18GB\n",
      "  \"This system does not have apparently enough memory to store the complete \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded DQNEveon_1mil_030322.zip!\n"
     ]
    }
   ],
   "source": [
    "loaded_model = Load_Model(\"Models/030322_Model_1mil_sofar\", \"DQNEveon_1mil_030322.zip\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Programs\\Anaconda\\envs\\RL\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:221: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 3.93GB > 3.78GB\n",
      "  \"This system does not have apparently enough memory to store the complete \"\n"
     ]
    }
   ],
   "source": [
    "#test2 is the successful one (but still just spams enter key)\n",
    "#LOADING MODEL FROM ZIP\n",
    "#loaded_model = DQN.load(\"DQNEveon_test2\")\n",
    "#loaded_model = DQN.load(\"DQNEveon_testing_surface_250222\")\n",
    "\n",
    "#2mil timestep model load\n",
    "#NOTE that old model on more complex game not compatible with new observation window\n",
    "#loaded_model = DQN.load(\"DQNEveon_2mil_270222\", device=\"cpu\")\n",
    "#best model load during 2mil training\n",
    "#loaded_model = DQN.load(\"./Models/270222_Model_2mil/best_model.zip\", device=\"cpu\")\n",
    "\n",
    "#loading 1mil semi simplified model with new rewards\n",
    "#loaded_model = DQN.load(\"DQNEveon_1mil_030322.zip\")\n",
    "#loaded_model = DQN.load(\"DQNEveon_2mil_030322.zip\")\n",
    "#loaded_model = DQN.load(\"DQNEveon_2+3mil_030322.zip\")\n",
    "#loaded_model = DQN.load(\"./Models/030322_Model_1mil_sofar/best_model.zip\")\n",
    "\n",
    "\n",
    "#loaded_model = DQN.load(\"DQNEveon_040322.zip\")\n",
    "#loaded_model = DQN.load(\"./tensorboard_logs/best_model.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Agent Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodeList, linkList = createTestTopology()\n",
    "\n",
    "#changed to only have 1 request per episode\n",
    "#from 6 originally\n",
    "requestList = generateRequests(nodeList, 6)\n",
    "\n",
    "user = User()\n",
    "eveon = game_gym(nodeList, linkList, requestList, user)\n",
    "\n",
    "check_env(eveon, warn=True)\n",
    "\n",
    "# THIS IS THE TESTING LOOP OF THE AGENT PLAYING THE GAME|\n",
    "obs = eveon.reset()\n",
    "while True :\n",
    "\n",
    "    #enable this if using older models with the wider observation space\n",
    "    #obs = cv2.resize(obs, dsize=(600, 1000))\n",
    "    \n",
    "    \n",
    "    #trying to test if deterministic true or false changes model actions\n",
    "    action, states_ = loaded_model.predict(obs, deterministic=True)\n",
    "    # action = 6\n",
    "    obs, rewards, dones, info = eveon.step(action)\n",
    "    \n",
    "    print(\"Action:\")\n",
    "    print(action)\n",
    "    #time.sleep(1)\n",
    "    #clear_output(wait=True)\n",
    "\n",
    "\n",
    "    if dones == True:\n",
    "        #debug print\n",
    "        print(\"########################Reward Obtained:\")\n",
    "        print(eveon.reward)\n",
    "        # with open('info.json', 'w') as outfile:\n",
    "        #     json.dump(info, outfile)\n",
    "\n",
    "        eveon.reset()\n",
    "\n",
    "    eveon.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Action Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#agent taking random actions\n",
    "nodeList, linkList = createTestTopology()\n",
    "\n",
    "#changed to only have 1 request per episode\n",
    "#from 6 originally\n",
    "requestList = generateRequests(nodeList, 6)\n",
    "\n",
    "user = User()\n",
    "eveon = game_gym(nodeList, linkList, requestList, user)\n",
    "\n",
    "check_env(eveon, warn=True)\n",
    "#resets the environment\n",
    "obs = eveon.reset()\n",
    "\n",
    "for step in range(2000):\n",
    "\teveon.render()\n",
    "\n",
    "\trand_action = eveon.action_space.sample()\n",
    "\teveon.step(rand_action)\n",
    "\tprint(rand_action)\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test main function\n",
    "nodeList, linkList = createTestTopology()\n",
    "requestList = generateRequests(nodeList, 6)\n",
    "\n",
    "user = User()\n",
    "eveon = game_gym(nodeList, linkList, requestList, user)\n",
    "\n",
    "check_env(eveon, warn=True)\n",
    "\n",
    "#defining the agent\n",
    "model = DQN('MlpPolicy', eveon, verbose=1, buffer_size=100, device='cuda')\n",
    "\n",
    "screen = eveon.render()\n",
    "#storing episode_durations during training to plot them\n",
    "#creating empty list to store\n",
    "#adding plot to check the training process?\n",
    "episode_durations = []\n",
    "timestep = 0\n",
    "\n",
    "\n",
    "#trying the double nested for loop\n",
    "\n",
    "for episode in range(100):\n",
    "\n",
    "#resets the environment\n",
    "obs = eveon.reset()\n",
    "\n",
    "#nested for loop iterate over time step\n",
    "for timesetp in count():\n",
    "\n",
    "    action, states_ = model.predict(obs, deterministic=True)\n",
    "    # action = 6\n",
    "    \n",
    "    obs, rewards, dones, info = eveon.step(action)\n",
    "    \n",
    "\n",
    "    print(action)\n",
    "    \n",
    "    #this only runs when the agent actually competed the level\n",
    "    if dones == True:\n",
    "        #debug print\n",
    "        print(\"########################Reward Obtained:\")\n",
    "        print(eveon.reward)\n",
    "\n",
    "        episode_durations.append(timestep)\n",
    "        plot(episode_durations, 100)\n",
    "        \n",
    "        # with open('info.json', 'w') as outfile:\n",
    "        #     json.dump(info, outfile)\n",
    "        timestep += 1\n",
    "        eveon.reset()\n",
    "\n",
    "    eveon.render()\n",
    "\n",
    "    screen = eveon.render('rgb_array')\n",
    "    plt.figure()\n",
    "    plt.imshow(screen)\n",
    "    plt.title('test screen')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40b93cbb1699659d2cb0e714698cbe72f80d4ea3079150298ec50c78e4aa7ede"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('RL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
